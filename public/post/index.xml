<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Lucas Enrich</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>es</language>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title></title>
      <link>/post/bayes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/bayes/</guid>
      <description>&lt;h1 id=&#34;interpretación-bayesiana-de-la-matriz-de-confusión&#34;&gt;Interpretación bayesiana de la matriz de confusión&lt;/h1&gt;
&lt;p&gt;¿Cómo evaluar modelos si solo tenemos la matriz de confusión?&lt;/p&gt;
&lt;p&gt;Esta interpretación pertenece originalmente a un paper homónimo de
Olivier Caelen (2000).Pero es una interesante forma de entender la
matriz de confusión, por lo que esto será una
traducción/interpretación/resumen de aquel.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lucasenrich.netlify.com/post/bayes_cm/&#34;&gt;Read more &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Puedes leer ese articulo aqui: &lt;a href=&#34;http://www.oliviercaelen.be/doc/confMatrixBayes_AMAI.pdf&#34;&gt;http://www.oliviercaelen.be/doc/confMatrixBayes_AMAI.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;La matriz de confusión puede ser a veces muy confusa, falsos positivos,
falsos negativos, verdaderos positivos, verdaderos negativos; y todas
las métricas que salen de ahi, el recall, el accuracy, el F-score.&lt;/p&gt;
&lt;p&gt;Todas ellas se basan en la evidencia concreta de los set testeados, pero
no indican cuanta incertidumbre hay en tal o cual indicador.&lt;/p&gt;
&lt;p&gt;Supongamos que comparamos dos modelos, uno tiene 72% de precisión y el
otro 70%. Dado que se trabaja con datos muestreados aleatoriamente (ya
sea por el muestreo de dataset usado para hacer el modelo, ya sea por
train-test split), los valores que terminan en la matriz de confusión
también son fruto de un proceso aleatorio.&lt;/p&gt;
&lt;p&gt;Para las tecnicas bayesianas, cualquier estimación es contingente a los
datos obtenidos y a las creencias que tiene quien investiga. Asimismo,
todo lo estimado tiene una distribución especifica que puede ser
investigada.&lt;/p&gt;
&lt;h4 id=&#34;la-matriz-de-confusión&#34;&gt;&lt;em&gt;La matriz de Confusión&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;Dado un dataset, se genera un modelo que mapea &lt;em&gt;X&lt;/em&gt; a &lt;em&gt;Y&lt;/em&gt;, esto puede ser
escrito como: $h: x y $, pero a veces se le pifia, y a veces no.&lt;/p&gt;
&lt;p&gt;El diagrama que implica la matriz de confusión es:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/mc.png&#34; width=&#34;400px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Pero podemos pensar en la matriz de confusión como un vector de valores:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;V&lt;/em&gt; = (&lt;em&gt;n&lt;/em&gt;°&lt;em&gt;T**P&lt;/em&gt;, &lt;em&gt;n&lt;/em&gt;°&lt;em&gt;T**N&lt;/em&gt;, &lt;em&gt;n&lt;/em&gt;°&lt;em&gt;F**P&lt;/em&gt;, &lt;em&gt;n&lt;/em&gt;°&lt;em&gt;F**N&lt;/em&gt;)&lt;/p&gt;
&lt;h4 id=&#34;el-planteo-bayesiano&#34;&gt;&lt;em&gt;El planteo Bayesiano&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;Este vector númerico, &lt;em&gt;V&lt;/em&gt;, puede ser entendido como que vino de una
función de distribución multinomial, la cual es una generalización de la
binomial, solo que en lugar de tener dos valores posibles los cuales
tienen una probabilidad cada uno, hay cuatro los cuales dependen de los
parametros de la distribución binomial.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;V&lt;/em&gt; − &lt;em&gt;M&lt;strong&gt;u&lt;/strong&gt;l**t&lt;/em&gt;(&lt;em&gt;N&lt;/em&gt;&lt;sub&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;θ&lt;/em&gt;)&lt;/p&gt;
&lt;p&gt;Donde:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;θ&lt;/em&gt; = (&lt;em&gt;θ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t**p&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;θ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t**n&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;θ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;f**p&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;θ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;f**n&lt;/em&gt;&lt;/sub&gt;)&lt;/p&gt;
&lt;p&gt;Son los parametros que determinan la realización de los valores dentro
de &lt;em&gt;V&lt;/em&gt;, es decir &lt;em&gt;v&lt;/em&gt; depende de &lt;em&gt;θ&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Entonces, si los valores de la matriz de confusión son aleatorios con
una función de distribución de probabilidad dada, la probabilidad de
obtener un set de valores &lt;em&gt;v&lt;/em&gt; que llene el vector que contiene los
valores de la matriz de confusión &lt;em&gt;V&lt;/em&gt; puede escribirse como
&lt;em&gt;P&lt;/em&gt;(&lt;em&gt;V&lt;/em&gt; = &lt;em&gt;v&lt;/em&gt;), en este caso, se considera que &lt;em&gt;θ&lt;/em&gt; es fijo.&lt;/p&gt;
&lt;p&gt;Pero si &lt;em&gt;θ&lt;/em&gt; proviene de una variable aleatoria, esto es, no es fijo,
&lt;em&gt;P&lt;/em&gt;(&lt;em&gt;V&lt;/em&gt; = &lt;em&gt;v&lt;/em&gt;) se vuelve contingente a los valores que pueda tomar &lt;em&gt;θ&lt;/em&gt;,
por lo que se podria escribirse:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;P&lt;/em&gt;(&lt;em&gt;V&lt;/em&gt; = &lt;em&gt;v&lt;/em&gt;|&lt;em&gt;Θ&lt;/em&gt; = &lt;em&gt;θ&lt;/em&gt;)&lt;/p&gt;
&lt;p&gt;Y de acá proviene el planteo bayesiano, yo quiero conocer &lt;em&gt;Θ&lt;/em&gt;, pero solo
veo las realizaciones de la matriz de confusión &lt;em&gt;v&lt;/em&gt;, por lo que no
quiero &lt;em&gt;P&lt;/em&gt;(&lt;em&gt;V&lt;/em&gt; = &lt;em&gt;v&lt;/em&gt;|&lt;em&gt;Θ&lt;/em&gt; = &lt;em&gt;θ&lt;/em&gt;), sino &lt;em&gt;P&lt;/em&gt;(&lt;em&gt;Θ&lt;/em&gt; = &lt;em&gt;θ&lt;/em&gt;|&lt;em&gt;V&lt;/em&gt; = &lt;em&gt;v&lt;/em&gt;), lo cual
se puede escribir según la regla de bayes:&lt;/p&gt;
&lt;p&gt;$$
f_{\Theta|V}(\theta|v) = \frac{P(V=v|\Theta=\theta)*f_{\Theta}(\theta)}{P(V=v)}
$$&lt;/p&gt;
&lt;p&gt;Este planteo permite comparar distintos modelos sin otra necesidad que
los valores de la matriz de confusión.&lt;/p&gt;
&lt;h3 id=&#34;el-prior&#34;&gt;&lt;em&gt;El prior&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;En analisis bayesiano hay distribuciones que van de la mano, que son
como amigas. Es decir, si mis variables tienen una funcion de
distribución determinada, los parametros tienen tal otra, se llaman
conjugados (en rigor el conjugado es que la posterior y la likelihood
pertenecen a la misma distribución)&lt;/p&gt;
&lt;p&gt;El amigo de la distribución multinomial, es la Dirichlet.&lt;/p&gt;
&lt;p&gt;En nuestro caso, &lt;em&gt;v&lt;/em&gt; sigue una distribución multinomial, y &lt;em&gt;θ&lt;/em&gt; una
distribución Dirichlet.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;V&lt;/em&gt; − &lt;em&gt;M&lt;strong&gt;u&lt;/strong&gt;l**t&lt;/em&gt;(&lt;em&gt;N&lt;/em&gt;&lt;sub&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;θ&lt;/em&gt;)
&lt;em&gt;Θ&lt;/em&gt; − &lt;em&gt;D&lt;strong&gt;i&lt;/strong&gt;r&lt;/em&gt;(&lt;em&gt;α&lt;/em&gt;)=&lt;em&gt;D&lt;strong&gt;i&lt;/strong&gt;r&lt;/em&gt;((&lt;em&gt;α&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;, &lt;em&gt;α&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;, &lt;em&gt;α&lt;/em&gt;&lt;sub&gt;3&lt;/sub&gt;, &lt;em&gt;α&lt;/em&gt;&lt;sub&gt;4&lt;/sub&gt;))&lt;/p&gt;
&lt;p&gt;Ahora bien, ¿Que es &lt;em&gt;α&lt;/em&gt;? Bueno, &lt;em&gt;α&lt;/em&gt; no es mas ni menos que el lugar
donde se mete el prior, porque dado mi matriz de confusión &lt;em&gt;v&lt;/em&gt;, la
posterior de &lt;em&gt;Θ&lt;/em&gt; esta dada por:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Θ&lt;/em&gt;|&lt;em&gt;ω&lt;/em&gt; = &lt;em&gt;D&lt;strong&gt;i&lt;/strong&gt;r&lt;/em&gt;((&lt;em&gt;v&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt; + &lt;em&gt;α&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;, &lt;em&gt;v&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt; + &lt;em&gt;α&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;, &lt;em&gt;v&lt;/em&gt;&lt;sub&gt;3&lt;/sub&gt; + &lt;em&gt;α&lt;/em&gt;&lt;sub&gt;3&lt;/sub&gt;, &lt;em&gt;v&lt;/em&gt;&lt;sub&gt;4&lt;/sub&gt; + &lt;em&gt;α&lt;/em&gt;&lt;sub&gt;4&lt;/sub&gt;)) = &lt;em&gt;D&lt;strong&gt;i&lt;/strong&gt;r&lt;/em&gt;(&lt;em&gt;ω&lt;/em&gt;)&lt;/p&gt;
&lt;p&gt;Entonces, habiendo visto los datos &lt;em&gt;v&lt;/em&gt;, y habiendo metido nuestro
conocimiento previo &lt;em&gt;α&lt;/em&gt; (que puede ser la matriz de confusión de otro
modelo, o del mismo modelo con nuevos datos, o el prior relevante al
caso concreto), y la posterior &lt;em&gt;D&lt;strong&gt;i&lt;/strong&gt;r&lt;/em&gt;(&lt;em&gt;ω&lt;/em&gt;) nos va a dar los
parametros que correspondan darle a la multinomial que nos da las
distribuciones de cada uno de los elementos de la matriz de confusión.&lt;/p&gt;
&lt;p&gt;¿Como se vuelve operativo esto?&lt;/p&gt;
&lt;p&gt;Simulando, si tenemos los parametros de &lt;em&gt;D&lt;strong&gt;i&lt;/strong&gt;r&lt;/em&gt;(&lt;em&gt;ω&lt;/em&gt;) como resultado de
nuestro analisis, no vamos a tener los &lt;em&gt;θ&lt;/em&gt;, sino que podemos extraer los
parametros de &lt;em&gt;θ&lt;/em&gt; con una frecuencia que refleje la distribución de
&lt;em&gt;D&lt;strong&gt;i&lt;/strong&gt;r&lt;/em&gt;(&lt;em&gt;ω&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Para simplificar podemos elegir una métrica a evaluar, en este caso, el
accuracy, que depende de matriz de confusión &lt;em&gt;A&lt;/em&gt;(&lt;em&gt;v&lt;/em&gt;)&lt;/p&gt;
&lt;p&gt;Esto es un algoritmo Monte Carlo&lt;/p&gt;
&lt;p&gt;&lt;em&gt;for i in 1:M:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Vamos a la caja &lt;em&gt;D&lt;strong&gt;i&lt;/strong&gt;r&lt;/em&gt;(&lt;em&gt;ω&lt;/em&gt;) y sacamos los &lt;em&gt;θ&lt;/em&gt;&lt;sub&gt;&lt;/em&gt;i&lt;em&gt;&lt;/sub&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Ponemos los &lt;em&gt;θ&lt;/em&gt;&lt;sub&gt;&lt;/em&gt;i&lt;em&gt;&lt;/sub&gt; en &lt;em&gt;P&lt;/em&gt;(&lt;em&gt;V&lt;/em&gt; = &lt;em&gt;v&lt;/em&gt;|&lt;/em&gt;Θ* = *θ*) y sacamos
los *v*&lt;sub&gt;*i*&lt;/sub&gt;*&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Sacamos &lt;em&gt;A&lt;/em&gt;(&lt;em&gt;v&lt;/em&gt;&lt;sub&gt;&lt;/em&gt;i&lt;em&gt;&lt;/sub&gt;) y lo guardamos en una lista&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Esto va a dar una lista de un montón (M) de A(v), este listado tiene sus
propios estadisticos, su media y su varianza.&lt;/p&gt;
&lt;p&gt;¿Confuso? Seguro, no te culpo, yo también lo estoy y eso que estoy
ecribiendo esto, pero veamos concretamente como se puede aplicar esto en
la práctica&lt;/p&gt;
&lt;h4 id=&#34;ejemplo-1comparando-dos-modelos&#34;&gt;*Ejemplo 1:*Comparando dos modelos&lt;/h4&gt;
&lt;p&gt;Supongamos que, de un mismo dataset, hicimos, ponele, un randomForest
(R) y un NaiveBayes (N) y la matriz de confusión de c/u es:&lt;/p&gt;
&lt;p&gt;$$
v^R=\begin{bmatrix}65 &amp;amp; 15 \\ 35 &amp;amp; 30\end{bmatrix} = (65,30,35,15)
$$
$$
v^N=\begin{bmatrix}50 &amp;amp; 30 \\ 30 &amp;amp; 35\end{bmatrix} = (50,35,30,30)
$$
Vamos a decir, que no tenemos información previa, porque lo que estamos
haciendo es comparar dos modelos, es decir que &lt;em&gt;α&lt;/em&gt; en ambos casos, será
(0, 0, 0, 0)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(DirichletReg)


library(extraDistr)


## The following objects are masked from &#39;package:DirichletReg&#39;:
## 
##     ddirichlet, rdirichlet

M = 1000
alpha = c(0,0,0,0)

#Para v_R
v_R = c(65,30,35,15)
omega_R = alpha + v_R
A_R = c()

for(i in 1:1000){
  theta_i = t(rdirichlet(1,omega_R))
  v_i = rmnom(1, sum(v_R), as.vector(theta_i))
  A_i = (v_i[1]+v_i[2])/sum(v_i)
  A_R = c(A_R,A_i)
}

#Para v_N
v_N = c(50,35,30,30)
omega_N = alpha + v_N
A_N = c()

for(i in 1:1000){
  theta_i = t(rdirichlet(1,omega_N))
  v_i = rmnom(1, sum(v_N), as.vector(theta_i))
  A_i = (v_i[1]+v_i[2])/sum(v_i)
  A_N = c(A_N,A_i)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ahora se pueden visualizar las distribuciones resultantesde estas
simulación&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/bayes_result.png&#34; width=&#34;400px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;En este caso, puede verse que el mejor modelo es el randomForest (R), ya
que su distribución está más a la derecha.&lt;/p&gt;
&lt;p&gt;De aquí puede extraerse también los intervalos de confianza y (de
credibilidad para el marco bayesiano) y otras cuestiones.&lt;/p&gt;
&lt;p&gt;Otra cosa para lo cual se puede usar este método es para evaluar de
forma muy barata cuan vigente es un model en funcionamiento a nueva
información.&lt;/p&gt;
&lt;p&gt;Si nos matamos haciendo un modelo que llego a un accuracy del 97% en el
set de test y un día con información nueva, llega a un accuracy del 90%,
es posible saber que tan probable sería encontrar tal resultado o si el
modelo necesita ser re-entrenado, es solo cuestión de que
&lt;em&gt;α&lt;/em&gt; = (&lt;em&gt;M**C&lt;/em&gt;&lt;sub&gt;0&lt;/sub&gt;) donde &lt;em&gt;M**C&lt;/em&gt;&lt;sub&gt;0&lt;/sub&gt; es la matriz de
confusión al momento 0.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/clusterizar-disenos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/clusterizar-disenos/</guid>
      <description>&lt;h1 id=&#34;clusterizar-diseños&#34;&gt;Clusterizar diseños&lt;/h1&gt;
&lt;p&gt;¿Como hago para clasificar estilos de banners?&lt;/p&gt;
&lt;p&gt;Con tantos adds dando vueltas en internet, sigue siendo, muchas veces,
un proceso relativamente manual y poco estandarizado los diseños. Para
eso existen diseñadores.&lt;/p&gt;
&lt;p&gt;Pero cuando ya tienen miles realizados, esta bueno mirar atrás e
identificar patrones recurrentes. Saber lo que venimos haciendo sirve
para mirar hacia adelante.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lucasenrich.netlify.com/post/clusterizar-disenos/&#34;&gt;Read more &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/banner-example.png&#34; width=&#34;400px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Input data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;El problema tenia originalmente cientos de diseños, en este ejemplo,
dado que tiene que ver con algoritmos de clusterización, se usaran
algunos hechos ad hoc, por lo que el ejemplo es con tamaños reducidos y
la clasificacion de imagenes, fuentes de texto y demás quedan afuera. Lo
que queremos saber es si la ubicación de los elementos en la imagen
sigue un patron en particular.&lt;/p&gt;
&lt;p&gt;Los datos venian de la siguiente forma:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/input_data.png&#34; width=&#34;65%&#34; style=&#34;float:left; padding:20px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Donde:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;font size = 3&gt; &lt;em&gt;y&lt;/em&gt; : Distancia desde arriba &lt;/font&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;font size = 3&gt; &lt;em&gt;x&lt;/em&gt; : Distancia desde la izquierda &lt;/font&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;font size = 3&gt; &lt;em&gt;w&lt;/em&gt; : Ancho (width) &lt;/font&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;font size = 3&gt; &lt;em&gt;h&lt;/em&gt; : Alto (height) &lt;/font&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Si tenemos 50 ejemplos, con 3 elementos cada uno, y hay 4 variables por
elemento, la forma del input es 50 × 3 × 4, esto, a los algoritmos, no
les gusta demasiado. Asique fue necesario achatar la base para obtener
una base de datos de 50 × 12, para lo cual:&lt;/p&gt;
&lt;p&gt;Primero se agarra cada uno (una base de 1 × 3 × 4) y se la transforma en
una sola fila 1 × 12 para lo cual se uso el código:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(tidyverse)
cols_used = c(&#39;element_top&#39;, &#39;element_left&#39;, &#39;element_width&#39;, &#39;element_height&#39;)
spread_file&amp;lt;-function(data, cols_used){
  cols_used_a = c(&#39;element_name&#39;,cols_used)
  y=data[cols_used]
  h = data[cols_used_a]
  z=c(1,1,1,1)
  for(i in 1:nrow(y)) {
    z = cbind(z,y[i,])
  }
  z = z[1,-1] 
  
  newcols &amp;lt;- c()
  for (i in  h[&#39;element_name&#39;]){
    newcols&amp;lt;-cbind(newcols,paste(i,cols_used[1], sep = &#39;.&#39;))
    newcols&amp;lt;-cbind(newcols,paste(i,cols_used[2], sep = &#39;.&#39;))
    newcols&amp;lt;-cbind(newcols,paste(i,cols_used[3], sep = &#39;.&#39;))
    newcols&amp;lt;-cbind(newcols,paste(i,cols_used[4], sep = &#39;.&#39;))
  }
  newcols2&amp;lt;-c()
  for(i in 1:nrow(newcols)) {
    for(j in 1:4){
      newcols2&amp;lt;-c(newcols2,newcols[i,j])
    }
  }
  colnames(z)&amp;lt;-newcols2
  n&amp;lt;-as_vector(data[&#39;id&#39;])
  z[&#39;id&#39;]&amp;lt;-n[1]
  z
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lo que transforma cada elementos con la forma:&lt;/p&gt;
&lt;p&gt;$$\begin{bmatrix} elem1 &amp;amp; y_1 &amp;amp; x_1 &amp;amp; w_1 &amp;amp; h_1 \\   elem2 &amp;amp; y_2 &amp;amp; x_2 &amp;amp; w_2 &amp;amp; h_2  \\   \vdots \\   elemk &amp;amp; y_2 &amp;amp; x_k &amp;amp; w_k &amp;amp; h_k \end{bmatrix}$$
a la forma:&lt;/p&gt;
&lt;p&gt;$$\begin{bmatrix}
id.1 &amp;amp; elem.1.x &amp;amp; elem.1.y &amp;amp; elem.1.h &amp;amp; elem.1.w &amp;amp; &amp;hellip; &amp;amp; elem.k.w
\end{bmatrix}$$
Así se pueden apilar todos elementos de la muestra para quedar una sola
base de datos con la forma:&lt;/p&gt;
&lt;p&gt;$$\begin{bmatrix} id.1 &amp;amp; elem.1.x &amp;amp; elem.1.y &amp;amp; elem.1.h &amp;amp; elem.1.w &amp;amp; &amp;hellip; &amp;amp; elem.k.w \\ id.2 &amp;amp; elem.1.x &amp;amp; elem.1.y &amp;amp; elem.1.h &amp;amp; elem.1.w &amp;amp; &amp;hellip; &amp;amp; elem.k.w \\ \vdots \\ id.N &amp;amp; elem.1.x &amp;amp; elem.1.y &amp;amp; elem.1.h &amp;amp; elem.1.w &amp;amp; &amp;hellip; &amp;amp; elem.k.w \end{bmatrix}$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;font size = 3&gt; Reduccion de dimensionalidad + Clustering &lt;/font&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;La forma más directa para realizar la tarea de clusterización es
simplemente usando el paquete &lt;strong&gt;dbscan&lt;/strong&gt; y correrlo sobre nuestra base
transformada. Pero no funcionó del todo. Asique lo siguiente fue reducir
la dimensionalidad de los objetos, en este caso se uso el algoritmo UMAP
y luego se hizo la clusterización.&lt;/p&gt;
&lt;p&gt;PCA (componentes principales), es el algoritmo más popular para reducir
la dimensionalidad. Luego esta t-SNE.&lt;/p&gt;
&lt;p&gt;En pocas palabras, UMAP fue la mejor opción porque utiliza un algoritmo
rapido que preserva mejor la estructura global.&lt;/p&gt;
&lt;p&gt;Y finalmente:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(umap)
library(dbscan)
umap_data&amp;lt;- umap(data)
cl &amp;lt;-hdbscan(x = umap_data, minPts = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pero también habia un tema de escala, quiza el diseño era parecido pero
un banner era dos veces mas grande, por lo que el resultado no era
adecuado.&lt;/p&gt;
&lt;p&gt;&lt;font size = 3&gt; Surge la necesidad de transformar los datos&lt;/p&gt;
&lt;p&gt;Opciones &lt;/font&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;font size = 2&gt; Estandarizacion (z-score): Representa el numero de
desvios estandar arriba o debajo del valor resultante. &lt;strong&gt;Útil para
variables normalmente distribuidas&lt;/strong&gt; &lt;/font&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;font size = 2&gt; Normalizacion (min-max scaler): Permite llevar los
valores entre 0 y 1. &lt;strong&gt;Útil para comparar variables de diferentes
ordenes de magnitud&lt;/strong&gt; (Precio de una casa y los m2 que ocupa)
&lt;/font&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/img/normaliz_data.png&#34; width=&#34;65%&#34; style=&#34;float:left; padding:20px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;¿Puedo usar estas transformaciones en estos datos?&lt;/strong&gt;&lt;/p&gt;
&lt;font size = 4&gt;
&lt;ul&gt;
&lt;li&gt;No, como las variables describen dimensiones (alto y ancho), y
posicion en el espacio no le encontré mucho sentido a la
estandarizacion ni la normalizacion.&lt;/li&gt;
&lt;/ul&gt;
&lt;/font&gt;
&lt;font size = 4&gt;
&lt;ul&gt;
&lt;li&gt;¿Que podría hacer? En lugar de ver las posiciones y dimensiones
&lt;em&gt;absolutas&lt;/em&gt;, ver las posiciones y dimensiones &lt;em&gt;relativas&lt;/em&gt;, lo que
voy a llamar &amp;ldquo;normalizacion geometrica&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- --&gt;
&lt;pre&gt;&lt;code&gt;normalize_geometric&amp;lt;-function(df){
  df[&#39;total_area&#39;]&amp;lt;-max(df[&#39;element_height&#39;])*max(df[&#39;element_width&#39;])

  df[&#39;rel_area&#39;]&amp;lt;-df[&#39;element_height&#39;]*df[&#39;element_width&#39;]/df[&#39;total_area&#39;]
  
  df[&#39;orientation&#39;]&amp;lt;-df[&#39;element_height&#39;]/df[&#39;element_width&#39;]
  
  df[&#39;element_top_relative&#39;]&amp;lt;-df[&#39;element_top&#39;]/max(df[&#39;element_height&#39;])
  
  df[&#39;element_left_relative&#39;]&amp;lt;-df[&#39;element_left&#39;]/max(df[&#39;element_width&#39;])
  
  df
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/font&gt;
&lt;ul&gt;
&lt;li&gt;x&amp;rsquo; es la proporcion de x respecto al rango total (ancho del canvas)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;font size = 3&gt; &lt;em&gt;mi nueva variable x&amp;rsquo; es: la linea roja dividida la
linea azul&lt;/em&gt; &lt;/font&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/x_demo_plot.jpeg&#34; width=&#34;65%&#34; style=&#34;float:left; padding:20px&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;y&amp;rsquo; es la proporcion de y respecto al rango total (alto del canvas)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;font size = 3&gt; &lt;em&gt;mi nueva variable y&amp;rsquo; es: la linea roja dividida la
linea azul&lt;/em&gt; &lt;/font&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/demo_plot_y.jpeg&#34; width=&#34;65%&#34; style=&#34;float:left; padding:20px&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;areaRelativa es la proporcion del area del elemento respecto al
total&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;font size = 3&gt; &lt;em&gt;mi nueva variable areaRelativa es: el area del cuadrado
chiquito dividido la del rectangulo grande&lt;/em&gt; &lt;/font&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/area_plot.jpeg&#34; width=&#34;65%&#34; style=&#34;float:left; padding:20px&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;disposicion (dividiendo alto por acho) es para saber si el elemento
es horizontal, vertical, o cuadrado&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;font size = 3&gt; &lt;em&gt;mi nueva variable disposicion es: el alto dividido por
el ancho&lt;/em&gt; &lt;/font&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/rectangular.png&#34; width=&#34;30%&#34; style=&#34;float:center; padding:0% 35%&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;resultados&#34;&gt;Resultados&lt;/h2&gt;
&lt;p&gt;Para empezar a evaluar los resultados, todo &amp;ldquo;spread&amp;rdquo; tiene que tener su
&amp;ldquo;gather&amp;rdquo;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gather_file&amp;lt;-function(gdf){
  x&amp;lt;-strsplit(colnames(gdf), &#39;\\.&#39;)
  
  element_name=unique(unlist(map(x, 1)))
  original_cols=unique(unlist(map(x, 2)))
  gdf1&amp;lt;-data.frame(element_name)
  gdf1[original_cols[1]]&amp;lt;-0
  gdf1[original_cols[2]]&amp;lt;-0
  gdf1[original_cols[3]]&amp;lt;-0
  gdf1[original_cols[4]]&amp;lt;-0
  
  
  
  rel_area&amp;lt;-c()
  orientation&amp;lt;-c()
  element_top_relative&amp;lt;-c()
  element_left_relative&amp;lt;-c()
  
  for(i in seq(from=1, to=length(gdf), by=4)){
    #  stuff, such as
    rel_area=c(rel_area,gdf[i])
    orientation=c(orientation,gdf[i+1])
    element_top_relative = c(element_top_relative,gdf[i+2])
    element_left_relative = c(element_left_relative,gdf[i+3])
  }
  
  gdf1[&#39;rel_area&#39;]=as_vector(unlist(rel_area))
  gdf1[&#39;orientation&#39;]=as_vector(unlist(orientation))
  gdf1[&#39;element_top_relative&#39;]=as_vector(unlist(element_top_relative))
  gdf1[&#39;element_left_relative&#39;]=as_vector(unlist(element_left_relative))
  gdf1}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;En principio veamos como quedaron los grupos sin normalizar y con la
normalización&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/unnamed-chunk-12-1.png&#34; width=&#34;30%&#34; style=&#34;float:center; padding:0% 35%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Y finalmente, un par de ejemplos por grupo:&lt;/p&gt;
&lt;p&gt;Primer cluster:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/c1img.jpeg&#34; width=&#34;30%&#34; style=&#34;float:center; padding:0% 35%&#34; /&gt;
000003.png&lt;/p&gt;
&lt;p&gt;Segundo cluster:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cl2img.jpeg&#34; width=&#34;30%&#34; style=&#34;float:center; padding:0% 35%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Tercero:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cl3img.jpeg&#34; width=&#34;30%&#34; style=&#34;float:center; padding:0% 35%&#34; /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/dedupl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/dedupl/</guid>
      <description>&lt;h1 id=&#34;de-duplicación-de-avisos&#34;&gt;De-duplicación de Avisos&lt;/h1&gt;
&lt;p&gt;Cualquiera que haya hecho un curso introductorio o leído un libro de
Ciencia de Datos sabe que se dice mucho que la limpieza de un dataset
es, por lo menos el 80% del trabajo y luego el modelado. Pero eso no
quiere decir que no hagan falta técnicas propias del modelado para
limpiar un dataset.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lucasenrich.netlify.com/post/dedupl/&#34;&gt;Read more &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Con un scrap hecho a ZonaProp, de las unidades en alquiler en Recoleta
en febrero 2020, luego de una exahustiva limpieza de unificación de la
moneda, de corrección de datos implícitos (NA&amp;rsquo;s que implicaban un dato,
por ejemplo, cocheras-si es NA, cocheras=0).&lt;/p&gt;
&lt;p&gt;El problema que quedó fue que el resultado es una base de datos de
&lt;em&gt;avisos&lt;/em&gt;, pero, ¿que pasa si yo quiero que mi base de datos sea de
&lt;em&gt;inmuebles&lt;/em&gt;? El problema que me encuentro es que hay avisos duplicados,
ya sea porque vuelven a estar publicados o porque un mismo inmueble es
publicado por más de una inmobiliaria.&lt;/p&gt;
&lt;p&gt;¿Cómo se pueden detectar sistematicamente esas duplicaciones? Debajo hay
un método posible&lt;/p&gt;
&lt;h3 id=&#34;los-datos&#34;&gt;Los Datos&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;library(tidyverse)
alqs&amp;lt;-read_csv(&amp;quot;alq_feb20_recoleta.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bien, este es el dataset. Una primera idea sería: si dos publicaciones
se parecen lo suficiente, sospechamos que son la misma.&lt;/p&gt;
&lt;p&gt;Las columnas son&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Columna&lt;/th&gt;
&lt;th&gt;Descrpción&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;WEB&lt;/td&gt;
&lt;td&gt;La url de la publicación&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Provincia&lt;/td&gt;
&lt;td&gt;La provincia de la publicación&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Tipo_Op&lt;/td&gt;
&lt;td&gt;Si corresponde a venta o alquiler&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Tipo&lt;/td&gt;
&lt;td&gt;Si es casa, comercio,oficina, PH, departamento, etc&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Zona&lt;/td&gt;
&lt;td&gt;El barrio, para CABA, el municipio para las provincias&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Dirección&lt;/td&gt;
&lt;td&gt;La dirección de la publicación&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Latitud y Longittud&lt;/td&gt;
&lt;td&gt;Georreferencia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Inmobiliaria&lt;/td&gt;
&lt;td&gt;Quien está publicando&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Tiempo&lt;/td&gt;
&lt;td&gt;Cuanto tiempo lleva publicado en días&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Cochera&lt;/td&gt;
&lt;td&gt;Si tiene cochera, boolean&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Expensas&lt;/td&gt;
&lt;td&gt;Cuanto se paga de expensas&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Prices&lt;/td&gt;
&lt;td&gt;El precio de venta/alquiler&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Antigüedad&lt;/td&gt;
&lt;td&gt;Cuantos años tiene de construido&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Metros&lt;/td&gt;
&lt;td&gt;Tamaño en metros cuadrados&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Ambientes&lt;/td&gt;
&lt;td&gt;Cantidad de Ambientes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Descripción&lt;/td&gt;
&lt;td&gt;El texto de la descripción&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Baños&lt;/td&gt;
&lt;td&gt;Cantidad de baños&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Ahora bien, cualquier procedimiento de detección de duplicados requiere
necesariamente cierta flexibilidad, sino buscamos aquellos identicos y
listo; pero la realidad es que la duplicación en general va a tener un
motivo en concreto, sea por corrección de algún dato en particular, sea
por ponerlo en otra inmobiliaria, sea por cambio de la descripción para
que sea más atractiva. Pero por otro lado, revisar todos los pares
posible nos va a llevar a que, si &lt;em&gt;N&lt;/em&gt; es la cantidad de publicaciones en
el dataset, los chequeos sean &lt;em&gt;N&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;. Manualmente, esto, es
inviable.&lt;/p&gt;
&lt;p&gt;Entonces, lo que se puede hacer es:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ver la tasa de variación de las variables numéricas.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Verificar la distancia (aprovechando que estan georreferenciados)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ver cuanto se parecen las descripciones&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tasa-de-variacion-de-las-variables-numericas&#34;&gt;Tasa de variacion de las variables numericas&lt;/h3&gt;
&lt;p&gt;Una opción sería considerar solo las variables que son iguales, pero eso
descarta la posibilidad que haya una corrección en los datos, ni hablar
si alguno de los datos se considera missing (&lt;em&gt;N**A&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Una cuestión antes de arrancar con esta sección, se estará generando una
matriz simétrica para cada una de estas variables númericas. En la cual
&lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i**j&lt;/em&gt;&lt;/sub&gt; va a ser la tasa de variación entre las
observaciones; y por lo tanto la diagonal cuando &lt;em&gt;i&lt;/em&gt; = &lt;em&gt;j&lt;/em&gt;,
&lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i**j&lt;/em&gt;&lt;/sub&gt; = 0&lt;/p&gt;
&lt;p&gt;Empecemos&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pmx&amp;lt;-matrix(nrow=nrow(alqs1), ncol=nrow(alqs1))
for(i in 1:nrow(alqs)){
  for(j in 1:nrow(alqs)){
    h= abs(alqs3$Prices[i]-alqs3$Prices[j])/max(alqs3$Prices[i],alqs3$Prices[j], na.rm = T)
    pmx[i,j] = h
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;¿Qué está mal con esta aproximación? Que es un loop anidado, por lo que
el Big O, es cuadrático, la forma &lt;em&gt;menos&lt;/em&gt; eficiente de llenar una
matriz. Esto quiere decir, que el tiempo que tarda el terminar este loop
depende cuadráticamente del tamaño de las filas.&lt;/p&gt;
&lt;p&gt;Se puede aprovechar las operaciones vectorizadas que tiene R-Base para
quedarnos con una Big O lineal.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pmx&amp;lt;-matrix(nrow=nrow(alqs1), ncol=nrow(alqs1))
for(i in 1:nrow(alqs)){
  pmx[i,] = abs(alqs3$Prices[i]-alqs3$Prices)/max(alqs3$Prices[i],alqs3$Prices, na.rm = T)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Esto es mejor, pero R tiene su propio diálecto para tratar con loops y
es la facilidad que tiene para trabajar vectorialmente y la familia de
funciones de &lt;em&gt;apply&lt;/em&gt;, y de paso ponemos en práctica la máxima de &lt;em&gt;evitar
los loops&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rel_dif = function(a,b){
  abs(a-b)/max(a,b, na.rm = T)
}
pmx&amp;lt;-sapply(FUN = rel_dif, alqs1$Prices,alqs1$Prices)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ahora que podemos obtener la matriz de diferencias con una sola linea de
código, hagamoslo para todoas las variables númericas.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mts.mx&amp;lt;-sapply(FUN = rel_dif, alqs1$Metros,alqs1$Metros) #Metros cuadrados

antig.pmx&amp;lt;-sapply(FUN = rel_dif, alqs1$Antiguedad,alqs1$Antiguedad) #Antigüedad
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Solo una cosa, lo que queremos es ver cuan cerca están, pero hasta ahora
vimos la variación, por lo que la cercacia va a estar dada por:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pmx&amp;lt;-1-as.matrix(pmx)
pmts&amp;lt;-1-as.matrix(pmts)
pantg&amp;lt;-1-as.matrix(pantg)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;distancia-geográfica&#34;&gt;Distancia geográfica&lt;/h4&gt;
&lt;p&gt;Para calcular la distancia geográfica vamos a estar necesitando el
paquete &lt;em&gt;geosphere&lt;/em&gt; para lo cual se usa el criterio de la distancia
entre dos puntos en una superficie esférica, llamada la distancia
Haversine.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;latlongs&amp;lt;-alqs1 %&amp;gt;% select(Latitud,Longitud)
dm &amp;lt;- distm(latlongs,latlongs, fun=distHaversine)  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;La función llamada &lt;strong&gt;distm&lt;/strong&gt; es realmente útil porque viene con una
implementación vectorizada,lo cual soluciona de entrada el problema de
los potenciales loops anidados que teniamos antes.&lt;/p&gt;
&lt;h4 id=&#34;descripción&#34;&gt;Descripción&lt;/h4&gt;
&lt;p&gt;Bueno, llegamos al punto álgido, y por esto me refiero a que el criterio
para determinar que tan cerca están dos descripciones no es tan obvio
como la distancia o la variación relativa de alguna variable, porque
estamos tratando ahora con variables no numéricas; por lo tanto antes de
realizar la comparación necesitamos convertir a ese texto en un vector
numérico, para lo cual existen varias vías.&lt;/p&gt;
&lt;p&gt;La utilizada en este caso fue &lt;em&gt;T**F&lt;/em&gt; − &lt;em&gt;I&lt;strong&gt;D&lt;/strong&gt;F&lt;/em&gt; que significa &lt;strong&gt;&amp;ldquo;term
frequency–inverse document frequency,&amp;quot;&lt;/strong&gt;, el cual genera un vector
numérico ponderado por la importancia de cada palabra en el texto (term
frequency) y la frecuencia de la palabra en todos los textos (inverse
document frequency)&lt;/p&gt;
&lt;p&gt;Es decir, la ponderación indica que no todas la palabras tienen el mismo
peso para describir una descripción, sino que hay algunas que deben
ponderarse más y otras que deben ponderarse menos. En el caso de una
descripción de un inmueble, las palabras que son más comunes a todas las
descripciones, como pueden ser &amp;ldquo;baño&amp;rdquo; ó &amp;ldquo;cocina&amp;rdquo;, son ponderadas menos
que las palabras que son menos frecuentes a cada descripción, como &amp;ldquo;a
tres cuadras de la estacion Primera Junta&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Este score esta definido por
$$
TFIDF_{xy} = TF_{xy}*log\frac{N}{df}
$$
donde:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;T**F&lt;/em&gt;&lt;sub&gt;&lt;em&gt;x**y&lt;/em&gt;&lt;/sub&gt; es la frecuencia de la palabra &lt;em&gt;x&lt;/em&gt; en la
descripción &lt;em&gt;y&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;N&lt;/em&gt; es el número total de descripciones&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;d**f&lt;/em&gt; es el número total de documentos que contienen la palabra &lt;em&gt;x&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Por suerte, python tienen una libreria para (casi)todo, por lo que esta
vectorización require solo unas pocas lineas de código;
desafortunadamente, el texto está &amp;ldquo;sucio&amp;rdquo; y debe ser limpiado antes de
la vectorización.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#Eliminar caracteres especiales y espacios innecesarios
cleanFun &amp;lt;- function(htmlString) {
  #Saco los tags de html
  t=(gsub(&amp;quot;\t&amp;quot;,&amp;quot;&amp;quot;,gsub(&amp;quot;\n&amp;quot;,&amp;quot;&amp;quot;,gsub(&amp;quot;&amp;lt;.*?&amp;gt;&amp;quot;, &amp;quot;&amp;quot;, htmlString))))
  #Lo paso a minuscula
  t=tolower(t)
  #Le saco los caracteres no alfanumericos
  t=str_replace_all(t, &amp;quot;[[:punct:]]&amp;quot;, &amp;quot; &amp;quot;)
  t
  
}
#Tokenización y eliminación de stopwords
prepTx &amp;lt;- function(tx){
  t1=word_tokenizer(cleanFun(tx))
  t2 = t1[!(t1%in%stopwords(kind=&amp;quot;es&amp;quot;))]
  t2 = unlist(t2)[nchar(unlist(t2))&amp;gt;2]
  t2
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, ahora que el text está limpio, es hora de generar la conversión a
numeros&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(tcR)
prep_fun &amp;lt;- cleanFun
tok_fun &amp;lt;- word_tokenizer
smp_size&amp;lt;-floor(0.75*length(descripciones))
set.seed(123)
train_ind &amp;lt;- sample(seq_len(length(descripciones)), size = smp_size)
train &amp;lt;- descripciones
it_train &amp;lt;- itoken(train, 
                   preprocessor = prep_fun, 
                   tokenizer = tok_fun,
                   progressbar = TRUE)
vocab &amp;lt;- create_vocabulary(it_train)
pruned_vocab = prune_vocabulary(vocab, term_count_min = 100,
                                doc_proportion_max = 0.5, doc_proportion_min = 0.001)
vectorizer &amp;lt;- vocab_vectorizer(pruned_vocab)
dtm_train &amp;lt;- create_dtm(it_train, vectorizer)

tfidf = TfIdf$new()
dtm_transformed = tfidf$fit_transform(dtm_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ahora &lt;em&gt;dtm_transformed&lt;/em&gt; es nuestra variable que contiene los vectores
numericos para cada descripción. Como son vectores, una forma sencilla
de compararlos es usando la distancia coseno.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;d1_d2_tfidf_cos_sim = sim2(x = dtm_transformed, method = &amp;quot;cosine&amp;quot;, norm = &amp;quot;l2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Y ahora si, nuestra matriz de similitud &lt;em&gt;d1_d2_tfidf_cos_sim&lt;/em&gt; nos
permite comparar las descripciones.&lt;/p&gt;
&lt;h4 id=&#34;unificación-de-las-matrices-de-similitud&#34;&gt;Unificación de las matrices de similitud&lt;/h4&gt;
&lt;p&gt;Ahora que tenemos todas las matrices que comparan todos los registros
con todos los demás, es hora de unificarlas.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;A&amp;lt;-pmx
B&amp;lt;-pmts
C&amp;lt;-pantg
D&amp;lt;-d1_d2_tfidf_cos_sim
E&amp;lt;-dm

colnames(A)&amp;lt;-alqs3$id
rownames(A)&amp;lt;-alqs3$id

colnames(B)&amp;lt;-alqs3$id
rownames(B)&amp;lt;-alqs3$id

colnames(C)&amp;lt;-alqs3$id
rownames(C)&amp;lt;-alqs3$id

colnames(D)&amp;lt;-alqs3$id
rownames(D)&amp;lt;-alqs3$id

colnames(E)&amp;lt;-alqs3$id
rownames(E)&amp;lt;-alqs3$id


Aa&amp;lt;-as.data.frame(as.table(A)) %&amp;gt;% distinct(Var1,Var2, .keep_all = T)
Bb&amp;lt;-as.data.frame(as.table(B)) %&amp;gt;% distinct(Var1,Var2, .keep_all = T)
Cc&amp;lt;-as.data.frame(as.table(C)) %&amp;gt;% distinct(Var1,Var2, .keep_all = T)
Dd&amp;lt;-as.data.frame(as.table(D)) %&amp;gt;% distinct(Var1,Var2, .keep_all = T)
Ee&amp;lt;-as.data.frame(as.table(E)) %&amp;gt;% distinct(Var1,Var2, .keep_all = T)


colnames(Aa)[3]=&#39;precios&#39;
colnames(Bb)[3]=&#39;metros&#39;
colnames(Cc)[3]=&#39;antiguedad&#39;
colnames(Dd)[3]=&#39;descripcion&#39;
colnames(Ee)[3]=&#39;dist_mts&#39;

AB&amp;lt;-right_join(Aa,Bb, by = c(&#39;Var1&#39;,&#39;Var2&#39;))
CD&amp;lt;-right_join(Cc,Dd, by = c(&#39;Var1&#39;,&#39;Var2&#39;))
ABCD&amp;lt;-right_join(AB,CD,by = c(&#39;Var1&#39;,&#39;Var2&#39;))
ABCDE&amp;lt;-right_join(ABCD,Ee,by = c(&#39;Var1&#39;,&#39;Var2&#39;))

ABCDE&amp;lt;-ABCDE %&amp;gt;% filter(Var1!=Var2) %&amp;gt;% as_tibble()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ahora &lt;em&gt;ABCDE&lt;/em&gt; es nuestra dataframe con todos los pares de &lt;em&gt;i**d&lt;/em&gt;′&lt;em&gt;s&lt;/em&gt;, y
las columnas con cada criterio de similitud se agregan en la misma fila&lt;/p&gt;
&lt;p&gt;Lo que sigue es un poco arbitrario y seguro hay mejores métodos para
hacerlo. Pero nuestros candidatos a duplicados son aquellos que,
decimos, cumplen los siguientes criterios (en simultaneo):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Estan a menos de 300mts entre sí.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tienen una variación en Metros menor al 10%&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tienen una variación en Precio menor al 25%&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tienen una variación en la descripcion menor al 30%&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tienen una variación en antigüedad menor al 10%&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;En todos los casos se mantienen los resultados &lt;em&gt;NA&lt;/em&gt; porque eso nos
indica que podría haberse agregado el dato.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cand_dupls&amp;lt;-ABCDE %&amp;gt;% filter(dist_mts&amp;lt;=300 | is.na(dist_mts)) %&amp;gt;% 
  filter(metros&amp;gt;0.9| is.na(metros)) %&amp;gt;% filter(precios&amp;gt;0.75| is.na(precios)) %&amp;gt;% 
  filter(descripcion&amp;gt;0.7| is.na(descripcion))%&amp;gt;% filter(antiguedad&amp;gt;0.90| is.na(antiguedad))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;El resultado de esto es de 840 pares que son potencialmente duplicados.
Lo cual es una reducción bastante drástica de los 1716&lt;sup&gt;2&lt;/sup&gt; que
llevaría revisar todos los registros de una base de 1716 registros.&lt;/p&gt;
&lt;p&gt;Finalmente, hay un paso más que puede hacerse para que el proceso sea
más robusto y es considerar la propiedad transitiva de los pares,
realizar un network analysis, lo que significa agrupar todos aquellos
id&amp;rsquo;s que tienen suficiente similitud, para eso generamos una variable
que sea 1 para aquellos pares que cumplen los criterios y 0 para
aquellos que no:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cand_dupls&amp;lt;-cand_dupls %&amp;gt;% mutate(
  potdup = case_when(
   (precios &amp;gt; 0.75 |  is.na(precios)) &amp;amp; (metros &amp;gt; 0.9 |  is.na(metros)) &amp;amp; 
     (antiguedad &amp;gt; 0.9 |  is.na(antiguedad)) &amp;amp; (descripcion &amp;gt; 0.9 |  is.na(descripcion)) &amp;amp;
     (dist_mts&amp;lt;300 | is.na(dist_mts)) ~ 1,
   T ~ 0
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lo que tenemos ahora, es un dataframe que tiene los pares que creemos
que son duplicados, estos van a ser nuestros vinculos en el analisis de
red, los &lt;em&gt;edges&lt;/em&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;edges&amp;lt;-cand_dupls %&amp;gt;% select(Var1, Var2, potdup)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Que tienen esta forma&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;ID1&lt;/th&gt;
&lt;th&gt;ID2&lt;/th&gt;
&lt;th&gt;match&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;45557690&lt;/td&gt;
&lt;td&gt;44375852&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;44349226&lt;/td&gt;
&lt;td&gt;45589067&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;45620618&lt;/td&gt;
&lt;td&gt;44714730&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Que es la forma que acepta el paquete igraph para generar el grafo que
va a vincular el id como en la imagen debajo, donde&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/nodes.png&#34; width=&#34;400px&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;6,9,10 y 4 serían un anuncio separado del resto, por ejemplo&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- --&gt;
&lt;pre&gt;&lt;code&gt;library(igraph)
g &amp;lt;- graph_from_data_frame(edges)
fc &amp;lt;- fastgreedy.community(as.undirected(g))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ahora fc es una lista en el que cada elemento es un vector de id&amp;rsquo;s que
corresponderían al mismo inmueble para ver por ejemplo el grupo 3
podemos hacer:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;alqs %&amp;gt;% filter(id%in%  as.numeric(fc[[3]])) %&amp;gt;% View()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Los links asociados al grupo 3, entonces, son:&lt;/p&gt;
&lt;p&gt;[1]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/arenales-y-callao-excelente-edificio-de-estilo-44714730.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/arenales-y-callao-excelente-edificio-de-estilo-44714730.html&lt;/a&gt;&amp;rdquo;&lt;br&gt;
[2]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/arenales-1700-43499494.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/arenales-1700-43499494.html&lt;/a&gt;&amp;rdquo;
[3]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/petit-hotel-arenales-y-callao-700-m-sup2--lote-propio-45138045.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/petit-hotel-arenales-y-callao-700-m-sup2--lote-propio-45138045.html&lt;/a&gt;&amp;rdquo;
[4]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/petit-hotel-arenales-y-callao-700-m-sup2--lote-propio-45564366.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/petit-hotel-arenales-y-callao-700-m-sup2--lote-propio-45564366.html&lt;/a&gt;&amp;rdquo;
[5]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/petit-hotel-arenales-y-callao-700-m-sup2--lote-propio-45564348.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/petit-hotel-arenales-y-callao-700-m-sup2--lote-propio-45564348.html&lt;/a&gt;&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Al grupo 10:&lt;/p&gt;
&lt;p&gt;[1]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/piso-pueyrredon-y-guido-191-m-sup2--una-o-dos-44047718.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/piso-pueyrredon-y-guido-191-m-sup2--una-o-dos-44047718.html&lt;/a&gt;&amp;rdquo;&lt;br&gt;
[2]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/pueyrredon-y-guido-piso-191-m-sup2--1-o-2-cocheras-44748468.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/pueyrredon-y-guido-piso-191-m-sup2--1-o-2-cocheras-44748468.html&lt;/a&gt;&amp;rdquo;
[3]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/piso-pueyrredon-y-guido-191-m-sup2--una-o-dos-45455991.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/piso-pueyrredon-y-guido-191-m-sup2--una-o-dos-45455991.html&lt;/a&gt;&amp;rdquo;&lt;br&gt;
[4]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/pueyrredon-2400-45078667.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/pueyrredon-2400-45078667.html&lt;/a&gt;&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Pero no todo es un éxito, tambien existen grupos tales como el 1, en el
que hay más de uno:&lt;/p&gt;
&lt;p&gt;[1]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/en-alquiler-temporario-departamentos-tipo-lofts-de-47-40313161.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/en-alquiler-temporario-departamentos-tipo-lofts-de-47-40313161.html&lt;/a&gt;&amp;rdquo;
[2]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/departamentos-en-alquiler-temporario-posadas-1300-40075639.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/departamentos-en-alquiler-temporario-posadas-1300-40075639.html&lt;/a&gt;&amp;rdquo;&lt;br&gt;
[3]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/alquiler-loft-m-sup2--47-posadas-1323-recoleta-amobl-y-41937518.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/alquiler-loft-m-sup2--47-posadas-1323-recoleta-amobl-y-41937518.html&lt;/a&gt;&amp;rdquo;
[4]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/gran-oport-recoleta-1-amb-47-m-sup2--piso-alto-en-41324130.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/gran-oport-recoleta-1-amb-47-m-sup2--piso-alto-en-41324130.html&lt;/a&gt;&amp;rdquo;&lt;br&gt;
[5]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/venta-1amb-al-frente-m-sup2--47-balcon-vista-a-los-41324195.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/venta-1amb-al-frente-m-sup2--47-balcon-vista-a-los-41324195.html&lt;/a&gt;&amp;rdquo;&lt;br&gt;
[6]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/venta-gran-oport-posadas-1323-1-amb-m-sup2--47-balcon-41163069.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/venta-gran-oport-posadas-1323-1-amb-m-sup2--47-balcon-41163069.html&lt;/a&gt;&amp;rdquo;
[7]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/departamento-tipo-lofts-m-sup2--47-amoblados-y-41242674.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/departamento-tipo-lofts-m-sup2--47-amoblados-y-41242674.html&lt;/a&gt;&amp;rdquo;&lt;br&gt;
[8]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/posadas-1323-edificio-alquiler-temporal-departamentos-41683158.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/posadas-1323-edificio-alquiler-temporal-departamentos-41683158.html&lt;/a&gt;&amp;rdquo;
[9]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/loft-al-frente.-amueblado-y-decoracion-de-diseno-42984563.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/loft-al-frente.-amueblado-y-decoracion-de-diseno-42984563.html&lt;/a&gt;&amp;rdquo;&lt;br&gt;
[10]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/posadas-1323-alquiler-depto-amobl.-y-equip-lofts-41775461.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/posadas-1323-alquiler-depto-amobl.-y-equip-lofts-41775461.html&lt;/a&gt;&amp;rdquo;&lt;br&gt;
[11]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/venta-y-alquiler-temporal-apartments-amoblados-41242680.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/venta-y-alquiler-temporal-apartments-amoblados-41242680.html&lt;/a&gt;&amp;rdquo;&lt;br&gt;
[12]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/venta-loft-m-sup2--47-balcon-al-frente.-o-en-alquiler-40116759.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/venta-loft-m-sup2--47-balcon-al-frente.-o-en-alquiler-40116759.html&lt;/a&gt;&amp;rdquo;
[13]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/posadas-1323-lofts-m-sup2--47-amobl-y-equip-confort-41768916.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/posadas-1323-lofts-m-sup2--47-amobl-y-equip-confort-41768916.html&lt;/a&gt;&amp;rdquo;&lt;br&gt;
[14]
&amp;ldquo;&lt;a href=&#34;https://www.zonaprop.com.ar/propiedades/posadas-1323-apart-hotel-departamento-41682584.html&#34;&gt;https://www.zonaprop.com.ar/propiedades/posadas-1323-apart-hotel-departamento-41682584.html&lt;/a&gt;&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Pero en todo caso, esto efectivamente detecta gran parte de los
duplicados y puede asistir a que un ser humano genere la identificación
de duplicados.&lt;/p&gt;
&lt;p&gt;Esto es un ejemplo de aprendizaje no-supervisado, pero si este analisis
se llevara a fondo obtendríamos un set de datos de duplicaciones
etiquetadas, lo cual podría ser la base para el analisis con
procedimientos de analisis supervisados, y así encontrar patrones de que
causa la duplicaciones; lo cual mejoraría el proceso de identificación
de duplicados y podría usarse también para optimizar los criterios
subjetivos que utilicé mas arriba.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/mardel_realstate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/mardel_realstate/</guid>
      <description>&lt;h1 id=&#34;analisis-de-la-oferta-inmobiliaria-en-mar-del-plata&#34;&gt;ANALISIS DE LA OFERTA INMOBILIARIA EN MAR DEL PLATA&lt;/h1&gt;
&lt;p&gt;Entender el mercado inmobiliario, nos permite detectar las necesidades y deseos de la porción de la población
que compran, o que invierten en la generación de servicios de vivienda (ya sea para consumo propio o para
terceros). Los datos se extrajeron de scrappear ZonaProp de manera exhaustiva, los datos estan geolocalizados
por lo cual se pueden hacer analisis espaciales.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lucasenrich.netlify.com/post/mardel_realstate/&#34;&gt;Read more &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;El mercado inmobiliario, está compuesto por un conjunto de bienes y servicios heterogéneos, no sólo en sus
características, sino también en su localización.&lt;/p&gt;
&lt;p&gt;Estas características de cada uno de los inmuebles que se encuentran en el mercado constituyen, en su
conjunto, las características de la oferta de inmuebles en un determinado período de tiempo. Habitualmente,
lo que se observa del mercado inmobiliario es un stock de la oferta y se asume que el precio es el de equilibrio,
en el sentido que los oferentes intentarían obtener el mayor precio que le posibilite la venta o el alquiler, en el
menor tiempo posible. Sin embargo, el flujo de inmuebles que se han vendido no es un conjunto observable y
tal vez sea el conjunto más importante por determinar. Este flujo que se ha vendido o alquilado representa a
aquellos inmuebles que hoy no se encuentran en el stock disponible a la venta o en alquiler, pero que en algún
momento lo estuvieron; por lo que es necesario conformar una base de datos con los inmuebles que están
disponibles actualmente en stock, pero también aquellos que en un lapso han salido y aquellos nuevos que
ingresan a conformarlo. Para ello se desarrolló una rutina a través de un software, que permite relevar los
datos de compra y venta de inmuebles disponibles en la web, para analizar su evolución en el tiempo, para
poder construir distintos indicadores que sirvan como herramientas para los desarrolladores inmobiliarios, a
la hora de decidir las características y el emplazamiento de sus proyectos.&lt;/p&gt;
&lt;h2 id=&#34;estudio-de-la-base&#34;&gt;Estudio de la base&lt;/h2&gt;
&lt;p&gt;El total de registro de ventas con los que contamos en para el tercer trimestre de 2019 de nuestra búsqueda
de datos en la web, una vez que se han eliminado los datos repetidos, es de aproximadamente 25675 casos, de
los cuales, 15562 corresponden a departamentos y 5489 corresponden a casas. En el cuadro resumen de la
base de ventas se tuvieron en cuenta algunas características como la cantidad de m2 ofertados, m2 promedio
y valor en dólares promedio para Mar del Plata.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;Caracteristicas de la base de datos&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Tipo&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Cantidad&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Precio_M2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Tamaño&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Precio&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Departamento&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15562&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2066&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;70&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;138747&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Casa&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5489&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;811&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;434&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;245960&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PH&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1645&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1207&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;101&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;99352&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Terrenos&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1451&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;743&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;700&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;316334&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Local comercial&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;659&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1751&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;208&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;197579&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Oficina comercial&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;287&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1569&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;98&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;125887&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Garage&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;245&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;912&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1932&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;803429&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fondo de Comercio&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;132&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1221&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;968&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1340016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Bodega-Galpón&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;99&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;875&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;336943&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Edificio&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1737&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;759&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;887350&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;De la tabla, se desprende que el 82.16% de oferta inmobiliaria de Mar del Plata esta compuesta por
Departamento (60.74%) y Casa (21.42%) y que los departamentos tienen el mayor precio por metro cuadrado,
por el menor tamaño de las ofertas.
Asimismo, los datos del trimestre revelan la variación de las cantidades, los precios y tamaños de cada tipo
de inmueble.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/unnamed-chunk-3-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;En este caso, podemos ver, por ejemplo, que el aumento del tamaño promedio de los inmuebles, causa que, a
pesar de un aumento del precio, el precio por metro cuadrado tenga una ligera caida. Lo contrario ocurre con
los PHs. Lo cual en el caso de los terrenos (una mejora metodologica en Octubre nos permite obtener mejores
datos de Terrenos y Locales Comerciales), las variaciones se compensan y se mantiene estabe el precio por
metro cuadrado
La base de datos cuenta también con inmuebles que han reducido su precio en los últimos 3 meses.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;Bajaron de Precio&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Tipo&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Cantidad&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Bajó&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Precio_M2&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;var Precio m2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Tamaño&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;var Tamaño&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Precio&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;var Precio&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Departamento&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;207&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8.63%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1891&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-13.51%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;54&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-33.65%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100875&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-35.72%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PH&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;9.80%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1173&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;12.05%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;81&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-50.80%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;91780&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-27.27%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;En el cuadro podemos ver que el precio promedio que bajaron los departamentos es del 8.63% mientras que
los PHs bajaron más, un promedio de 9.8%.
Los inmuebles que bajaron de precio también tienen una serie de caracteristicas particulares. Los precios por
metro cuadrado de los departamentos que bajaron son 13.5% más baratos que el precio promedio, es decir,
que los que bajan son aquellos que de por sí, ya están por debajo del precio promedio. Lo mismo ocurre
respecto al tamaño, los que bajan son 33% más pequeños que la muestra general&lt;/p&gt;
&lt;p&gt;Los PH’s tienen una caracteristica particular que tiene que ver con que a pesar de ser más pequeños, tienen
un precio por metro cuadrado mayor que el general de PH en la muestra. Esto se debe a que la diferencia
respecto al promedio en el tamaño es más chica que la diferencia en el precio del inmueble.&lt;/p&gt;
&lt;h2 id=&#34;publicaciones-por-inmobiliaria&#34;&gt;Publicaciones por Inmobiliaria&lt;/h2&gt;
&lt;p&gt;La base de datos cuenta también con las inmobiliarias, extraídas desde noviembre 2019. Lo cual permite
evaluar mejor la oferta disponible. En principio, podemos ver que la gran mayoría de las inmobiliarias tienen
pocas publicaciones, el 77% tiene menos de 22 publicaciones en el trimestre analizado. Mientras que si
éxisten inmobiliarias que tienen muchas más publicaciones, el 1% de las inmobiliarias tienen entre 276 y 419
publicaciones activas en el trimestre.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/unnamed-chunk-6-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Estos datos también pueden ser utilizados para hacer un seguimiento del market-share de cada una de ellas y
eventualmente hacer una valuación de los inmuebles que tienen a la venta.&lt;/p&gt;
&lt;h2 id=&#34;zonas-de-densidad-de-oferta&#34;&gt;Zonas de Densidad de oferta&lt;/h2&gt;
&lt;p&gt;La infomación obtenida esta geolocalizada en un 97%, lo cual nos permite ubicar en un mapa los inmuebles a
la venta, y poder visualizar la densidad de la oferta de inmuebles. La mayor densida, de acuerdo a los datos,
se encuentra en los barrios de La Perla, donde una hay una desproporcionada densidad de inmuebles.
En el norte, cerca de Parque Peña, hay un foco de oferta; al sur, este foco está en Los Acantilados
Por lo general, estos focos de oferta se encuentran en la costa (salvo Los Acantilados). Aquellos que se
encuentran fuera, se caracterizan por ser los barrios cerrados Rumencó y Arenas del Sur&lt;/p&gt;
&lt;iframe seamless src=&#34;/img/densidad.html&#34; width=&#34;100%&#34; height=&#34;500&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;De esta información se puede extraer las zonas y obtener descripciones de cada una de ellas.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/unnamed-chunk-8-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/unnamed-chunk-9-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Esto nos dice que si bien la zona 6 ocupa el 10% de los m² ofrecidos,
constituye el 26% de las publicaciones. Y, conjuntamente con la zona 5,
son el 25% de los m² ofrecidos, pero superan el 52% del total de
publicaciones.&lt;/p&gt;
&lt;p&gt;Las variables que puede ser descriptas en este sentido son, para cada zona:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tipo de Inmueble&lt;/li&gt;
&lt;li&gt;Tamaño de los inmuebles&lt;/li&gt;
&lt;li&gt;Precio por metro cuadrado&lt;/li&gt;
&lt;li&gt;Precio&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tipo&#34;&gt;Tipo&lt;/h3&gt;
&lt;p&gt;Debajo se puede ver que en la zona de menor densidad de oferta (zona 0), el mercado consiste predominantemente en terrenos y casas. Y a medida que nos vamos acercando a la zona de mayor concentración de oferta (zona 6) ocurren varias cosas. En primer lugar los terrenos dejan de estar disponibles; luego, se incorporan casas, los departamentos van ocupando la mayor parte de la oferta y en el centro comercial, donde mayor oferta hay, se incorporan los locales comerciales y garages, desplazando a los PH’s.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/unnamed-chunk-10-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Este tipo de distribución es entendible desde el punto de vista que, las unidades de menor tamaño permiten que haya más oferta por unidad de terreno.&lt;/p&gt;
&lt;h3 id=&#34;tamaños&#34;&gt;Tamaños&lt;/h3&gt;
&lt;p&gt;Los tamaños de los inmuebles juegan un rol fundamental en la concentración geográfica de la oferta. Porque inmuebles de menor tamaño permiten una mayor cantidad de inmuebles por unidad de terreno.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/unnamed-chunk-11-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Como se vió en la sección de tipos de inmuebles se corrobora que, a menor densidad de oferta, mayor son los tamaños de los inmuebles a la venta. Por ejemplo, en la zona 0, donde predominan terrenos y casas, el tamaño promedio es de 1163m², lo cual se reduce exponencialmente a medida que se acerca a la zona 0.&lt;/p&gt;
&lt;h3 id=&#34;precios&#34;&gt;Precios&lt;/h3&gt;
&lt;p&gt;Los precios de los inmuebles en cada zona son más homogeneos que las variables previamente analizadas. Aunque cierta tendencia sigue existiendo. En la zona 6, donde mayor inmuebles hay, es donde se encuentran los menores precios.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/unnamed-chunk-12-2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Esta información, sin embargo, no refleja la variabilidad de los precios. Ya que si bien los precios en la zona 6 son más baratos, en promedio, la variabilidad de los precios en esa zona es 50% mayor a la variabilidad de los precios en la zona 0. Esto quiere decir que si bien hay más con precios más bajos, también los hay con precios mucho mayores. Es decir, en las zonas donde predominan los departamentos los precios son mucho menos consistentes que los precios donde dominan los terrenos y las casas.&lt;/p&gt;
&lt;h3 id=&#34;precio-por-metro-cuadrado&#34;&gt;Precio por metro cuadrado&lt;/h3&gt;
&lt;p&gt;Si a medida que hay mayor concentración de oferta, las unidades son de mayor tamaño y los precios promedios son más homogéneos transversalmente a las zonas se concluye que los precios por metro cuadrado serán mayores allí donde haya mayor concentración de oferta&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/unnamed-chunk-13-1.png&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;determinación-de-zonas-de-precios&#34;&gt;Determinación de zonas de precios&amp;hellip;&lt;/h3&gt;
&lt;p&gt;De la misma manera que se determinaron las zonas de mayor y menor concentración de oferta puede determinarse las zonas de mayor o menor precios por metro cuadrado.&lt;/p&gt;
&lt;iframe seamless src=&#34;/img/pm2_d.html&#34; width=&#34;100%&#34; height=&#34;500&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;Así también como de cualquier variable númerica disponible. Y esta información se puede tener filtrada por tipo de inmueble, inmobiliaria, barrio, mes, etc según sea necesario.
Alguno de los mapas alternativos pueden visualizarse en: 
&lt;a href=&#34;https://lucariel.shinyapps.io/mapa_inmobiliario/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mapa
interactivo&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/paso2019/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/paso2019/</guid>
      <description>&lt;h1 id=&#34;twitter-y-las-paso&#34;&gt;Twitter y las Paso&lt;/h1&gt;
&lt;p&gt;Esta fue mi primera aproximacion a minar datos de redes sociales,
amigarme con las APIs, y algún analysis rudimentario. Siguiendo
metodologias propuestas por otros.&lt;/p&gt;
&lt;p&gt;En principio vamos a comenzar con lo primero ¿como hice para minar los
datos de twitter? Bueno para eso use tweepy (&lt;a href=&#34;http://www.tweepy.org/&#34;&gt;http://www.tweepy.org/&lt;/a&gt;)
Asique la primera parte va a estar en Python.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lucasenrich.netlify.com/post/PASO2019/&#34;&gt;Read more &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Importando lo importante&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import tweepy
from tweepy.streaming import StreamListener
from tweepy import Stream
import time
from slistener import SListener
import os
import matplotlib.pyplot as plt
import json
import requests
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;por slistener es un script cortesia de
&lt;a href=&#34;https://github.com/alexhanna/hse-twitter/blob/master/bin/slistener.py&#34;&gt;https://github.com/alexhanna/hse-twitter/blob/master/bin/slistener.py&lt;/a&gt;
que permite crear un objeto que va a ser el &amp;ldquo;listener&amp;rdquo; o &amp;ldquo;escuchante&amp;rdquo; de
twitter. Para tomar los datos en tiempo real y poder ir guardandolos.&lt;/p&gt;
&lt;p&gt;Instanciando lo instanciable y setiando los paths donde van las cosas&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;auth = tweepy.OAuthHandler(&#39;zarazatoken&#39;, &#39;zarazatoken)
auth.set_access_token(&#39;zarazatoken&#39;, &#39;zarazatoken&#39;)
api = tweepy.API(auth)

datapath = os.path.join(os.getcwd(), &#39;data&#39;)
datafiles = os.listdir(datapath)

Y vamos a poder el escuchante a escuchar twitter

keywords_to_track = [&#39;EleccionesPASO2019&#39;, &#39;FrenteDeTodos&#39;,&#39;Frente Todos&#39;,
                     &#39;Juntos por el Cambio&#39;,&#39;Juntos Cambio&#39;,&#39;Elecciones&#39;,&#39;PASO&#39;,
                     &#39;YoTeVotoAlberto&#39;,&#39;NoVuelvenNuncaMas&#39;,
                    
&#39;ArgentinaVota&#39;,&#39;Macri&#39;,&#39;YoLoVoto&#39;,&#39;Fernandez&#39;,&#39;Kirchner&#39;]
stream.filter(track = keywords_to_track)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Los keywords a trackear se eligieron tomando los trending topic en
argentina referidos a las elecciones y algunos elegidos por mi, a mano&lt;/p&gt;
&lt;p&gt;stream.filter() lo que se encarga de hacer es ir tomando la muestra en
tiempo real de datos de twitter que se ajusten al filtro. Mientras corra
(es decir, mientras no se interrumpa) va a ir juntando los datos. Esto
lo empece a correr el domingo de las paso a las 7am y lo frené el mismo
día a las 17hs.&lt;/p&gt;
&lt;p&gt;La siguiente parte me fue bastante mas dificil de lo que habia
anticipado, porque estas muestras se guardan en formato &amp;ldquo;.json&amp;rdquo; lo cual
tenia que convertir a &amp;ldquo;.csv&amp;rdquo; para poder trabajar mejor&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import pandas as pd
import numpy as np
tweets = []
with open(os.path.join(datapath,datafiles[1]), &#39;r&#39;,encoding=&#39;utf-8&#39;, errors=&#39;ignore&#39;) as t:
    tw_json = t.read().split(&#39;\n&#39;)
    for tw in tw_json:
        #print(tw)
        #print(&#39;\t\t&#39;)
        try:
            tweet_obj = json.loads(tw)
        except:
            pass
        if &#39;extended_tweet&#39;in tweet_obj:
            tweet_obj[&#39;extended_tweet-text&#39;] =  tweet_obj[&#39;extended_tweet&#39;][&#39;full_text&#39;]
            if tw != &#39;&#39;:
                tweets.append(tw)
        
pd.DataFrame(tweets)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;¡Ahora si! Ya tenemos bonito el data_frame en pandas para guardarlo y
seguir desde allí&lt;/p&gt;
&lt;p&gt;La siguiente tarea seria el topic extraction, pero la realidad es que
cuando lo hice no llegue a ningun lado, porque, obviamente y como es de
esperar, estaba todo referido a as elecciones. Lo que si termine
haciendo fue filtrar el dataset que me quedo por las keywords que
nombrar a los dos principales candidatos&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{&#39;YoVotoMM&#39;,&#39;juntosporelcambio&#39;, &#39;votomm&#39;,&#39;NoVuelvenNuncaMas&#39;, &#39;yolovoto&#39;,&#39;Macri&#39;} ## Quedarón 6,320 registros

{&#39;FrenteDeTodos&#39;,&#39;futurocontodos&#39;,&#39;YoTeVotoAlberto&#39;,&#39;FernandezFernandez&#39;,&#39;CFK,&#39;cristina kirchner&#39;&#39;Alberto Fernandez&#39;} ## Quedarón 5,554 registros
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ahora bien, ¿de quien se hablaba mas en twitter?&lt;/p&gt;
&lt;p&gt;Primero al dataframe de cada topic se agrega la variable
correspondiente, se generan las dummies y luego se saca el promedio por
hora, lo que resulta en la proporcion de tuits de cada uno&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;macri[&#39;p&#39;] = &#39;MM&#39;
frente_de_todos[&#39;p&#39;] = &#39;FF&#39;

df1 = pd.concat([macri, frente_de_todos])

df2 = pd.get_dummies(df1.p)


mean_mm = df2[&#39;MM&#39;].resample(&#39;1 h&#39;).mean()
mean_ff = df2[&#39;FF&#39;].resample(&#39;1 h&#39;).mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sacando el plot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/plotpaso1.jpeg&#34; width=&#34;30%&#34; style=&#34;float:center; padding:0% 35%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Se ve que salvo a la mañana y bien entrada la tarde, se hablo mas de
Macri.&lt;/p&gt;
&lt;p&gt;Bueno, habiendo hecho la primera parte en Python, es hora de continuar
con la parte de sentiment analysis de los tuits de las PASO. Esta vez,
en R. Vamos a empezar por las bibliotecas que necesitamos:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(tidyverse)
library(tidytext)
library(stopwords)
library(syuzhet)
library(stopwords)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Luego traemos los datos:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;paso&amp;lt;-read_csv(&#39;paso.csv&#39;)
paso&amp;lt;-paso[colnames(paso)!=&amp;quot;X1&amp;quot;]
paso_unique&amp;lt;-unique(paso$`extended_tweet-full_text`)
paso_unique2&amp;lt;-as_tibble(paso_unique)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;El unique() nos sirve para filtrar tuits duplicados. Que pueden ocurrir
por que un usuario citó a otro.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;length(paso$`extended_tweet-full_text`)
 
#Quedan 44423 registros


length(unique(paso$`extended_tweet-full_text`)) 
#Quedan 43996 registros
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Vamos a tokenizar las palabras:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tweet_token&amp;lt;-paso_unique2%&amp;gt;%
  unnest_tokens(word, txt)


tweet_token&amp;lt;-tweet_token%&amp;gt;%
  count(word, sort = T)%&amp;gt;%
  filter(!word%in% stopwords(&#39;es&#39;))%&amp;gt;%
  filter(!word%in% stopwords(&#39;en&#39;))%&amp;gt;%
  filter(str_detect(word, &amp;quot;^[a-zA-z]|^#|^@&amp;quot;))%&amp;gt;%
  ungroup()%&amp;gt;%
  arrange(desc(n))%&amp;gt;%
  mutate(w = word,
         freq = n)%&amp;gt;%
  select(w, freq)

## Resultado

   w                   freq
   &amp;lt;chr&amp;gt;              &amp;lt;int&amp;gt;
 1 t.co               18839
 2 https              18834
 3 paso               16293
 4 elecciones          8401
 5 macri               7982
 6 si                  5853
 7 eleccionespaso2019  5498
 8 votar               4950
 9 q                   4318
10 hoy                 3304
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Esto no es muuy bueno, hay tokens que hay que sacar.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tweet_token_2&amp;lt;-tweet_token%&amp;gt;%filter(w!=&#39;t.co&#39;)%&amp;gt;%filter(w!=&#39;https&#39;)%&amp;gt;%
  filter(w!=&#39;q&#39;)%&amp;gt;%filter(w!=&#39;to&#39;)%&amp;gt;%filter(w!=&#39;si&#39;)%&amp;gt;%filter(w!=&#39;and&#39;)%&amp;gt;%
  filter(w!=&#39;rt&#39;)

   w                    freq
   &amp;lt;chr&amp;gt;               &amp;lt;int&amp;gt;
 1 paso                16293
 2 elecciones           8401
 3 macri                7982
 4 eleccionespaso2019   5498
 5 votar                4950
 6 hoy                  3304
 7 argentinavota        3043
 8 eleccionesargentina  2878
 9 voto                 2588
10 trump                2409
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ahora si, mira que loco lo de Trump. Igual esto se ve mucho mejor con un
gráfico, además, no filtre todavia los stopwords y no filtre por tuits
en español, asique probablemente sean tuits colados de otro tema&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tweet_token_2 [ 1 : 25 , ] %&amp;gt;%
  mutate ( w = forcats :: fct_inorder ( w ) ) %&amp;gt;%
  ggplot ( aes ( x = w , y = freq ) ) +
  geom_segment ( aes ( x = w , xend = w , y = 0 , yend = freq ) , color= &amp;quot;grey&amp;quot; )+
  geom_point(size = 3, color = &amp;quot;#009A44&amp;quot;)+
  coord_flip()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/plotpaso2.jpeg&#34; width=&#34;30%&#34; style=&#34;float:center; padding:0% 35%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Bueno, el termino &amp;ldquo;paso&amp;rdquo; es evidentemente el mas frecuente, lo cual es
mas que esperable. Luego, nos quedaria ver como se sentia la gente
respecto a esto. Para esto se uso la libreria syuzhet&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;base_emocion&amp;lt;-get_nrc_sentiment(unlist(paso_unique2))
base_emocion &amp;lt;- data.frame(t(base_emocion))
base_emocion &amp;lt;- data.frame ( rowMeans ( base_emocion ) )
names ( base_emocion ) [ 1 ] &amp;lt;- &amp;quot;Proporcion&amp;quot;
base_emocion &amp;lt;- cbind ( &#39;Sentimiento&#39; = rownames ( base_emocion ) , base_emocion )

base_emocion%&amp;gt;%
  ggplot()+geom_bar(aes(x = Sentimiento, y = Proporcion), stat = &#39;identity&#39;, fill = &#39;green&#39;, alpha = 0.8)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Esta es la parte mas relevante que tome de Hernan, la diferencia que
tome, fue que él tomo la suma de cada una de los casos de cada
sentimiento, y yo la proporcion. Creo que eso puede reflejar de otra
forma cual es la emocion predominante en cada caso:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/plotpaso3.jpeg&#34; width=&#34;30%&#34; style=&#34;float:center; padding:0% 35%&#34; /&gt;
En este caso general, se ve que entre &amp;ldquo;positivo&amp;rdquo; y &amp;ldquo;negativo&amp;rdquo; son los
predominantes, seguidos por &amp;ldquo;confianza&amp;rdquo; y &amp;ldquo;enojo&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Esta misma metodologia se puede usar para los dos datasets separados
para cada topic pre-seleccionado, los referidos al frente de todos y a
juntos por el cambio&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/plotpaso4.jpeg&#34; width=&#34;30%&#34; style=&#34;float:center; padding:0% 35%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/plotpaso5.jpeg&#34; width=&#34;30%&#34; style=&#34;float:center; padding:0% 35%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Todos se quejan, pero de Cristina hablan todos. Igual hay que considerar
que de este conteo, se filtraron los nombres y apellidos de los
candidatos a la presidencia ya que es lo que se uso de filtro.&lt;/p&gt;
&lt;p&gt;¿Como se sienten?
&lt;img src=&#34;/img/plotpaso6.jpeg&#34; width=&#34;30%&#34; style=&#34;float:center; padding:0% 35%&#34; /&gt;
&lt;img src=&#34;/img/plotpaso7.jpeg&#34; width=&#34;30%&#34; style=&#34;float:center; padding:0% 35%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Un poco de esto confirma no solo los resultado de la eleccion sino
tambien la lectura del voto &amp;ldquo;enojo&amp;rdquo;. Porque los sentimientos asociados a
cambiemos tienen mayor participacion de enojo y sentimientos negativos.
Mientras que los asociados al frente de todos tiene mucha mayor
participacion los tuits positivos.&lt;/p&gt;
&lt;p&gt;Un bonus track de python nada mas (esbozo de network analysis) ¿A quien
se le contestaba mas para cada grupo?&lt;/p&gt;
&lt;p&gt;Las libreras de python son las mismas que el post anterior solo con la
adicional de Networkx que permite hacer el analisis de redes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import networkx as nx

frente_de_todos = pd.read_csv(&#39;frente_de_todos.csv&#39;)
cambiemos = pd.read_csv(&#39;cambiemos.csv&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Una vez leidos, filtramos los tuits que &amp;ldquo;son respuesta a&amp;rdquo;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cambiemos_nx = cambiemos_nx[-cambiemos_nx[&#39;in_reply_to_screen_name&#39;].isnull()]
cambiemos_nx[&#39;in_reply_to_screen_name&#39;]
frente_de_todos_nx = frente_de_todos_nx[-frente_de_todos_nx[&#39;in_reply_to_screen_name&#39;].isnull()]
frente_de_todos_nx[&#39;in_reply_to_screen_name&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Se generan las redes de c/u; esto genera que cada usuario sea un nodo y
que la relacion entre los usuarios se dá, en este caso particular, si
responden a un tuit es decir: si yo te respondo un tuit, nosotros dos
generamos una red que tiene mi nombre (mi usuario) como nodo de inicio y
tu nodo (tu usuario) como nodo destino. Cada objeto, entonces, va a
tener tantas salidas como respuestas haya hecho y tantas entradas como
respuestas haya recibido:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;G_reply_c = nx.from_pandas_edgelist(
    cambiemos_nx,
    source = &#39;user-screen_name&#39;,
    target = &#39;in_reply_to_screen_name&#39;,
    create_using = nx.DiGraph())

G_reply_f = nx.from_pandas_edgelist(
    frente_de_todos_nx,
    source = &#39;user-screen_name&#39;,
    target = &#39;in_reply_to_screen_name&#39;,
    create_using = nx.DiGraph())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Y vemos la centralidad de cada tuitero (&amp;ldquo;in-degree-centrality&amp;rdquo;), que en
realidad seria la respuesta a la pregunta &amp;ldquo;¿A quien se le esta
contestando más?&amp;rdquo;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#Para Frente de Todos - ¿a quien se le contesta cuando se habla de estse #tema?

G_reply_f = nx.from_pandas_edgelist(
    frente_de_todos_nx,
    source = &#39;user-screen_name&#39;,
    target = &#39;in_reply_to_screen_name&#39;,
    create_using = nx.DiGraph())


bc = nx.in_degree_centrality(G_reply_f)
indg = pd.DataFrame(list(bc.items()), columns =[&amp;quot;Name&amp;quot;,&#39;Cent&#39;])
indg.sort_values(&#39;Cent&#39;, ascending=False)
Name    Cent
153 alferdez        0.018328
18  ierrejon        0.017182
137 LotusHerbals    0.017182
115 todonoticias    0.016037
113 fllorenteantoni 0.013746
746 AlbertoRavell   0.010309
159 FernandezAnibal 0.010309
147 LeonelFernandez 0.006873
236 mirthalegrand   0.006873
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Y en el caso del Juntos Por el Cambio&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;G_reply_c = nx.from_pandas_edgelist(
    cambiemos_nx,
    source = &#39;user-screen_name&#39;,
    target = &#39;in_reply_to_screen_name&#39;,
    create_using = nx.DiGraph())

bc = nx.in_degree_centrality(G_reply_c)
indg = pd.DataFrame(list(bc.items()), columns =[&amp;quot;Name&amp;quot;,&#39;Cent&#39;])
indg.sort_values(&#39;Cent&#39;, ascending=False)

Name    Cent
36  mauriciomacri   0.021858
148 fllorenteantoni 0.012610
131 gabicerru       0.011349
84  juansolervalls  0.008827
119 EsmeraldaMitre  0.007566
3   todonoticias    0.005885
99  CamiSolovitas   0.004624
17  Alfredo5019     0.004624
23  SantoroLeandro  0.004203 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;FIN! Gracias por leer hasta acá! Si tienen alguna recomendacion para
tener en cuenta futuros analisis se los agradece!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/review_rating/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/review_rating/</guid>
      <description>&lt;h1 id=&#34;review-rating&#34;&gt;Review Rating&lt;/h1&gt;
&lt;p&gt;¿Cómo generar un puntaje númerico en base a un texto?&lt;/p&gt;
&lt;p&gt;Mucho de lo expuesto es en realidad distintas formas de pensar el
problema y quedarse con la mejor solución.&lt;/p&gt;
&lt;p&gt;Paseo por Doc2Vec, Regresiones lineales, randomForests y redes convolutivas
con GloVe&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lucasenrich.netlify.com/post/review_rating/&#34;&gt;Read more &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;En principio, lo que se va a ver es las calificaciones haciendo uso de
Doc2Vec para convertir el texto en un vector numerico y poder, con esos
vectores numericos como input, realizar la predicción de cual sería la
calificación que hubiera tenido según el texto. Haciendo uso de
algortimos de aprendizaje supervisado.&lt;/p&gt;
&lt;p&gt;Por otro lado, dado que los datos no son tantos, lo que perjudica la
construcción del vector numerico a partir de los textos, se hará uso de
un &amp;ldquo;word embedding&amp;rdquo; ya entrenado y posteriormente se verá como mejora el
poder predictivo.&lt;/p&gt;
&lt;h3 id=&#34;los-datos-y-la-limpieza&#34;&gt;Los datos y la limpieza&lt;/h3&gt;
&lt;p&gt;Para empezar, veamos como se ven los datos:&lt;/p&gt;
&lt;table style=&#34;width:39%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;col width=&#34;22%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;review.rating&lt;/th&gt;
&lt;th&gt;review.text&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;Our experience at Rancho Valencia was absolutely perfect from beginning to end!!!! We felt special and very happy during our stayed. I would come back in a heart beat!!!&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;Amazing place. Everyone was extremely warm and welcoming. We&#39;ve stayed at some top notch places and this is definitely in our top 2. Great for a romantic getaway or take the kids along as we did. Had a couple stuffed animals waiting for our girls upon arrival. Can&#39;t wait to go back.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;We booked a 3 night stay at Rancho Valencia to play some tennis, since it is one of the highest rated tennis resorts in America. This place is really over the top from a luxury standpoint and overall experience. The villas are really perfect, the staff is great, attention to details (includes fresh squeezed orange juice each morning), restaurants, bar and room service amazing, and the tennis program was really impressive as well. We will want to come back here again.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;En lugar de importar todos los paquetes juntos, vamos a ir importando a
medida que vayamos necesitando.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import pandas as pd
cc = pd.read_csv(&#39;./hotel-reviews/Datafiniti_Hotel_Reviews.csv&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Vamos a seleccionar las columnas necesarias y cambiarle el nombre para
que sea mas facil luego seleccionarlas.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cc = cc[[&#39;reviews.title&#39;,&#39;reviews.text&#39;,&#39;reviews.rating&#39;]]
cc.columns = [&#39;titulo&#39;,&#39;comentarios&#39;,&#39;calificacion&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;El primer paso a la hora de trabajar con estos textos, es reducir a la
maxima expresión la cardinalidad del vocabulario. ¿Que significa esto?
Si tenemos una gran cantidad de usos de un verbo, como por ejemplo,
&amp;ldquo;correr&amp;rdquo; en sus distintas conjugaciones, &amp;ldquo;corría&amp;rdquo;,&amp;ldquo;corriendo&amp;rdquo;,&amp;ldquo;corrian&amp;rdquo;
y queremos armar un listado de frecuencias de palabras, esto daria como
resultado que cada uno de esas palabras aparezca una sola vez; pero si
logramos que la referencia a la acción concreta de &amp;ldquo;correr&amp;rdquo; sume
independientemente de su conjugación, reduciríamos la cardinalidad de
nuestro diccionario, eso es para lo que se usa la &lt;em&gt;lematización&lt;/em&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Original&lt;/th&gt;
&lt;th&gt;Lemmatizacion&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;corrian&lt;/td&gt;
&lt;td&gt;correr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;corriendo&lt;/td&gt;
&lt;td&gt;correr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;corrio&lt;/td&gt;
&lt;td&gt;correr&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Luego, hay sustantivos y otras palabras que tienen la misma raiz y
dependiendo del sujeto, se puede reducir el tamaño del diccionario
cortando de la raiz la palabra, lo que se conoce como &lt;em&gt;stemmización&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Original&lt;/th&gt;
&lt;th&gt;Stemmización&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;niña&lt;/td&gt;
&lt;td&gt;niñe&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;niño&lt;/td&gt;
&lt;td&gt;niñe&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;niñe&lt;/td&gt;
&lt;td&gt;niñe&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;El proceso de lematización y stemización, en su conjunto, se puede
entender como normalizar el vocabulario y por lo tanto, una función que
se encarge de hacer estas dos cosas, puede llamarse &lt;em&gt;normalize(text)&lt;/em&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def normalize(text):
    from nltk.stem import PorterStemmer
    from nltk.tokenize import word_tokenize
    import unidecode
    import spacy
    nlp = spacy.load(&#39;en_core_web_sm&#39;)
    porter = PorterStemmer()
    doc = nlp(text)
    lemmas = [unidecode.unidecode(tok.lemma_.lower()) for tok in doc if not tok.is_punct ]
    #En este caso, estoy eliminando palabras con menos de 3 letras y las negaciones, esto no es necesario estrictamente, y depende mucho del caso de aplicación, a veces funciona, a veces no.
    lexical_tokens = [t.lower() for t in lemmas if (len(t) &amp;gt; 3 or t ==&amp;quot;no&amp;quot;) and t.isalpha()]
    lexical_tokens = [porter.stem(t) for t in lexical_tokens]
    return lexical_tokens
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Esta funcion tiene como input una frace y escupe objeto tipo list()
asique lo que haré es aplicarla a cada texto y despues volverla a unir
para que cada fila tenga un texto y no un array&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;norm_token = []
for i in range(len(cc.comentarios)):
    try:
        a = normalize(cc.comentarios[i])
    except:
        a = &#39;&#39;
    norm_token.append(a)
norm_text = [&#39; &#39;.join(x) for x in norm_token]
cc[&#39;norm_text&#39;] = norm_text
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;doc2vec-generando-el-embedding-numérico&#34;&gt;Doc2Vec: Generando el embedding numérico&lt;/h3&gt;
&lt;p&gt;¿Porque no tratar directo con los tokens? Por la sencilla razón hay un
paquete que permite aplicar Doc2Vec, asociando un texto a una clase,
&lt;em&gt;gensim&lt;/em&gt; nos va a venir bien para esto:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from gensim.models.doc2vec import Doc2Vec, TaggedDocument
#Primera la separacion entre test y train
train, test = train_test_split(cc, test_size=0.2, random_state=42)

train_tagged = train.apply(
    lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1)
test_tagged = test.apply(
    lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Una vez que tenemos los elementos de train y test, hay que entrenar el
Doc2Vec, para, así pasar el texto a un vector númerico que pueda ser el
input del algoritmo de clasificación&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for epoch in range(30):

    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)
    model_dbow.alpha -= 0.002
    model_dbow.min_alpha = model_dbow.alpha
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Con el modelo Doc2Vec entrenado, podemos darle pasar los textos y
obtener el vector numerico deseado. Ahora bien, para facilitar la
implementación del modelo después, generemos una función con dos ouputs,
el vector numerico por un lado, y el rating asociado a ese vector
numérico&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def vec_for_learning(model, tagged_docs):
    sents = tagged_docs.values
    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])
    return targets, regressors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ahora que tenemos la función que nos genera el vector númerico
terminamos con el proceso de preparación de los datos. Es curioso notar
que siempre se dice que el 80% del trabajo consiste en la preparación y
limpieza de datos, y el 20% el modelado. Hasta ahora se puede ver que no
es una distinción tan discreta, sino que es continua. ¿A qué me refiero?
Bueno, para preparar los datos hizo falta algoritmos de embedding
(Doc2Vec). Y no es poco común que ocurran estas cosas.&lt;/p&gt;
&lt;p&gt;Ahora bien, volvamos a lo nuestro, es hora de correr los algoritmos de
regresión:&lt;/p&gt;
&lt;h3 id=&#34;regresión&#34;&gt;Regresión&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression

y_train, X_train = vec_for_learning(model_dbow, train_tagged)
y_test, X_test = vec_for_learning(model_dbow, test_tagged)

lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)

print(np.sqrt(mean_squared_error(y_test, y_pred))) #1.15
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Este resultado no me convence demasiado si consideramos al puntaje como
una regresión. El error cuadratico medio es del 1.15pts, para los que no
recuerdan, el error cuadratico medio toma la diferencia entre el valor
predecido y el valor real, lo eleva al cuadrado y de ello toma el
promedio. Les dejo un &lt;a href=&#34;https://www.youtube.com/watch?v=8wgy8Vopv3E&#34;&gt;video &lt;/a&gt; de mi canal de Youtube con la visualización
de lo que significa&lt;/p&gt;
&lt;h3 id=&#34;clasificación&#34;&gt;Clasificación&lt;/h3&gt;
&lt;p&gt;Otra forma de entender el problema es como uno de clasificación. ¿Pero
como pasamos de un target continuo a uno discreto? Podríamos pensar que
los puntajes de 4 ó 5 son &amp;ldquo;buenos&amp;rdquo;, y asignarles un 1, y los de menos de
4 son &amp;ldquo;malos&amp;rdquo;, y asignarles un 0, y nos quedamos con un problema de
clasificación binaria.&lt;/p&gt;
&lt;p&gt;Además, con estas conceptualización, tenes que volver a correr el
embedding porque los &amp;ldquo;tags&amp;rdquo; no son ahora los puntajes del 1 al 5 sino
que son {1,0}&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cl = []
for i in cc.calificacion:
    if i &amp;lt;4:
        cl.append(0)
    else:
        cl.append(1)
cc.calificacion = cl

train, test = train_test_split(cc, test_size=0.2, random_state=42)

train_tagged = train.apply(
    lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1)
test_tagged = test.apply(
    lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1)
model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0)
model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])
for epoch in range(30):
    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)
    model_dbow.alpha -= 0.002
    model_dbow.min_alpha = model_dbow.alpha
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ahora si, sacamos los vectores del modelo Doc2Vec y lo fiteamos a un
randomForest clasificador:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from sklearn.ensemble import RandomForestClassifier
y_train, X_train = vec_for_learning(model_dbow, train_tagged)
y_test, X_test = vec_for_learning(model_dbow, test_tagged)


rfr = RandomForestClassifier(n_estimators = 500, random_state = 42)
rfr.fit(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;¿Cómo evaluamos esta clasificación? Bueno, primero nos fijamos cuanto
coincide la predicción respecto al valor real, para eso se usa la matriz
de confusión&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from sklearn.metrics import confusion_matrix
y_pred = rfr.predict(X_test)
tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0,1]).ravel()


np.mean(y_test == y_pred)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nuestra precisión es del 72,4%, nada mal, solo un detalle. Si mandamos
un modelo que siempre diga &amp;ldquo;1&amp;rdquo;, tendremos una precisión equivalente a la
proporción de &amp;ldquo;1&amp;rdquo; en el set. Que en la base completa es de 72,8%. Es
decir, este modelo no mejor que decir que todas son igual a &amp;ldquo;1&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;polaridad-como-métrica&#34;&gt;Polaridad como métrica&lt;/h3&gt;
&lt;p&gt;Otra alternativa puede ser extraer la polaridad del texto, herramienta
muy útil en los procesos de sentiment analysis. Y podríamos pensar que,
cuan más positivo sea la polaridad, estará asociado a un mejor puntaje&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from textblob import TextBlob 
pls = []
sbj = []
for i in range(len(cc.comentarios)):
    try:
        senti = TextBlob(cc.comentarios[i]) 
        polarity = senti.sentiment
        pls.append(polarity[0])
        sbj.append(polarity[1])
    except:
        pls.append(0)
        sbj.append(0)
        

polscore = [int(x &amp;gt; 0) for x in pls] # Acá 0 es un valor arbitrario de corte, 
#un ejericio podría incluir la optimización de este valor como un hiperparametro. 
#La polaridad genera un indice de -1, 1. Siendo -1 cuan más negativo es, y 1 cuan más #positivo es, y polscore dice que aquellos que tienen valoracion positiva sean 1 y los demás 0
np.mean(polscore == np.array(cc.calificacion))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sirve esto? Y, esto esta dando un resultado de 78.55%, es, a mi
sorpresa, una mejora respecto al punto anterior. Aunque todavía no es
satisfactorio.&lt;/p&gt;
&lt;p&gt;Evidentemente el score de polarización agrega información.&lt;/p&gt;
&lt;h3 id=&#34;redes-neuronales-y-glove&#34;&gt;Redes neuronales y GloVe&lt;/h3&gt;
&lt;p&gt;Global Vectors ó &lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34;&gt;GloVe &lt;/a&gt; es una tecnica que, a diferencia de Doc2Vec, que
es un algortimo supervisado, es no-supervisado y obtiene embedings
númericos de palabras según estadisticas de co-ocurrencia. De esta
manera puede encontrar analogías tales como &amp;ldquo;los que varón es a mujer,
rey es a reina&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Más allá las &lt;a href=&#34;https://towardsdatascience.com/gender-bias-word-embeddings-76d9806a0e17&#34;&gt;controversias &lt;/a&gt;,
es una herramienta bastante útil para muchos casos&lt;/p&gt;
&lt;p&gt;El objetivo de esta sección es ver como, haciendo uso de un modelo de
lenguage pre-existente, se puede usar el proceso de transfer-learning
para incorporar nuestros textos y sus calificaciones y adaptarlo a
nuestras necesidades.&lt;/p&gt;
&lt;p&gt;Esto es interesante e importante porque hay muchos modelos de lenguage
que han hecho uso de datasets enormes en hardwares mucho más potentes
que los que podría pagar, que se puede aprovechar&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import os
import sys
import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.layers import Dense, Input, GlobalMaxPooling1D
from keras.layers import Conv1D, MaxPooling1D, Embedding
from keras.models import Model
from keras.initializers import Constant


BASE_DIR = os.getcwd()
GLOVE_DIR = os.path.join(BASE_DIR, &#39;glove.6B&#39;) #Este es el modelo pre-entrenado
TEXT_DATA_DIR = os.path.join(BASE_DIR, &#39;20_newsgroup&#39;)
MAX_SEQUENCE_LENGTH = 1000 #entrenar sobre oraciones de hasta estas palabras
MAX_NUM_WORDS = 20000 #tamaño máximo del vocabulario
EMBEDDING_DIM = 300 #dimensión del vector númerico resultante
VALIDATION_SPLIT = 0.2

#Volvemos a cargar la información
import pandas as pd
cc = pd.read_csv(&#39;./hotel-reviews/Datafiniti_Hotel_Reviews.csv&#39;)
cc = cc[[&#39;reviews.title&#39;,&#39;reviews.text&#39;,&#39;reviews.rating&#39;]]
cc.columns = [&#39;titulo&#39;,&#39;comentarios&#39;,&#39;calificacion&#39;]

cl = []
for i in cc.calificacion:
    if i &amp;lt;4:
        cl.append(0)
    else:
        cl.append(1)
cc.calificacion=cl

#Preparación de los datos
TEXT_DATA_DIR = cc.comentarios

embeddings_index = {}
with open(os.path.join(GLOVE_DIR, &#39;glove.6B.100d.txt&#39;)) as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, &#39;f&#39;, sep=&#39; &#39;)
        embeddings_index[word] = coefs

texts = [x for x  in cc.comentarios]  # listado de muestras de texto
labels_index = {1:1, 2:2, 3:3, 4:4, 5:5}  # diccionario mapeando los revies a los target
labels = [int(x) for x in cc.calificacion] # target

# Tokenizar las palabras
texts = np.array(texts)
tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index


#Con las palabras  tokenizadas, se hace el padding, para que todos tengan la misma longitud, para eso se agrega 0 hasta que se llene
data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

labels = to_categorical(np.asarray(labels))

indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])

x_train = data[:-num_validation_samples]
y_train = labels[:-num_validation_samples]
x_val = data[-num_validation_samples:]
y_val = labels[-num_validation_samples:]


num_words = min(MAX_NUM_WORDS, len(word_index) + 1)
embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))
for word, i in word_index.items():
    if i &amp;gt;= MAX_NUM_WORDS:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

#Este es el proceso que genera los embeddings
embedding_layer = Embedding(num_words,
                            EMBEDDING_DIM,
                            embeddings_initializer=Constant(embedding_matrix),
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, hasta ahi la preparación de los datos, es hora de entrenar una red
neuronal convolutiva con los embeddings realizados para obtener el
modelo que clasifique las reiews:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=&#39;int32&#39;)
embedded_sequences = embedding_layer(sequence_input)
x = Conv1D(128, 6, activation=&#39;relu&#39;)(embedded_sequences)
x = MaxPooling1D(5)(x)
x = Conv1D(128, 6, activation=&#39;relu&#39;)(x)
x = MaxPooling1D(6)(x)
x = Conv1D(128, 6, activation=&#39;relu&#39;)(x)
x = GlobalMaxPooling1D()(x)
x = Dense(128, activation=&#39;relu&#39;)(x)
preds = Dense(2, activation=&#39;softmax&#39;)(x)

model = Model(sequence_input, preds)
model.compile(loss=&#39;categorical_crossentropy&#39;,
              optimizer=&#39;rmsprop&#39;,
              metrics=[&#39;acc&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hay mucho código repetido en este caso, pero eso es para simplicidad de
exposición, lo relevante a enteder es que así se define la capa
convolutiva, que esla que se repite:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;x = Conv1D(128, 6, activation=&#39;relu&#39;)(embedded_sequences)
x = MaxPooling1D(5)(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Y así la ultima capa, que como terminamos con las &amp;ldquo;buenas&amp;rdquo; y &amp;ldquo;malas&amp;rdquo;
reviews, tiene una función de activación binaria en el output&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;x = Dense(128, activation=&#39;relu&#39;)(x)
preds = Dense(2, activation=&#39;softmax&#39;)(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Una vez preparados los datos, y una vez definidas las capas de la red
neuronal se entrena:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model.fit(x_train, y_train,
          batch_size=128,
          epochs=10,
          validation_data=(x_val, y_val))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Este fit, obtiene un accuracy que supera el 90%. Un gran paso adelante.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
