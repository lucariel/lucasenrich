<!DOCTYPE html><html lang="es" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Lucas Enrich">

  
  
  
    
  
  <meta name="description" content="Review Rating
¿Cómo generar un puntaje númerico en base a un texto?
Mucho de lo expuesto es en realidad distintas formas de pensar el
problema y quedarse con la mejor solución.
Paseo por Doc2Vec, Regresiones lineales, randomForests y redes convolutivas
con GloVe
Read more ">

  
  <link rel="alternate" hreflang="es" href="/post/review_rating/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      
        
      

      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/post/review_rating/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Lucas Enrich">
  <meta property="og:url" content="/post/review_rating/">
  <meta property="og:title" content=" | Lucas Enrich">
  <meta property="og:description" content="Review Rating
¿Cómo generar un puntaje númerico en base a un texto?
Mucho de lo expuesto es en realidad distintas formas de pensar el
problema y quedarse con la mejor solución.
Paseo por Doc2Vec, Regresiones lineales, randomForests y redes convolutivas
con GloVe
Read more "><meta property="og:image" content="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="es">
  
    
    
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/review_rating/"
  },
  "headline": "",
  
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Lucas Enrich"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Lucas Enrich",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "\u003ch1 id=\"review-rating\"\u003eReview Rating\u003c/h1\u003e\n\u003cp\u003e¿Cómo generar un puntaje númerico en base a un texto?\u003c/p\u003e\n\u003cp\u003eMucho de lo expuesto es en realidad distintas formas de pensar el\nproblema y quedarse con la mejor solución.\u003c/p\u003e\n\u003cp\u003ePaseo por Doc2Vec, Regresiones lineales, randomForests y redes convolutivas\ncon GloVe\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://lucasenrich.netlify.com/post/review_rating/\"\u003eRead more \u003c/a\u003e\u003c/p\u003e"
}
</script>

  

  


  


  





  <title> | Lucas Enrich</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Buscar</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Buscar..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Lucas Enrich</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Barra de navegación">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Lucas Enrich</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Buscar"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1></h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jan 1, 0001
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    11 min de lectura
  </span>
  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <h1 id="review-rating">Review Rating</h1>
<p>¿Cómo generar un puntaje númerico en base a un texto?</p>
<p>Mucho de lo expuesto es en realidad distintas formas de pensar el
problema y quedarse con la mejor solución.</p>
<p>Paseo por Doc2Vec, Regresiones lineales, randomForests y redes convolutivas
con GloVe</p>
<p><a href="https://lucasenrich.netlify.com/post/review_rating/">Read more </a></p>
<p>En principio, lo que se va a ver es las calificaciones haciendo uso de
Doc2Vec para convertir el texto en un vector numerico y poder, con esos
vectores numericos como input, realizar la predicción de cual sería la
calificación que hubiera tenido según el texto. Haciendo uso de
algortimos de aprendizaje supervisado.</p>
<p>Por otro lado, dado que los datos no son tantos, lo que perjudica la
construcción del vector numerico a partir de los textos, se hará uso de
un &ldquo;word embedding&rdquo; ya entrenado y posteriormente se verá como mejora el
poder predictivo.</p>
<h3 id="los-datos-y-la-limpieza">Los datos y la limpieza</h3>
<p>Para empezar, veamos como se ven los datos:</p>
<table style="width:39%;">
<colgroup>
<col width="16%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th>review.rating</th>
<th>review.text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>5</td>
<td>Our experience at Rancho Valencia was absolutely perfect from beginning to end!!!! We felt special and very happy during our stayed. I would come back in a heart beat!!!</td>
</tr>
<tr class="even">
<td>5</td>
<td>Amazing place. Everyone was extremely warm and welcoming. We've stayed at some top notch places and this is definitely in our top 2. Great for a romantic getaway or take the kids along as we did. Had a couple stuffed animals waiting for our girls upon arrival. Can't wait to go back.</td>
</tr>
<tr class="odd">
<td>2</td>
<td>We booked a 3 night stay at Rancho Valencia to play some tennis, since it is one of the highest rated tennis resorts in America. This place is really over the top from a luxury standpoint and overall experience. The villas are really perfect, the staff is great, attention to details (includes fresh squeezed orange juice each morning), restaurants, bar and room service amazing, and the tennis program was really impressive as well. We will want to come back here again.</td>
</tr>
</tbody>
</table>
<p>En lugar de importar todos los paquetes juntos, vamos a ir importando a
medida que vayamos necesitando.</p>
<pre><code>import pandas as pd
cc = pd.read_csv('./hotel-reviews/Datafiniti_Hotel_Reviews.csv')
</code></pre>
<p>Vamos a seleccionar las columnas necesarias y cambiarle el nombre para
que sea mas facil luego seleccionarlas.</p>
<pre><code>cc = cc[['reviews.title','reviews.text','reviews.rating']]
cc.columns = ['titulo','comentarios','calificacion']
</code></pre>
<p>El primer paso a la hora de trabajar con estos textos, es reducir a la
maxima expresión la cardinalidad del vocabulario. ¿Que significa esto?
Si tenemos una gran cantidad de usos de un verbo, como por ejemplo,
&ldquo;correr&rdquo; en sus distintas conjugaciones, &ldquo;corría&rdquo;,&ldquo;corriendo&rdquo;,&ldquo;corrian&rdquo;
y queremos armar un listado de frecuencias de palabras, esto daria como
resultado que cada uno de esas palabras aparezca una sola vez; pero si
logramos que la referencia a la acción concreta de &ldquo;correr&rdquo; sume
independientemente de su conjugación, reduciríamos la cardinalidad de
nuestro diccionario, eso es para lo que se usa la <em>lematización</em>.</p>
<table>
<thead>
<tr class="header">
<th>Original</th>
<th>Lemmatizacion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>corrian</td>
<td>correr</td>
</tr>
<tr class="even">
<td>corriendo</td>
<td>correr</td>
</tr>
<tr class="odd">
<td>corrio</td>
<td>correr</td>
</tr>
</tbody>
</table>
<p>Luego, hay sustantivos y otras palabras que tienen la misma raiz y
dependiendo del sujeto, se puede reducir el tamaño del diccionario
cortando de la raiz la palabra, lo que se conoce como <em>stemmización</em></p>
<table>
<thead>
<tr class="header">
<th>Original</th>
<th>Stemmización</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>niña</td>
<td>niñe</td>
</tr>
<tr class="even">
<td>niño</td>
<td>niñe</td>
</tr>
<tr class="odd">
<td>niñe</td>
<td>niñe</td>
</tr>
</tbody>
</table>
<p>El proceso de lematización y stemización, en su conjunto, se puede
entender como normalizar el vocabulario y por lo tanto, una función que
se encarge de hacer estas dos cosas, puede llamarse <em>normalize(text)</em>:</p>
<pre><code>def normalize(text):
    from nltk.stem import PorterStemmer
    from nltk.tokenize import word_tokenize
    import unidecode
    import spacy
    nlp = spacy.load('en_core_web_sm')
    porter = PorterStemmer()
    doc = nlp(text)
    lemmas = [unidecode.unidecode(tok.lemma_.lower()) for tok in doc if not tok.is_punct ]
    #En este caso, estoy eliminando palabras con menos de 3 letras y las negaciones, esto no es necesario estrictamente, y depende mucho del caso de aplicación, a veces funciona, a veces no.
    lexical_tokens = [t.lower() for t in lemmas if (len(t) &gt; 3 or t ==&quot;no&quot;) and t.isalpha()]
    lexical_tokens = [porter.stem(t) for t in lexical_tokens]
    return lexical_tokens
</code></pre>
<p>Esta funcion tiene como input una frace y escupe objeto tipo list()
asique lo que haré es aplicarla a cada texto y despues volverla a unir
para que cada fila tenga un texto y no un array</p>
<pre><code>norm_token = []
for i in range(len(cc.comentarios)):
    try:
        a = normalize(cc.comentarios[i])
    except:
        a = ''
    norm_token.append(a)
norm_text = [' '.join(x) for x in norm_token]
cc['norm_text'] = norm_text
</code></pre>
<h3 id="doc2vec-generando-el-embedding-numérico">Doc2Vec: Generando el embedding numérico</h3>
<p>¿Porque no tratar directo con los tokens? Por la sencilla razón hay un
paquete que permite aplicar Doc2Vec, asociando un texto a una clase,
<em>gensim</em> nos va a venir bien para esto:</p>
<pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
#Primera la separacion entre test y train
train, test = train_test_split(cc, test_size=0.2, random_state=42)

train_tagged = train.apply(
    lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1)
test_tagged = test.apply(
    lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1)
</code></pre>
<p>Una vez que tenemos los elementos de train y test, hay que entrenar el
Doc2Vec, para, así pasar el texto a un vector númerico que pueda ser el
input del algoritmo de clasificación</p>
<pre><code>for epoch in range(30):

    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)
    model_dbow.alpha -= 0.002
    model_dbow.min_alpha = model_dbow.alpha
</code></pre>
<p>Con el modelo Doc2Vec entrenado, podemos darle pasar los textos y
obtener el vector numerico deseado. Ahora bien, para facilitar la
implementación del modelo después, generemos una función con dos ouputs,
el vector numerico por un lado, y el rating asociado a ese vector
numérico</p>
<pre><code>def vec_for_learning(model, tagged_docs):
    sents = tagged_docs.values
    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])
    return targets, regressors
</code></pre>
<p>Ahora que tenemos la función que nos genera el vector númerico
terminamos con el proceso de preparación de los datos. Es curioso notar
que siempre se dice que el 80% del trabajo consiste en la preparación y
limpieza de datos, y el 20% el modelado. Hasta ahora se puede ver que no
es una distinción tan discreta, sino que es continua. ¿A qué me refiero?
Bueno, para preparar los datos hizo falta algoritmos de embedding
(Doc2Vec). Y no es poco común que ocurran estas cosas.</p>
<p>Ahora bien, volvamos a lo nuestro, es hora de correr los algoritmos de
regresión:</p>
<h3 id="regresión">Regresión</h3>
<pre><code>from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression

y_train, X_train = vec_for_learning(model_dbow, train_tagged)
y_test, X_test = vec_for_learning(model_dbow, test_tagged)

lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)

print(np.sqrt(mean_squared_error(y_test, y_pred))) #1.15
</code></pre>
<p>Este resultado no me convence demasiado si consideramos al puntaje como
una regresión. El error cuadratico medio es del 1.15pts, para los que no
recuerdan, el error cuadratico medio toma la diferencia entre el valor
predecido y el valor real, lo eleva al cuadrado y de ello toma el
promedio. Les dejo un <a href="https://www.youtube.com/watch?v=8wgy8Vopv3E">video </a> de mi canal de Youtube con la visualización
de lo que significa</p>
<h3 id="clasificación">Clasificación</h3>
<p>Otra forma de entender el problema es como uno de clasificación. ¿Pero
como pasamos de un target continuo a uno discreto? Podríamos pensar que
los puntajes de 4 ó 5 son &ldquo;buenos&rdquo;, y asignarles un 1, y los de menos de
4 son &ldquo;malos&rdquo;, y asignarles un 0, y nos quedamos con un problema de
clasificación binaria.</p>
<p>Además, con estas conceptualización, tenes que volver a correr el
embedding porque los &ldquo;tags&rdquo; no son ahora los puntajes del 1 al 5 sino
que son {1,0}</p>
<pre><code>cl = []
for i in cc.calificacion:
    if i &lt;4:
        cl.append(0)
    else:
        cl.append(1)
cc.calificacion = cl

train, test = train_test_split(cc, test_size=0.2, random_state=42)

train_tagged = train.apply(
    lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1)
test_tagged = test.apply(
    lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1)
model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0)
model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])
for epoch in range(30):
    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)
    model_dbow.alpha -= 0.002
    model_dbow.min_alpha = model_dbow.alpha
</code></pre>
<p>Ahora si, sacamos los vectores del modelo Doc2Vec y lo fiteamos a un
randomForest clasificador:</p>
<pre><code>from sklearn.ensemble import RandomForestClassifier
y_train, X_train = vec_for_learning(model_dbow, train_tagged)
y_test, X_test = vec_for_learning(model_dbow, test_tagged)


rfr = RandomForestClassifier(n_estimators = 500, random_state = 42)
rfr.fit(X_train, y_train)
</code></pre>
<p>¿Cómo evaluamos esta clasificación? Bueno, primero nos fijamos cuanto
coincide la predicción respecto al valor real, para eso se usa la matriz
de confusión</p>
<pre><code>from sklearn.metrics import confusion_matrix
y_pred = rfr.predict(X_test)
tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0,1]).ravel()


np.mean(y_test == y_pred)
</code></pre>
<p>Nuestra precisión es del 72,4%, nada mal, solo un detalle. Si mandamos
un modelo que siempre diga &ldquo;1&rdquo;, tendremos una precisión equivalente a la
proporción de &ldquo;1&rdquo; en el set. Que en la base completa es de 72,8%. Es
decir, este modelo no mejor que decir que todas son igual a &ldquo;1&rdquo;.</p>
<h3 id="polaridad-como-métrica">Polaridad como métrica</h3>
<p>Otra alternativa puede ser extraer la polaridad del texto, herramienta
muy útil en los procesos de sentiment analysis. Y podríamos pensar que,
cuan más positivo sea la polaridad, estará asociado a un mejor puntaje</p>
<pre><code>from textblob import TextBlob 
pls = []
sbj = []
for i in range(len(cc.comentarios)):
    try:
        senti = TextBlob(cc.comentarios[i]) 
        polarity = senti.sentiment
        pls.append(polarity[0])
        sbj.append(polarity[1])
    except:
        pls.append(0)
        sbj.append(0)
        

polscore = [int(x &gt; 0) for x in pls] # Acá 0 es un valor arbitrario de corte, 
#un ejericio podría incluir la optimización de este valor como un hiperparametro. 
#La polaridad genera un indice de -1, 1. Siendo -1 cuan más negativo es, y 1 cuan más #positivo es, y polscore dice que aquellos que tienen valoracion positiva sean 1 y los demás 0
np.mean(polscore == np.array(cc.calificacion))
</code></pre>
<p>Sirve esto? Y, esto esta dando un resultado de 78.55%, es, a mi
sorpresa, una mejora respecto al punto anterior. Aunque todavía no es
satisfactorio.</p>
<p>Evidentemente el score de polarización agrega información.</p>
<h3 id="redes-neuronales-y-glove">Redes neuronales y GloVe</h3>
<p>Global Vectors ó <a href="https://nlp.stanford.edu/projects/glove/">GloVe </a> es una tecnica que, a diferencia de Doc2Vec, que
es un algortimo supervisado, es no-supervisado y obtiene embedings
númericos de palabras según estadisticas de co-ocurrencia. De esta
manera puede encontrar analogías tales como &ldquo;los que varón es a mujer,
rey es a reina&rdquo;.</p>
<p>Más allá las <a href="https://towardsdatascience.com/gender-bias-word-embeddings-76d9806a0e17">controversias </a>,
es una herramienta bastante útil para muchos casos</p>
<p>El objetivo de esta sección es ver como, haciendo uso de un modelo de
lenguage pre-existente, se puede usar el proceso de transfer-learning
para incorporar nuestros textos y sus calificaciones y adaptarlo a
nuestras necesidades.</p>
<p>Esto es interesante e importante porque hay muchos modelos de lenguage
que han hecho uso de datasets enormes en hardwares mucho más potentes
que los que podría pagar, que se puede aprovechar</p>
<pre><code>import os
import sys
import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.layers import Dense, Input, GlobalMaxPooling1D
from keras.layers import Conv1D, MaxPooling1D, Embedding
from keras.models import Model
from keras.initializers import Constant


BASE_DIR = os.getcwd()
GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B') #Este es el modelo pre-entrenado
TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')
MAX_SEQUENCE_LENGTH = 1000 #entrenar sobre oraciones de hasta estas palabras
MAX_NUM_WORDS = 20000 #tamaño máximo del vocabulario
EMBEDDING_DIM = 300 #dimensión del vector númerico resultante
VALIDATION_SPLIT = 0.2

#Volvemos a cargar la información
import pandas as pd
cc = pd.read_csv('./hotel-reviews/Datafiniti_Hotel_Reviews.csv')
cc = cc[['reviews.title','reviews.text','reviews.rating']]
cc.columns = ['titulo','comentarios','calificacion']

cl = []
for i in cc.calificacion:
    if i &lt;4:
        cl.append(0)
    else:
        cl.append(1)
cc.calificacion=cl

#Preparación de los datos
TEXT_DATA_DIR = cc.comentarios

embeddings_index = {}
with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, 'f', sep=' ')
        embeddings_index[word] = coefs

texts = [x for x  in cc.comentarios]  # listado de muestras de texto
labels_index = {1:1, 2:2, 3:3, 4:4, 5:5}  # diccionario mapeando los revies a los target
labels = [int(x) for x in cc.calificacion] # target

# Tokenizar las palabras
texts = np.array(texts)
tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index


#Con las palabras  tokenizadas, se hace el padding, para que todos tengan la misma longitud, para eso se agrega 0 hasta que se llene
data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

labels = to_categorical(np.asarray(labels))

indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])

x_train = data[:-num_validation_samples]
y_train = labels[:-num_validation_samples]
x_val = data[-num_validation_samples:]
y_val = labels[-num_validation_samples:]


num_words = min(MAX_NUM_WORDS, len(word_index) + 1)
embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))
for word, i in word_index.items():
    if i &gt;= MAX_NUM_WORDS:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

#Este es el proceso que genera los embeddings
embedding_layer = Embedding(num_words,
                            EMBEDDING_DIM,
                            embeddings_initializer=Constant(embedding_matrix),
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)
</code></pre>
<p>Ok, hasta ahi la preparación de los datos, es hora de entrenar una red
neuronal convolutiva con los embeddings realizados para obtener el
modelo que clasifique las reiews:</p>
<pre><code>sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)
x = Conv1D(128, 6, activation='relu')(embedded_sequences)
x = MaxPooling1D(5)(x)
x = Conv1D(128, 6, activation='relu')(x)
x = MaxPooling1D(6)(x)
x = Conv1D(128, 6, activation='relu')(x)
x = GlobalMaxPooling1D()(x)
x = Dense(128, activation='relu')(x)
preds = Dense(2, activation='softmax')(x)

model = Model(sequence_input, preds)
model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['acc'])
</code></pre>
<p>Hay mucho código repetido en este caso, pero eso es para simplicidad de
exposición, lo relevante a enteder es que así se define la capa
convolutiva, que esla que se repite:</p>
<pre><code>x = Conv1D(128, 6, activation='relu')(embedded_sequences)
x = MaxPooling1D(5)(x)
</code></pre>
<p>Y así la ultima capa, que como terminamos con las &ldquo;buenas&rdquo; y &ldquo;malas&rdquo;
reviews, tiene una función de activación binaria en el output</p>
<pre><code>x = Dense(128, activation='relu')(x)
preds = Dense(2, activation='softmax')(x)
</code></pre>
<p>Una vez preparados los datos, y una vez definidas las capas de la red
neuronal se entrena:</p>
<pre><code>model.fit(x_train, y_train,
          batch_size=128,
          epochs=10,
          validation_data=(x_val, y_val))
</code></pre>
<p>Este fit, obtiene un accuracy que supera el 90%. Un gran paso adelante.</p>
    </div>

    








<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/post/review_rating/&amp;text=" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/post/review_rating/&amp;t=" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=&amp;body=/post/review_rating/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/post/review_rating/&amp;title=" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=%20/post/review_rating/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/post/review_rating/&amp;title=" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/lucas-enrich/avatar_hu52a603635ecebd45650b162dadabb4e5_12861_270x270_fill_q90_lanczos_center.jpg" alt="Lucas Enrich">
      

      <div class="media-body">
        <h5 class="card-title"><a href="/">Lucas Enrich</a></h5>
        <h6 class="card-subtitle">Economia- UNLaM</h6>
        <p class="card-text">Mis intereses incluyen la inteligencia artificial, las ciencias sociales, la visualización de datos y la música</p>
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/lucasenrich" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/lucariel/" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/lucas-ariel-enrich-a7320a93/" target="_blank" rel="noopener">
        <i class="fas fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.youtube.com/channel/UCpq1UzLu2s4vSbghkN3mIHw" target="_blank" rel="noopener">
        <i class="fas fa-youtube"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  














  
  





  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No se encontraron resultados","placeholder":"Buscar...","results":"resultados encontrados"};
      const content_type = {
        'post': "Posts",
        'project': "Proyectos",
        'publication' : "Publicaciones",
        'talk' : "Charlas",
        'slides' : "Diapositivas"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.37431be2d92d7fb0160054761ab79602.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  <p class="powered-by">
    
  </p>

  
  






  <p class="powered-by">
    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Citar</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copiar
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Descargar
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
