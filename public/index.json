[{"authors":["admin"],"categories":null,"content":"Estudie Economia en la Universidad Nacional de la Matanza, luego de haber realizado varios cursos de forma online y presencial comencé a trabajar como consultor freelance en ciencia de datos, desempeñandome en el area de diseño, real state, NLP para clientes\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"es","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/lucas-enrich/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/lucas-enrich/","section":"authors","summary":"Estudie Economia en la Universidad Nacional de la Matanza, luego de haber realizado varios cursos de forma online y presencial comencé a trabajar como consultor freelance en ciencia de datos, desempeñandome en el area de diseño, real state, NLP para clientes","tags":null,"title":"Lucas Enrich","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":null,"content":"Interpretación bayesiana de la matriz de confusión ¿Cómo evaluar modelos si solo tenemos la matriz de confusión?\nEsta interpretación pertenece originalmente a un paper homónimo de Olivier Caelen (2000).Pero es una interesante forma de entender la matriz de confusión, por lo que esto será una traducción/interpretación/resumen de aquel.\nRead more \nPuedes leer ese articulo aqui: http://www.oliviercaelen.be/doc/confMatrixBayes_AMAI.pdf\nLa matriz de confusión puede ser a veces muy confusa, falsos positivos, falsos negativos, verdaderos positivos, verdaderos negativos; y todas las métricas que salen de ahi, el recall, el accuracy, el F-score.\nTodas ellas se basan en la evidencia concreta de los set testeados, pero no indican cuanta incertidumbre hay en tal o cual indicador.\nSupongamos que comparamos dos modelos, uno tiene 72% de precisión y el otro 70%. Dado que se trabaja con datos muestreados aleatoriamente (ya sea por el muestreo de dataset usado para hacer el modelo, ya sea por train-test split), los valores que terminan en la matriz de confusión también son fruto de un proceso aleatorio.\nPara las tecnicas bayesianas, cualquier estimación es contingente a los datos obtenidos y a las creencias que tiene quien investiga. Asimismo, todo lo estimado tiene una distribución especifica que puede ser investigada.\nLa matriz de Confusión Dado un dataset, se genera un modelo que mapea X a Y, esto puede ser escrito como: $h: x y $, pero a veces se le pifia, y a veces no.\nEl diagrama que implica la matriz de confusión es:\nPero podemos pensar en la matriz de confusión como un vector de valores:\nV = (n°T**P, n°T**N, n°F**P, n°F**N)\nEl planteo Bayesiano Este vector númerico, V, puede ser entendido como que vino de una función de distribución multinomial, la cual es una generalización de la binomial, solo que en lugar de tener dos valores posibles los cuales tienen una probabilidad cada uno, hay cuatro los cuales dependen de los parametros de la distribución binomial.\nV − Mul**t(NT, θ)\nDonde:\nθ = (θt**p, θt**n, θf**p, θf**n)\nSon los parametros que determinan la realización de los valores dentro de V, es decir v depende de θ.\nEntonces, si los valores de la matriz de confusión son aleatorios con una función de distribución de probabilidad dada, la probabilidad de obtener un set de valores v que llene el vector que contiene los valores de la matriz de confusión V puede escribirse como P(V = v), en este caso, se considera que θ es fijo.\nPero si θ proviene de una variable aleatoria, esto es, no es fijo, P(V = v) se vuelve contingente a los valores que pueda tomar θ, por lo que se podria escribirse:\nP(V = v|Θ = θ)\nY de acá proviene el planteo bayesiano, yo quiero conocer Θ, pero solo veo las realizaciones de la matriz de confusión v, por lo que no quiero P(V = v|Θ = θ), sino P(Θ = θ|V = v), lo cual se puede escribir según la regla de bayes:\n$$ f_{\\Theta|V}(\\theta|v) = \\frac{P(V=v|\\Theta=\\theta)*f_{\\Theta}(\\theta)}{P(V=v)} $$\nEste planteo permite comparar distintos modelos sin otra necesidad que los valores de la matriz de confusión.\nEl prior En analisis bayesiano hay distribuciones que van de la mano, que son como amigas. Es decir, si mis variables tienen una funcion de distribución determinada, los parametros tienen tal otra, se llaman conjugados (en rigor el conjugado es que la posterior y la likelihood pertenecen a la misma distribución)\nEl amigo de la distribución multinomial, es la Dirichlet.\nEn nuestro caso, v sigue una distribución multinomial, y θ una distribución Dirichlet.\nV − Mul**t(NT, θ) Θ − Dir(α)=Dir((α1, α2, α3, α4))\nAhora bien, ¿Que es α? Bueno, α no es mas ni menos que el lugar donde se mete el prior, porque dado mi matriz de confusión v, la posterior de Θ esta dada por:\nΘ|ω = Dir((v1 + α1, v2 + α2, v3 + α3, v4 + α4)) = Dir(ω)\nEntonces, habiendo visto los datos v, y habiendo metido nuestro conocimiento previo α (que puede ser la matriz de confusión de otro modelo, o del mismo modelo con nuevos datos, o el prior relevante al caso concreto), y la posterior Dir(ω) nos va a dar los parametros que correspondan darle a la multinomial que nos da las distribuciones de cada uno de los elementos de la matriz de confusión.\n¿Como se vuelve operativo esto?\nSimulando, si tenemos los parametros de Dir(ω) como resultado de nuestro analisis, no vamos a tener los θ, sino que podemos extraer los parametros de θ con una frecuencia que refleje la distribución de Dir(ω).\nPara simplificar podemos elegir una métrica a evaluar, en este caso, el accuracy, que depende de matriz de confusión A(v)\nEsto es un algoritmo Monte Carlo\nfor i in 1:M:\n  Vamos a la caja Dir(ω) y sacamos los θi\n  Ponemos los θi en P(V = v|Θ* = *θ*) y sacamos los *v**i**\n  Sacamos A(vi) y lo guardamos en una lista\n  Esto va a dar una lista de un montón (M) de A(v), este listado tiene sus propios estadisticos, su media y su varianza.\n¿Confuso? Seguro, no te culpo, yo también lo estoy y eso que estoy ecribiendo esto, pero veamos concretamente como se puede aplicar esto en la práctica\n*Ejemplo 1:*Comparando dos modelos Supongamos que, de un mismo dataset, hicimos, ponele, un randomForest (R) y un NaiveBayes (N) y la matriz de confusión de c/u es:\n$$ v^R=\\begin{bmatrix}65 \u0026amp; 15 \\\\ 35 \u0026amp; 30\\end{bmatrix} = (65,30,35,15) $$ $$ v^N=\\begin{bmatrix}50 \u0026amp; 30 \\\\ 30 \u0026amp; 35\\end{bmatrix} = (50,35,30,30) $$ Vamos a decir, que no tenemos información previa, porque lo que estamos haciendo es comparar dos modelos, es decir que α en ambos casos, será (0, 0, 0, 0)\nlibrary(DirichletReg) library(extraDistr) ## The following objects are masked from 'package:DirichletReg': ## ## ddirichlet, rdirichlet M = 1000 alpha = c(0,0,0,0) #Para v_R v_R = c(65,30,35,15) omega_R = alpha + v_R A_R = c() for(i in 1:1000){ theta_i = t(rdirichlet(1,omega_R)) v_i = rmnom(1, sum(v_R), as.vector(theta_i)) A_i = (v_i[1]+v_i[2])/sum(v_i) A_R = c(A_R,A_i) } #Para v_N v_N = c(50,35,30,30) omega_N = alpha + v_N A_N = c() for(i in 1:1000){ theta_i = t(rdirichlet(1,omega_N)) v_i = rmnom(1, sum(v_N), as.vector(theta_i)) A_i = (v_i[1]+v_i[2])/sum(v_i) A_N = c(A_N,A_i) }  Ahora se pueden visualizar las distribuciones resultantesde estas simulación\nEn este caso, puede verse que el mejor modelo es el randomForest (R), ya que su distribución está más a la derecha.\nDe aquí puede extraerse también los intervalos de confianza y (de credibilidad para el marco bayesiano) y otras cuestiones.\nOtra cosa para lo cual se puede usar este método es para evaluar de forma muy barata cuan vigente es un model en funcionamiento a nueva información.\nSi nos matamos haciendo un modelo que llego a un accuracy del 97% en el set de test y un día con información nueva, llega a un accuracy del 90%, es posible saber que tan probable sería encontrar tal resultado o si el modelo necesita ser re-entrenado, es solo cuestión de que α = (M**C0) donde M**C0 es la matriz de confusión al momento 0.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":-62135596800,"objectID":"1c9e25fb8e3a1d588f22ba475530f0af","permalink":"/post/bayes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/bayes/","section":"post","summary":"Interpretación bayesiana de la matriz de confusión ¿Cómo evaluar modelos si solo tenemos la matriz de confusión?\nEsta interpretación pertenece originalmente a un paper homónimo de Olivier Caelen (2000).Pero es una interesante forma de entender la matriz de confusión, por lo que esto será una traducción/interpretación/resumen de aquel.\nRead more \n","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"Clusterizar diseños ¿Como hago para clasificar estilos de banners?\nCon tantos adds dando vueltas en internet, sigue siendo, muchas veces, un proceso relativamente manual y poco estandarizado los diseños. Para eso existen diseñadores.\nPero cuando ya tienen miles realizados, esta bueno mirar atrás e identificar patrones recurrentes. Saber lo que venimos haciendo sirve para mirar hacia adelante.\nRead more \nInput data\nEl problema tenia originalmente cientos de diseños, en este ejemplo, dado que tiene que ver con algoritmos de clusterización, se usaran algunos hechos ad hoc, por lo que el ejemplo es con tamaños reducidos y la clasificacion de imagenes, fuentes de texto y demás quedan afuera. Lo que queremos saber es si la ubicación de los elementos en la imagen sigue un patron en particular.\nLos datos venian de la siguiente forma:\nDonde:\n  y : Distancia desde arriba \n  x : Distancia desde la izquierda \n  w : Ancho (width) \n  h : Alto (height) \n  Si tenemos 50 ejemplos, con 3 elementos cada uno, y hay 4 variables por elemento, la forma del input es 50 × 3 × 4, esto, a los algoritmos, no les gusta demasiado. Asique fue necesario achatar la base para obtener una base de datos de 50 × 12, para lo cual:\nPrimero se agarra cada uno (una base de 1 × 3 × 4) y se la transforma en una sola fila 1 × 12 para lo cual se uso el código:\nlibrary(tidyverse) cols_used = c('element_top', 'element_left', 'element_width', 'element_height') spread_file\u0026lt;-function(data, cols_used){ cols_used_a = c('element_name',cols_used) y=data[cols_used] h = data[cols_used_a] z=c(1,1,1,1) for(i in 1:nrow(y)) { z = cbind(z,y[i,]) } z = z[1,-1] newcols \u0026lt;- c() for (i in h['element_name']){ newcols\u0026lt;-cbind(newcols,paste(i,cols_used[1], sep = '.')) newcols\u0026lt;-cbind(newcols,paste(i,cols_used[2], sep = '.')) newcols\u0026lt;-cbind(newcols,paste(i,cols_used[3], sep = '.')) newcols\u0026lt;-cbind(newcols,paste(i,cols_used[4], sep = '.')) } newcols2\u0026lt;-c() for(i in 1:nrow(newcols)) { for(j in 1:4){ newcols2\u0026lt;-c(newcols2,newcols[i,j]) } } colnames(z)\u0026lt;-newcols2 n\u0026lt;-as_vector(data['id']) z['id']\u0026lt;-n[1] z }  Lo que transforma cada elementos con la forma:\n$$\\begin{bmatrix} elem1 \u0026amp; y_1 \u0026amp; x_1 \u0026amp; w_1 \u0026amp; h_1 \\\\ elem2 \u0026amp; y_2 \u0026amp; x_2 \u0026amp; w_2 \u0026amp; h_2 \\\\ \\vdots \\\\ elemk \u0026amp; y_2 \u0026amp; x_k \u0026amp; w_k \u0026amp; h_k \\end{bmatrix}$$ a la forma:\n$$\\begin{bmatrix} id.1 \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\end{bmatrix}$$ Así se pueden apilar todos elementos de la muestra para quedar una sola base de datos con la forma:\n$$\\begin{bmatrix} id.1 \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\\\ id.2 \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\\\ \\vdots \\\\ id.N \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\end{bmatrix}$$\n Reduccion de dimensionalidad + Clustering   La forma más directa para realizar la tarea de clusterización es simplemente usando el paquete dbscan y correrlo sobre nuestra base transformada. Pero no funcionó del todo. Asique lo siguiente fue reducir la dimensionalidad de los objetos, en este caso se uso el algoritmo UMAP y luego se hizo la clusterización.\nPCA (componentes principales), es el algoritmo más popular para reducir la dimensionalidad. Luego esta t-SNE.\nEn pocas palabras, UMAP fue la mejor opción porque utiliza un algoritmo rapido que preserva mejor la estructura global.\nY finalmente:\nlibrary(umap) library(dbscan) umap_data\u0026lt;- umap(data) cl \u0026lt;-hdbscan(x = umap_data, minPts = 3)  Pero también habia un tema de escala, quiza el diseño era parecido pero un banner era dos veces mas grande, por lo que el resultado no era adecuado.\nSurge la necesidad de transformar los datos\nOpciones \n  Estandarizacion (z-score): Representa el numero de desvios estandar arriba o debajo del valor resultante. Útil para variables normalmente distribuidas \n  Normalizacion (min-max scaler): Permite llevar los valores entre 0 y 1. Útil para comparar variables de diferentes ordenes de magnitud (Precio de una casa y los m2 que ocupa) \n  ¿Puedo usar estas transformaciones en estos datos?\n No, como las variables describen dimensiones (alto y ancho), y posicion en el espacio no le encontré mucho sentido a la estandarizacion ni la normalizacion.    ¿Que podría hacer? En lugar de ver las posiciones y dimensiones absolutas, ver las posiciones y dimensiones relativas, lo que voy a llamar \u0026ldquo;normalizacion geometrica\u0026rdquo;  normalize_geometric\u0026lt;-function(df){ df['total_area']\u0026lt;-max(df['element_height'])*max(df['element_width']) df['rel_area']\u0026lt;-df['element_height']*df['element_width']/df['total_area'] df['orientation']\u0026lt;-df['element_height']/df['element_width'] df['element_top_relative']\u0026lt;-df['element_top']/max(df['element_height']) df['element_left_relative']\u0026lt;-df['element_left']/max(df['element_width']) df }    x\u0026rsquo; es la proporcion de x respecto al rango total (ancho del canvas)  mi nueva variable x\u0026rsquo; es: la linea roja dividida la linea azul \n y\u0026rsquo; es la proporcion de y respecto al rango total (alto del canvas)  mi nueva variable y\u0026rsquo; es: la linea roja dividida la linea azul \n areaRelativa es la proporcion del area del elemento respecto al total  mi nueva variable areaRelativa es: el area del cuadrado chiquito dividido la del rectangulo grande \n disposicion (dividiendo alto por acho) es para saber si el elemento es horizontal, vertical, o cuadrado  mi nueva variable disposicion es: el alto dividido por el ancho \nResultados Para empezar a evaluar los resultados, todo \u0026ldquo;spread\u0026rdquo; tiene que tener su \u0026ldquo;gather\u0026rdquo;:\ngather_file\u0026lt;-function(gdf){ x\u0026lt;-strsplit(colnames(gdf), '\\\\.') element_name=unique(unlist(map(x, 1))) original_cols=unique(unlist(map(x, 2))) gdf1\u0026lt;-data.frame(element_name) gdf1[original_cols[1]]\u0026lt;-0 gdf1[original_cols[2]]\u0026lt;-0 gdf1[original_cols[3]]\u0026lt;-0 gdf1[original_cols[4]]\u0026lt;-0 rel_area\u0026lt;-c() orientation\u0026lt;-c() element_top_relative\u0026lt;-c() element_left_relative\u0026lt;-c() for(i in seq(from=1, to=length(gdf), by=4)){ # stuff, such as rel_area=c(rel_area,gdf[i]) orientation=c(orientation,gdf[i+1]) element_top_relative = c(element_top_relative,gdf[i+2]) element_left_relative = c(element_left_relative,gdf[i+3]) } gdf1['rel_area']=as_vector(unlist(rel_area)) gdf1['orientation']=as_vector(unlist(orientation)) gdf1['element_top_relative']=as_vector(unlist(element_top_relative)) gdf1['element_left_relative']=as_vector(unlist(element_left_relative)) gdf1}  En principio veamos como quedaron los grupos sin normalizar y con la normalización\nY finalmente, un par de ejemplos por grupo:\nPrimer cluster:\n000003.png\nSegundo cluster:\nTercero:\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":-62135596800,"objectID":"948608562215d2782146bbf54fa61e41","permalink":"/post/clusterizar-disenos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/clusterizar-disenos/","section":"post","summary":"Clusterizar diseños ¿Como hago para clasificar estilos de banners?\nCon tantos adds dando vueltas en internet, sigue siendo, muchas veces, un proceso relativamente manual y poco estandarizado los diseños. Para eso existen diseñadores.\nPero cuando ya tienen miles realizados, esta bueno mirar atrás e identificar patrones recurrentes. Saber lo que venimos haciendo sirve para mirar hacia adelante.\nRead more \n","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"De-duplicación de Avisos Cualquiera que haya hecho un curso introductorio o leído un libro de Ciencia de Datos sabe que se dice mucho que la limpieza de un dataset es, por lo menos el 80% del trabajo y luego el modelado. Pero eso no quiere decir que no hagan falta técnicas propias del modelado para limpiar un dataset.\nRead more \nCon un scrap hecho a ZonaProp, de las unidades en alquiler en Recoleta en febrero 2020, luego de una exahustiva limpieza de unificación de la moneda, de corrección de datos implícitos (NA\u0026rsquo;s que implicaban un dato, por ejemplo, cocheras-si es NA, cocheras=0).\nEl problema que quedó fue que el resultado es una base de datos de avisos, pero, ¿que pasa si yo quiero que mi base de datos sea de inmuebles? El problema que me encuentro es que hay avisos duplicados, ya sea porque vuelven a estar publicados o porque un mismo inmueble es publicado por más de una inmobiliaria.\n¿Cómo se pueden detectar sistematicamente esas duplicaciones? Debajo hay un método posible\nLos Datos library(tidyverse) alqs\u0026lt;-read_csv(\u0026quot;alq_feb20_recoleta.csv\u0026quot;)  Bien, este es el dataset. Una primera idea sería: si dos publicaciones se parecen lo suficiente, sospechamos que son la misma.\nLas columnas son\n  Columna Descrpción    WEB La url de la publicación  Provincia La provincia de la publicación  Tipo_Op Si corresponde a venta o alquiler  Tipo Si es casa, comercio,oficina, PH, departamento, etc  Zona El barrio, para CABA, el municipio para las provincias  Dirección La dirección de la publicación  Latitud y Longittud Georreferencia  Inmobiliaria Quien está publicando  Tiempo Cuanto tiempo lleva publicado en días  Cochera Si tiene cochera, boolean  Expensas Cuanto se paga de expensas  Prices El precio de venta/alquiler  Antigüedad Cuantos años tiene de construido  Metros Tamaño en metros cuadrados  Ambientes Cantidad de Ambientes  Descripción El texto de la descripción  Baños Cantidad de baños    Ahora bien, cualquier procedimiento de detección de duplicados requiere necesariamente cierta flexibilidad, sino buscamos aquellos identicos y listo; pero la realidad es que la duplicación en general va a tener un motivo en concreto, sea por corrección de algún dato en particular, sea por ponerlo en otra inmobiliaria, sea por cambio de la descripción para que sea más atractiva. Pero por otro lado, revisar todos los pares posible nos va a llevar a que, si N es la cantidad de publicaciones en el dataset, los chequeos sean N2. Manualmente, esto, es inviable.\nEntonces, lo que se puede hacer es:\n  Ver la tasa de variación de las variables numéricas.\n  Verificar la distancia (aprovechando que estan georreferenciados)\n  Ver cuanto se parecen las descripciones\n  Tasa de variacion de las variables numericas Una opción sería considerar solo las variables que son iguales, pero eso descarta la posibilidad que haya una corrección en los datos, ni hablar si alguno de los datos se considera missing (N**A).\nUna cuestión antes de arrancar con esta sección, se estará generando una matriz simétrica para cada una de estas variables númericas. En la cual xi**j va a ser la tasa de variación entre las observaciones; y por lo tanto la diagonal cuando i = j, xi**j = 0\nEmpecemos\npmx\u0026lt;-matrix(nrow=nrow(alqs1), ncol=nrow(alqs1)) for(i in 1:nrow(alqs)){ for(j in 1:nrow(alqs)){ h= abs(alqs3$Prices[i]-alqs3$Prices[j])/max(alqs3$Prices[i],alqs3$Prices[j], na.rm = T) pmx[i,j] = h } }  ¿Qué está mal con esta aproximación? Que es un loop anidado, por lo que el Big O, es cuadrático, la forma menos eficiente de llenar una matriz. Esto quiere decir, que el tiempo que tarda el terminar este loop depende cuadráticamente del tamaño de las filas.\nSe puede aprovechar las operaciones vectorizadas que tiene R-Base para quedarnos con una Big O lineal.\npmx\u0026lt;-matrix(nrow=nrow(alqs1), ncol=nrow(alqs1)) for(i in 1:nrow(alqs)){ pmx[i,] = abs(alqs3$Prices[i]-alqs3$Prices)/max(alqs3$Prices[i],alqs3$Prices, na.rm = T) }  Esto es mejor, pero R tiene su propio diálecto para tratar con loops y es la facilidad que tiene para trabajar vectorialmente y la familia de funciones de apply, y de paso ponemos en práctica la máxima de evitar los loops\nrel_dif = function(a,b){ abs(a-b)/max(a,b, na.rm = T) } pmx\u0026lt;-sapply(FUN = rel_dif, alqs1$Prices,alqs1$Prices)  Ahora que podemos obtener la matriz de diferencias con una sola linea de código, hagamoslo para todoas las variables númericas.\nmts.mx\u0026lt;-sapply(FUN = rel_dif, alqs1$Metros,alqs1$Metros) #Metros cuadrados antig.pmx\u0026lt;-sapply(FUN = rel_dif, alqs1$Antiguedad,alqs1$Antiguedad) #Antigüedad  Solo una cosa, lo que queremos es ver cuan cerca están, pero hasta ahora vimos la variación, por lo que la cercacia va a estar dada por:\npmx\u0026lt;-1-as.matrix(pmx) pmts\u0026lt;-1-as.matrix(pmts) pantg\u0026lt;-1-as.matrix(pantg)  Distancia geográfica Para calcular la distancia geográfica vamos a estar necesitando el paquete geosphere para lo cual se usa el criterio de la distancia entre dos puntos en una superficie esférica, llamada la distancia Haversine.\nlatlongs\u0026lt;-alqs1 %\u0026gt;% select(Latitud,Longitud) dm \u0026lt;- distm(latlongs,latlongs, fun=distHaversine)  La función llamada distm es realmente útil porque viene con una implementación vectorizada,lo cual soluciona de entrada el problema de los potenciales loops anidados que teniamos antes.\nDescripción Bueno, llegamos al punto álgido, y por esto me refiero a que el criterio para determinar que tan cerca están dos descripciones no es tan obvio como la distancia o la variación relativa de alguna variable, porque estamos tratando ahora con variables no numéricas; por lo tanto antes de realizar la comparación necesitamos convertir a ese texto en un vector numérico, para lo cual existen varias vías.\nLa utilizada en este caso fue T**F − IDF que significa \u0026ldquo;term frequency–inverse document frequency,\u0026quot;, el cual genera un vector numérico ponderado por la importancia de cada palabra en el texto (term frequency) y la frecuencia de la palabra en todos los textos (inverse document frequency)\nEs decir, la ponderación indica que no todas la palabras tienen el mismo peso para describir una descripción, sino que hay algunas que deben ponderarse más y otras que deben ponderarse menos. En el caso de una descripción de un inmueble, las palabras que son más comunes a todas las descripciones, como pueden ser \u0026ldquo;baño\u0026rdquo; ó \u0026ldquo;cocina\u0026rdquo;, son ponderadas menos que las palabras que son menos frecuentes a cada descripción, como \u0026ldquo;a tres cuadras de la estacion Primera Junta\u0026rdquo;.\nEste score esta definido por $$ TFIDF_{xy} = TF_{xy}*log\\frac{N}{df} $$ donde:\n  T**Fx**y es la frecuencia de la palabra x en la descripción y\n  N es el número total de descripciones\n  d**f es el número total de documentos que contienen la palabra x\n  Por suerte, python tienen una libreria para (casi)todo, por lo que esta vectorización require solo unas pocas lineas de código; desafortunadamente, el texto está \u0026ldquo;sucio\u0026rdquo; y debe ser limpiado antes de la vectorización.\n#Eliminar caracteres especiales y espacios innecesarios cleanFun \u0026lt;- function(htmlString) { #Saco los tags de html t=(gsub(\u0026quot;\\t\u0026quot;,\u0026quot;\u0026quot;,gsub(\u0026quot;\\n\u0026quot;,\u0026quot;\u0026quot;,gsub(\u0026quot;\u0026lt;.*?\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, htmlString)))) #Lo paso a minuscula t=tolower(t) #Le saco los caracteres no alfanumericos t=str_replace_all(t, \u0026quot;[[:punct:]]\u0026quot;, \u0026quot; \u0026quot;) t } #Tokenización y eliminación de stopwords prepTx \u0026lt;- function(tx){ t1=word_tokenizer(cleanFun(tx)) t2 = t1[!(t1%in%stopwords(kind=\u0026quot;es\u0026quot;))] t2 = unlist(t2)[nchar(unlist(t2))\u0026gt;2] t2 }  Ok, ahora que el text está limpio, es hora de generar la conversión a numeros\nlibrary(tcR) prep_fun \u0026lt;- cleanFun tok_fun \u0026lt;- word_tokenizer smp_size\u0026lt;-floor(0.75*length(descripciones)) set.seed(123) train_ind \u0026lt;- sample(seq_len(length(descripciones)), size = smp_size) train \u0026lt;- descripciones it_train \u0026lt;- itoken(train, preprocessor = prep_fun, tokenizer = tok_fun, progressbar = TRUE) vocab \u0026lt;- create_vocabulary(it_train) pruned_vocab = prune_vocabulary(vocab, term_count_min = 100, doc_proportion_max = 0.5, doc_proportion_min = 0.001) vectorizer \u0026lt;- vocab_vectorizer(pruned_vocab) dtm_train \u0026lt;- create_dtm(it_train, vectorizer) tfidf = TfIdf$new() dtm_transformed = tfidf$fit_transform(dtm_train)  Ahora dtm_transformed es nuestra variable que contiene los vectores numericos para cada descripción. Como son vectores, una forma sencilla de compararlos es usando la distancia coseno.\nd1_d2_tfidf_cos_sim = sim2(x = dtm_transformed, method = \u0026quot;cosine\u0026quot;, norm = \u0026quot;l2\u0026quot;)  Y ahora si, nuestra matriz de similitud d1_d2_tfidf_cos_sim nos permite comparar las descripciones.\nUnificación de las matrices de similitud Ahora que tenemos todas las matrices que comparan todos los registros con todos los demás, es hora de unificarlas.\nA\u0026lt;-pmx B\u0026lt;-pmts C\u0026lt;-pantg D\u0026lt;-d1_d2_tfidf_cos_sim E\u0026lt;-dm colnames(A)\u0026lt;-alqs3$id rownames(A)\u0026lt;-alqs3$id colnames(B)\u0026lt;-alqs3$id rownames(B)\u0026lt;-alqs3$id colnames(C)\u0026lt;-alqs3$id rownames(C)\u0026lt;-alqs3$id colnames(D)\u0026lt;-alqs3$id rownames(D)\u0026lt;-alqs3$id colnames(E)\u0026lt;-alqs3$id rownames(E)\u0026lt;-alqs3$id Aa\u0026lt;-as.data.frame(as.table(A)) %\u0026gt;% distinct(Var1,Var2, .keep_all = T) Bb\u0026lt;-as.data.frame(as.table(B)) %\u0026gt;% distinct(Var1,Var2, .keep_all = T) Cc\u0026lt;-as.data.frame(as.table(C)) %\u0026gt;% distinct(Var1,Var2, .keep_all = T) Dd\u0026lt;-as.data.frame(as.table(D)) %\u0026gt;% distinct(Var1,Var2, .keep_all = T) Ee\u0026lt;-as.data.frame(as.table(E)) %\u0026gt;% distinct(Var1,Var2, .keep_all = T) colnames(Aa)[3]='precios' colnames(Bb)[3]='metros' colnames(Cc)[3]='antiguedad' colnames(Dd)[3]='descripcion' colnames(Ee)[3]='dist_mts' AB\u0026lt;-right_join(Aa,Bb, by = c('Var1','Var2')) CD\u0026lt;-right_join(Cc,Dd, by = c('Var1','Var2')) ABCD\u0026lt;-right_join(AB,CD,by = c('Var1','Var2')) ABCDE\u0026lt;-right_join(ABCD,Ee,by = c('Var1','Var2')) ABCDE\u0026lt;-ABCDE %\u0026gt;% filter(Var1!=Var2) %\u0026gt;% as_tibble()  Ahora ABCDE es nuestra dataframe con todos los pares de i**d′s, y las columnas con cada criterio de similitud se agregan en la misma fila\nLo que sigue es un poco arbitrario y seguro hay mejores métodos para hacerlo. Pero nuestros candidatos a duplicados son aquellos que, decimos, cumplen los siguientes criterios (en simultaneo):\n  Estan a menos de 300mts entre sí.\n  Tienen una variación en Metros menor al 10%\n  Tienen una variación en Precio menor al 25%\n  Tienen una variación en la descripcion menor al 30%\n  Tienen una variación en antigüedad menor al 10%\n  En todos los casos se mantienen los resultados NA porque eso nos indica que podría haberse agregado el dato.\ncand_dupls\u0026lt;-ABCDE %\u0026gt;% filter(dist_mts\u0026lt;=300 | is.na(dist_mts)) %\u0026gt;% filter(metros\u0026gt;0.9| is.na(metros)) %\u0026gt;% filter(precios\u0026gt;0.75| is.na(precios)) %\u0026gt;% filter(descripcion\u0026gt;0.7| is.na(descripcion))%\u0026gt;% filter(antiguedad\u0026gt;0.90| is.na(antiguedad))  El resultado de esto es de 840 pares que son potencialmente duplicados. Lo cual es una reducción bastante drástica de los 17162 que llevaría revisar todos los registros de una base de 1716 registros.\nFinalmente, hay un paso más que puede hacerse para que el proceso sea más robusto y es considerar la propiedad transitiva de los pares, realizar un network analysis, lo que significa agrupar todos aquellos id\u0026rsquo;s que tienen suficiente similitud, para eso generamos una variable que sea 1 para aquellos pares que cumplen los criterios y 0 para aquellos que no:\ncand_dupls\u0026lt;-cand_dupls %\u0026gt;% mutate( potdup = case_when( (precios \u0026gt; 0.75 | is.na(precios)) \u0026amp; (metros \u0026gt; 0.9 | is.na(metros)) \u0026amp; (antiguedad \u0026gt; 0.9 | is.na(antiguedad)) \u0026amp; (descripcion \u0026gt; 0.9 | is.na(descripcion)) \u0026amp; (dist_mts\u0026lt;300 | is.na(dist_mts)) ~ 1, T ~ 0 ) )  Lo que tenemos ahora, es un dataframe que tiene los pares que creemos que son duplicados, estos van a ser nuestros vinculos en el analisis de red, los edges:\nedges\u0026lt;-cand_dupls %\u0026gt;% select(Var1, Var2, potdup)  Que tienen esta forma\n  ID1 ID2 match    45557690 44375852 1  44349226 45589067 1  45620618 44714730 1    Que es la forma que acepta el paquete igraph para generar el grafo que va a vincular el id como en la imagen debajo, donde\n 6,9,10 y 4 serían un anuncio separado del resto, por ejemplo  library(igraph) g \u0026lt;- graph_from_data_frame(edges) fc \u0026lt;- fastgreedy.community(as.undirected(g))  Ahora fc es una lista en el que cada elemento es un vector de id\u0026rsquo;s que corresponderían al mismo inmueble para ver por ejemplo el grupo 3 podemos hacer:\nalqs %\u0026gt;% filter(id%in% as.numeric(fc[[3]])) %\u0026gt;% View()  Los links asociados al grupo 3, entonces, son:\n[1] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/arenales-y-callao-excelente-edificio-de-estilo-44714730.html\u0026rdquo;\n[2] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/arenales-1700-43499494.html\u0026rdquo; [3] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/petit-hotel-arenales-y-callao-700-m-sup2--lote-propio-45138045.html\u0026rdquo; [4] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/petit-hotel-arenales-y-callao-700-m-sup2--lote-propio-45564366.html\u0026rdquo; [5] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/petit-hotel-arenales-y-callao-700-m-sup2--lote-propio-45564348.html\u0026rdquo;\nAl grupo 10:\n[1] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/piso-pueyrredon-y-guido-191-m-sup2--una-o-dos-44047718.html\u0026rdquo;\n[2] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/pueyrredon-y-guido-piso-191-m-sup2--1-o-2-cocheras-44748468.html\u0026rdquo; [3] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/piso-pueyrredon-y-guido-191-m-sup2--una-o-dos-45455991.html\u0026rdquo;\n[4] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/pueyrredon-2400-45078667.html\u0026rdquo;\nPero no todo es un éxito, tambien existen grupos tales como el 1, en el que hay más de uno:\n[1] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/en-alquiler-temporario-departamentos-tipo-lofts-de-47-40313161.html\u0026rdquo; [2] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/departamentos-en-alquiler-temporario-posadas-1300-40075639.html\u0026rdquo;\n[3] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/alquiler-loft-m-sup2--47-posadas-1323-recoleta-amobl-y-41937518.html\u0026rdquo; [4] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/gran-oport-recoleta-1-amb-47-m-sup2--piso-alto-en-41324130.html\u0026rdquo;\n[5] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/venta-1amb-al-frente-m-sup2--47-balcon-vista-a-los-41324195.html\u0026rdquo;\n[6] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/venta-gran-oport-posadas-1323-1-amb-m-sup2--47-balcon-41163069.html\u0026rdquo; [7] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/departamento-tipo-lofts-m-sup2--47-amoblados-y-41242674.html\u0026rdquo;\n[8] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/posadas-1323-edificio-alquiler-temporal-departamentos-41683158.html\u0026rdquo; [9] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/loft-al-frente.-amueblado-y-decoracion-de-diseno-42984563.html\u0026rdquo;\n[10] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/posadas-1323-alquiler-depto-amobl.-y-equip-lofts-41775461.html\u0026rdquo;\n[11] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/venta-y-alquiler-temporal-apartments-amoblados-41242680.html\u0026rdquo;\n[12] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/venta-loft-m-sup2--47-balcon-al-frente.-o-en-alquiler-40116759.html\u0026rdquo; [13] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/posadas-1323-lofts-m-sup2--47-amobl-y-equip-confort-41768916.html\u0026rdquo;\n[14] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/posadas-1323-apart-hotel-departamento-41682584.html\u0026rdquo;\nPero en todo caso, esto efectivamente detecta gran parte de los duplicados y puede asistir a que un ser humano genere la identificación de duplicados.\nEsto es un ejemplo de aprendizaje no-supervisado, pero si este analisis se llevara a fondo obtendríamos un set de datos de duplicaciones etiquetadas, lo cual podría ser la base para el analisis con procedimientos de analisis supervisados, y así encontrar patrones de que causa la duplicaciones; lo cual mejoraría el proceso de identificación de duplicados y podría usarse también para optimizar los criterios subjetivos que utilicé mas arriba.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":-62135596800,"objectID":"f0d21bf764911f2d99530e07105d2f33","permalink":"/post/dedupl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/dedupl/","section":"post","summary":"De-duplicación de Avisos Cualquiera que haya hecho un curso introductorio o leído un libro de Ciencia de Datos sabe que se dice mucho que la limpieza de un dataset es, por lo menos el 80% del trabajo y luego el modelado. Pero eso no quiere decir que no hagan falta técnicas propias del modelado para limpiar un dataset.\nRead more \n","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"ANALISIS DE LA OFERTA INMOBILIARIA EN MAR DEL PLATA Entender el mercado inmobiliario, nos permite detectar las necesidades y deseos de la porción de la población que compran, o que invierten en la generación de servicios de vivienda (ya sea para consumo propio o para terceros). Los datos se extrajeron de scrappear ZonaProp de manera exhaustiva, los datos estan geolocalizados por lo cual se pueden hacer analisis espaciales.\nRead more \nEl mercado inmobiliario, está compuesto por un conjunto de bienes y servicios heterogéneos, no sólo en sus características, sino también en su localización.\nEstas características de cada uno de los inmuebles que se encuentran en el mercado constituyen, en su conjunto, las características de la oferta de inmuebles en un determinado período de tiempo. Habitualmente, lo que se observa del mercado inmobiliario es un stock de la oferta y se asume que el precio es el de equilibrio, en el sentido que los oferentes intentarían obtener el mayor precio que le posibilite la venta o el alquiler, en el menor tiempo posible. Sin embargo, el flujo de inmuebles que se han vendido no es un conjunto observable y tal vez sea el conjunto más importante por determinar. Este flujo que se ha vendido o alquilado representa a aquellos inmuebles que hoy no se encuentran en el stock disponible a la venta o en alquiler, pero que en algún momento lo estuvieron; por lo que es necesario conformar una base de datos con los inmuebles que están disponibles actualmente en stock, pero también aquellos que en un lapso han salido y aquellos nuevos que ingresan a conformarlo. Para ello se desarrolló una rutina a través de un software, que permite relevar los datos de compra y venta de inmuebles disponibles en la web, para analizar su evolución en el tiempo, para poder construir distintos indicadores que sirvan como herramientas para los desarrolladores inmobiliarios, a la hora de decidir las características y el emplazamiento de sus proyectos.\nEstudio de la base El total de registro de ventas con los que contamos en para el tercer trimestre de 2019 de nuestra búsqueda de datos en la web, una vez que se han eliminado los datos repetidos, es de aproximadamente 25675 casos, de los cuales, 15562 corresponden a departamentos y 5489 corresponden a casas. En el cuadro resumen de la base de ventas se tuvieron en cuenta algunas características como la cantidad de m2 ofertados, m2 promedio y valor en dólares promedio para Mar del Plata.\n Caracteristicas de la base de datos  Tipo Cantidad Precio_M2 Tamaño Precio    Departamento 15562 2066 70 138747  Casa 5489 811 434 245960  PH 1645 1207 101 99352  Terrenos 1451 743 700 316334  Local comercial 659 1751 208 197579  Oficina comercial 287 1569 98 125887  Garage 245 912 1932 803429  Fondo de Comercio 132 1221 968 1340016  Bodega-Galpón 99 611 875 336943  Edificio 51 1737 759 887350    De la tabla, se desprende que el 82.16% de oferta inmobiliaria de Mar del Plata esta compuesta por Departamento (60.74%) y Casa (21.42%) y que los departamentos tienen el mayor precio por metro cuadrado, por el menor tamaño de las ofertas. Asimismo, los datos del trimestre revelan la variación de las cantidades, los precios y tamaños de cada tipo de inmueble.\nEn este caso, podemos ver, por ejemplo, que el aumento del tamaño promedio de los inmuebles, causa que, a pesar de un aumento del precio, el precio por metro cuadrado tenga una ligera caida. Lo contrario ocurre con los PHs. Lo cual en el caso de los terrenos (una mejora metodologica en Octubre nos permite obtener mejores datos de Terrenos y Locales Comerciales), las variaciones se compensan y se mantiene estabe el precio por metro cuadrado La base de datos cuenta también con inmuebles que han reducido su precio en los últimos 3 meses.\n Bajaron de Precio  Tipo Cantidad Bajó Precio_M2 var Precio m2 Tamaño var Tamaño Precio var Precio    Departamento 207 8.63% 1891 -13.51% 54 -33.65% 100875 -35.72%  PH 10 9.80% 1173 12.05% 81 -50.80% 91780 -27.27%    En el cuadro podemos ver que el precio promedio que bajaron los departamentos es del 8.63% mientras que los PHs bajaron más, un promedio de 9.8%. Los inmuebles que bajaron de precio también tienen una serie de caracteristicas particulares. Los precios por metro cuadrado de los departamentos que bajaron son 13.5% más baratos que el precio promedio, es decir, que los que bajan son aquellos que de por sí, ya están por debajo del precio promedio. Lo mismo ocurre respecto al tamaño, los que bajan son 33% más pequeños que la muestra general\nLos PH’s tienen una caracteristica particular que tiene que ver con que a pesar de ser más pequeños, tienen un precio por metro cuadrado mayor que el general de PH en la muestra. Esto se debe a que la diferencia respecto al promedio en el tamaño es más chica que la diferencia en el precio del inmueble.\nPublicaciones por Inmobiliaria La base de datos cuenta también con las inmobiliarias, extraídas desde noviembre 2019. Lo cual permite evaluar mejor la oferta disponible. En principio, podemos ver que la gran mayoría de las inmobiliarias tienen pocas publicaciones, el 77% tiene menos de 22 publicaciones en el trimestre analizado. Mientras que si éxisten inmobiliarias que tienen muchas más publicaciones, el 1% de las inmobiliarias tienen entre 276 y 419 publicaciones activas en el trimestre.\nEstos datos también pueden ser utilizados para hacer un seguimiento del market-share de cada una de ellas y eventualmente hacer una valuación de los inmuebles que tienen a la venta.\nZonas de Densidad de oferta La infomación obtenida esta geolocalizada en un 97%, lo cual nos permite ubicar en un mapa los inmuebles a la venta, y poder visualizar la densidad de la oferta de inmuebles. La mayor densida, de acuerdo a los datos, se encuentra en los barrios de La Perla, donde una hay una desproporcionada densidad de inmuebles. En el norte, cerca de Parque Peña, hay un foco de oferta; al sur, este foco está en Los Acantilados Por lo general, estos focos de oferta se encuentran en la costa (salvo Los Acantilados). Aquellos que se encuentran fuera, se caracterizan por ser los barrios cerrados Rumencó y Arenas del Sur\n De esta información se puede extraer las zonas y obtener descripciones de cada una de ellas.\nEsto nos dice que si bien la zona 6 ocupa el 10% de los m² ofrecidos, constituye el 26% de las publicaciones. Y, conjuntamente con la zona 5, son el 25% de los m² ofrecidos, pero superan el 52% del total de publicaciones.\nLas variables que puede ser descriptas en este sentido son, para cada zona:\n Tipo de Inmueble Tamaño de los inmuebles Precio por metro cuadrado Precio  Tipo Debajo se puede ver que en la zona de menor densidad de oferta (zona 0), el mercado consiste predominantemente en terrenos y casas. Y a medida que nos vamos acercando a la zona de mayor concentración de oferta (zona 6) ocurren varias cosas. En primer lugar los terrenos dejan de estar disponibles; luego, se incorporan casas, los departamentos van ocupando la mayor parte de la oferta y en el centro comercial, donde mayor oferta hay, se incorporan los locales comerciales y garages, desplazando a los PH’s.\nEste tipo de distribución es entendible desde el punto de vista que, las unidades de menor tamaño permiten que haya más oferta por unidad de terreno.\nTamaños Los tamaños de los inmuebles juegan un rol fundamental en la concentración geográfica de la oferta. Porque inmuebles de menor tamaño permiten una mayor cantidad de inmuebles por unidad de terreno.\nComo se vió en la sección de tipos de inmuebles se corrobora que, a menor densidad de oferta, mayor son los tamaños de los inmuebles a la venta. Por ejemplo, en la zona 0, donde predominan terrenos y casas, el tamaño promedio es de 1163m², lo cual se reduce exponencialmente a medida que se acerca a la zona 0.\nPrecios Los precios de los inmuebles en cada zona son más homogeneos que las variables previamente analizadas. Aunque cierta tendencia sigue existiendo. En la zona 6, donde mayor inmuebles hay, es donde se encuentran los menores precios.\nEsta información, sin embargo, no refleja la variabilidad de los precios. Ya que si bien los precios en la zona 6 son más baratos, en promedio, la variabilidad de los precios en esa zona es 50% mayor a la variabilidad de los precios en la zona 0. Esto quiere decir que si bien hay más con precios más bajos, también los hay con precios mucho mayores. Es decir, en las zonas donde predominan los departamentos los precios son mucho menos consistentes que los precios donde dominan los terrenos y las casas.\nPrecio por metro cuadrado Si a medida que hay mayor concentración de oferta, las unidades son de mayor tamaño y los precios promedios son más homogéneos transversalmente a las zonas se concluye que los precios por metro cuadrado serán mayores allí donde haya mayor concentración de oferta\nDeterminación de zonas de precios\u0026hellip; De la misma manera que se determinaron las zonas de mayor y menor concentración de oferta puede determinarse las zonas de mayor o menor precios por metro cuadrado.\n Así también como de cualquier variable númerica disponible. Y esta información se puede tener filtrada por tipo de inmueble, inmobiliaria, barrio, mes, etc según sea necesario. Alguno de los mapas alternativos pueden visualizarse en: mapa interactivo\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":-62135596800,"objectID":"fab9bc56564ce5367aaea9affa80c930","permalink":"/post/mardel_realstate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/mardel_realstate/","section":"post","summary":"ANALISIS DE LA OFERTA INMOBILIARIA EN MAR DEL PLATA Entender el mercado inmobiliario, nos permite detectar las necesidades y deseos de la porción de la población que compran, o que invierten en la generación de servicios de vivienda (ya sea para consumo propio o para terceros). Los datos se extrajeron de scrappear ZonaProp de manera exhaustiva, los datos estan geolocalizados por lo cual se pueden hacer analisis espaciales.\nRead more \n","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"Twitter y las Paso Esta fue mi primera aproximacion a minar datos de redes sociales, amigarme con las APIs, y algún analysis rudimentario. Siguiendo metodologias propuestas por otros.\nEn principio vamos a comenzar con lo primero ¿como hice para minar los datos de twitter? Bueno para eso use tweepy (http://www.tweepy.org/) Asique la primera parte va a estar en Python.\nRead more \nImportando lo importante\nimport tweepy from tweepy.streaming import StreamListener from tweepy import Stream import time from slistener import SListener import os import matplotlib.pyplot as plt import json import requests  por slistener es un script cortesia de https://github.com/alexhanna/hse-twitter/blob/master/bin/slistener.py que permite crear un objeto que va a ser el \u0026ldquo;listener\u0026rdquo; o \u0026ldquo;escuchante\u0026rdquo; de twitter. Para tomar los datos en tiempo real y poder ir guardandolos.\nInstanciando lo instanciable y setiando los paths donde van las cosas\nauth = tweepy.OAuthHandler('zarazatoken', 'zarazatoken) auth.set_access_token('zarazatoken', 'zarazatoken') api = tweepy.API(auth) datapath = os.path.join(os.getcwd(), 'data') datafiles = os.listdir(datapath) Y vamos a poder el escuchante a escuchar twitter keywords_to_track = ['EleccionesPASO2019', 'FrenteDeTodos','Frente Todos', 'Juntos por el Cambio','Juntos Cambio','Elecciones','PASO', 'YoTeVotoAlberto','NoVuelvenNuncaMas', 'ArgentinaVota','Macri','YoLoVoto','Fernandez','Kirchner'] stream.filter(track = keywords_to_track)  Los keywords a trackear se eligieron tomando los trending topic en argentina referidos a las elecciones y algunos elegidos por mi, a mano\nstream.filter() lo que se encarga de hacer es ir tomando la muestra en tiempo real de datos de twitter que se ajusten al filtro. Mientras corra (es decir, mientras no se interrumpa) va a ir juntando los datos. Esto lo empece a correr el domingo de las paso a las 7am y lo frené el mismo día a las 17hs.\nLa siguiente parte me fue bastante mas dificil de lo que habia anticipado, porque estas muestras se guardan en formato \u0026ldquo;.json\u0026rdquo; lo cual tenia que convertir a \u0026ldquo;.csv\u0026rdquo; para poder trabajar mejor\nimport pandas as pd import numpy as np tweets = [] with open(os.path.join(datapath,datafiles[1]), 'r',encoding='utf-8', errors='ignore') as t: tw_json = t.read().split('\\n') for tw in tw_json: #print(tw) #print('\\t\\t') try: tweet_obj = json.loads(tw) except: pass if 'extended_tweet'in tweet_obj: tweet_obj['extended_tweet-text'] = tweet_obj['extended_tweet']['full_text'] if tw != '': tweets.append(tw) pd.DataFrame(tweets)  ¡Ahora si! Ya tenemos bonito el data_frame en pandas para guardarlo y seguir desde allí\nLa siguiente tarea seria el topic extraction, pero la realidad es que cuando lo hice no llegue a ningun lado, porque, obviamente y como es de esperar, estaba todo referido a as elecciones. Lo que si termine haciendo fue filtrar el dataset que me quedo por las keywords que nombrar a los dos principales candidatos\n{'YoVotoMM','juntosporelcambio', 'votomm','NoVuelvenNuncaMas', 'yolovoto','Macri'} ## Quedarón 6,320 registros {'FrenteDeTodos','futurocontodos','YoTeVotoAlberto','FernandezFernandez','CFK,'cristina kirchner''Alberto Fernandez'} ## Quedarón 5,554 registros  Ahora bien, ¿de quien se hablaba mas en twitter?\nPrimero al dataframe de cada topic se agrega la variable correspondiente, se generan las dummies y luego se saca el promedio por hora, lo que resulta en la proporcion de tuits de cada uno\nmacri['p'] = 'MM' frente_de_todos['p'] = 'FF' df1 = pd.concat([macri, frente_de_todos]) df2 = pd.get_dummies(df1.p) mean_mm = df2['MM'].resample('1 h').mean() mean_ff = df2['FF'].resample('1 h').mean()  Sacando el plot:\nSe ve que salvo a la mañana y bien entrada la tarde, se hablo mas de Macri.\nBueno, habiendo hecho la primera parte en Python, es hora de continuar con la parte de sentiment analysis de los tuits de las PASO. Esta vez, en R. Vamos a empezar por las bibliotecas que necesitamos:\nlibrary(tidyverse) library(tidytext) library(stopwords) library(syuzhet) library(stopwords)  Luego traemos los datos:\npaso\u0026lt;-read_csv('paso.csv') paso\u0026lt;-paso[colnames(paso)!=\u0026quot;X1\u0026quot;] paso_unique\u0026lt;-unique(paso$`extended_tweet-full_text`) paso_unique2\u0026lt;-as_tibble(paso_unique)  El unique() nos sirve para filtrar tuits duplicados. Que pueden ocurrir por que un usuario citó a otro.\nlength(paso$`extended_tweet-full_text`) #Quedan 44423 registros length(unique(paso$`extended_tweet-full_text`)) #Quedan 43996 registros  Vamos a tokenizar las palabras:\ntweet_token\u0026lt;-paso_unique2%\u0026gt;% unnest_tokens(word, txt) tweet_token\u0026lt;-tweet_token%\u0026gt;% count(word, sort = T)%\u0026gt;% filter(!word%in% stopwords('es'))%\u0026gt;% filter(!word%in% stopwords('en'))%\u0026gt;% filter(str_detect(word, \u0026quot;^[a-zA-z]|^#|^@\u0026quot;))%\u0026gt;% ungroup()%\u0026gt;% arrange(desc(n))%\u0026gt;% mutate(w = word, freq = n)%\u0026gt;% select(w, freq) ## Resultado w freq \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; 1 t.co 18839 2 https 18834 3 paso 16293 4 elecciones 8401 5 macri 7982 6 si 5853 7 eleccionespaso2019 5498 8 votar 4950 9 q 4318 10 hoy 3304  Esto no es muuy bueno, hay tokens que hay que sacar.\ntweet_token_2\u0026lt;-tweet_token%\u0026gt;%filter(w!='t.co')%\u0026gt;%filter(w!='https')%\u0026gt;% filter(w!='q')%\u0026gt;%filter(w!='to')%\u0026gt;%filter(w!='si')%\u0026gt;%filter(w!='and')%\u0026gt;% filter(w!='rt') w freq \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; 1 paso 16293 2 elecciones 8401 3 macri 7982 4 eleccionespaso2019 5498 5 votar 4950 6 hoy 3304 7 argentinavota 3043 8 eleccionesargentina 2878 9 voto 2588 10 trump 2409  Ahora si, mira que loco lo de Trump. Igual esto se ve mucho mejor con un gráfico, además, no filtre todavia los stopwords y no filtre por tuits en español, asique probablemente sean tuits colados de otro tema\ntweet_token_2 [ 1 : 25 , ] %\u0026gt;% mutate ( w = forcats :: fct_inorder ( w ) ) %\u0026gt;% ggplot ( aes ( x = w , y = freq ) ) + geom_segment ( aes ( x = w , xend = w , y = 0 , yend = freq ) , color= \u0026quot;grey\u0026quot; )+ geom_point(size = 3, color = \u0026quot;#009A44\u0026quot;)+ coord_flip()  Bueno, el termino \u0026ldquo;paso\u0026rdquo; es evidentemente el mas frecuente, lo cual es mas que esperable. Luego, nos quedaria ver como se sentia la gente respecto a esto. Para esto se uso la libreria syuzhet\nbase_emocion\u0026lt;-get_nrc_sentiment(unlist(paso_unique2)) base_emocion \u0026lt;- data.frame(t(base_emocion)) base_emocion \u0026lt;- data.frame ( rowMeans ( base_emocion ) ) names ( base_emocion ) [ 1 ] \u0026lt;- \u0026quot;Proporcion\u0026quot; base_emocion \u0026lt;- cbind ( 'Sentimiento' = rownames ( base_emocion ) , base_emocion ) base_emocion%\u0026gt;% ggplot()+geom_bar(aes(x = Sentimiento, y = Proporcion), stat = 'identity', fill = 'green', alpha = 0.8)+ theme(axis.text.x = element_text(angle = 45, hjust = 1))  Esta es la parte mas relevante que tome de Hernan, la diferencia que tome, fue que él tomo la suma de cada una de los casos de cada sentimiento, y yo la proporcion. Creo que eso puede reflejar de otra forma cual es la emocion predominante en cada caso:\nEn este caso general, se ve que entre \u0026ldquo;positivo\u0026rdquo; y \u0026ldquo;negativo\u0026rdquo; son los predominantes, seguidos por \u0026ldquo;confianza\u0026rdquo; y \u0026ldquo;enojo\u0026rdquo;.\nEsta misma metodologia se puede usar para los dos datasets separados para cada topic pre-seleccionado, los referidos al frente de todos y a juntos por el cambio\nTodos se quejan, pero de Cristina hablan todos. Igual hay que considerar que de este conteo, se filtraron los nombres y apellidos de los candidatos a la presidencia ya que es lo que se uso de filtro.\n¿Como se sienten? Un poco de esto confirma no solo los resultado de la eleccion sino tambien la lectura del voto \u0026ldquo;enojo\u0026rdquo;. Porque los sentimientos asociados a cambiemos tienen mayor participacion de enojo y sentimientos negativos. Mientras que los asociados al frente de todos tiene mucha mayor participacion los tuits positivos.\nUn bonus track de python nada mas (esbozo de network analysis) ¿A quien se le contestaba mas para cada grupo?\nLas libreras de python son las mismas que el post anterior solo con la adicional de Networkx que permite hacer el analisis de redes.\nimport networkx as nx frente_de_todos = pd.read_csv('frente_de_todos.csv') cambiemos = pd.read_csv('cambiemos.csv')  Una vez leidos, filtramos los tuits que \u0026ldquo;son respuesta a\u0026rdquo;\ncambiemos_nx = cambiemos_nx[-cambiemos_nx['in_reply_to_screen_name'].isnull()] cambiemos_nx['in_reply_to_screen_name'] frente_de_todos_nx = frente_de_todos_nx[-frente_de_todos_nx['in_reply_to_screen_name'].isnull()] frente_de_todos_nx['in_reply_to_screen_name']  Se generan las redes de c/u; esto genera que cada usuario sea un nodo y que la relacion entre los usuarios se dá, en este caso particular, si responden a un tuit es decir: si yo te respondo un tuit, nosotros dos generamos una red que tiene mi nombre (mi usuario) como nodo de inicio y tu nodo (tu usuario) como nodo destino. Cada objeto, entonces, va a tener tantas salidas como respuestas haya hecho y tantas entradas como respuestas haya recibido:\nG_reply_c = nx.from_pandas_edgelist( cambiemos_nx, source = 'user-screen_name', target = 'in_reply_to_screen_name', create_using = nx.DiGraph()) G_reply_f = nx.from_pandas_edgelist( frente_de_todos_nx, source = 'user-screen_name', target = 'in_reply_to_screen_name', create_using = nx.DiGraph())  Y vemos la centralidad de cada tuitero (\u0026ldquo;in-degree-centrality\u0026rdquo;), que en realidad seria la respuesta a la pregunta \u0026ldquo;¿A quien se le esta contestando más?\u0026rdquo;\n#Para Frente de Todos - ¿a quien se le contesta cuando se habla de estse #tema? G_reply_f = nx.from_pandas_edgelist( frente_de_todos_nx, source = 'user-screen_name', target = 'in_reply_to_screen_name', create_using = nx.DiGraph()) bc = nx.in_degree_centrality(G_reply_f) indg = pd.DataFrame(list(bc.items()), columns =[\u0026quot;Name\u0026quot;,'Cent']) indg.sort_values('Cent', ascending=False) Name Cent 153 alferdez 0.018328 18 ierrejon 0.017182 137 LotusHerbals 0.017182 115 todonoticias 0.016037 113 fllorenteantoni 0.013746 746 AlbertoRavell 0.010309 159 FernandezAnibal 0.010309 147 LeonelFernandez 0.006873 236 mirthalegrand 0.006873  Y en el caso del Juntos Por el Cambio\nG_reply_c = nx.from_pandas_edgelist( cambiemos_nx, source = 'user-screen_name', target = 'in_reply_to_screen_name', create_using = nx.DiGraph()) bc = nx.in_degree_centrality(G_reply_c) indg = pd.DataFrame(list(bc.items()), columns =[\u0026quot;Name\u0026quot;,'Cent']) indg.sort_values('Cent', ascending=False) Name Cent 36 mauriciomacri 0.021858 148 fllorenteantoni 0.012610 131 gabicerru 0.011349 84 juansolervalls 0.008827 119 EsmeraldaMitre 0.007566 3 todonoticias 0.005885 99 CamiSolovitas 0.004624 17 Alfredo5019 0.004624 23 SantoroLeandro 0.004203  FIN! Gracias por leer hasta acá! Si tienen alguna recomendacion para tener en cuenta futuros analisis se los agradece!\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":-62135596800,"objectID":"4d99c02b37a76e409d42b675e8e0ee0a","permalink":"/post/paso2019/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/paso2019/","section":"post","summary":"Twitter y las Paso Esta fue mi primera aproximacion a minar datos de redes sociales, amigarme con las APIs, y algún analysis rudimentario. Siguiendo metodologias propuestas por otros.\nEn principio vamos a comenzar con lo primero ¿como hice para minar los datos de twitter? Bueno para eso use tweepy (http://www.tweepy.org/) Asique la primera parte va a estar en Python.\nRead more \n","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"Review Rating ¿Cómo generar un puntaje númerico en base a un texto?\nMucho de lo expuesto es en realidad distintas formas de pensar el problema y quedarse con la mejor solución.\nPaseo por Doc2Vec, Regresiones lineales, randomForests y redes convolutivas con GloVe\nRead more \nEn principio, lo que se va a ver es las calificaciones haciendo uso de Doc2Vec para convertir el texto en un vector numerico y poder, con esos vectores numericos como input, realizar la predicción de cual sería la calificación que hubiera tenido según el texto. Haciendo uso de algortimos de aprendizaje supervisado.\nPor otro lado, dado que los datos no son tantos, lo que perjudica la construcción del vector numerico a partir de los textos, se hará uso de un \u0026ldquo;word embedding\u0026rdquo; ya entrenado y posteriormente se verá como mejora el poder predictivo.\nLos datos y la limpieza Para empezar, veamos como se ven los datos:\n   review.rating review.text    5 Our experience at Rancho Valencia was absolutely perfect from beginning to end!!!! We felt special and very happy during our stayed. I would come back in a heart beat!!!  5 Amazing place. Everyone was extremely warm and welcoming. We've stayed at some top notch places and this is definitely in our top 2. Great for a romantic getaway or take the kids along as we did. Had a couple stuffed animals waiting for our girls upon arrival. Can't wait to go back.  2 We booked a 3 night stay at Rancho Valencia to play some tennis, since it is one of the highest rated tennis resorts in America. This place is really over the top from a luxury standpoint and overall experience. The villas are really perfect, the staff is great, attention to details (includes fresh squeezed orange juice each morning), restaurants, bar and room service amazing, and the tennis program was really impressive as well. We will want to come back here again.    En lugar de importar todos los paquetes juntos, vamos a ir importando a medida que vayamos necesitando.\nimport pandas as pd cc = pd.read_csv('./hotel-reviews/Datafiniti_Hotel_Reviews.csv')  Vamos a seleccionar las columnas necesarias y cambiarle el nombre para que sea mas facil luego seleccionarlas.\ncc = cc[['reviews.title','reviews.text','reviews.rating']] cc.columns = ['titulo','comentarios','calificacion']  El primer paso a la hora de trabajar con estos textos, es reducir a la maxima expresión la cardinalidad del vocabulario. ¿Que significa esto? Si tenemos una gran cantidad de usos de un verbo, como por ejemplo, \u0026ldquo;correr\u0026rdquo; en sus distintas conjugaciones, \u0026ldquo;corría\u0026rdquo;,\u0026ldquo;corriendo\u0026rdquo;,\u0026ldquo;corrian\u0026rdquo; y queremos armar un listado de frecuencias de palabras, esto daria como resultado que cada uno de esas palabras aparezca una sola vez; pero si logramos que la referencia a la acción concreta de \u0026ldquo;correr\u0026rdquo; sume independientemente de su conjugación, reduciríamos la cardinalidad de nuestro diccionario, eso es para lo que se usa la lematización.\n  Original Lemmatizacion    corrian correr  corriendo correr  corrio correr    Luego, hay sustantivos y otras palabras que tienen la misma raiz y dependiendo del sujeto, se puede reducir el tamaño del diccionario cortando de la raiz la palabra, lo que se conoce como stemmización\n  Original Stemmización    niña niñe  niño niñe  niñe niñe    El proceso de lematización y stemización, en su conjunto, se puede entender como normalizar el vocabulario y por lo tanto, una función que se encarge de hacer estas dos cosas, puede llamarse normalize(text):\ndef normalize(text): from nltk.stem import PorterStemmer from nltk.tokenize import word_tokenize import unidecode import spacy nlp = spacy.load('en_core_web_sm') porter = PorterStemmer() doc = nlp(text) lemmas = [unidecode.unidecode(tok.lemma_.lower()) for tok in doc if not tok.is_punct ] #En este caso, estoy eliminando palabras con menos de 3 letras y las negaciones, esto no es necesario estrictamente, y depende mucho del caso de aplicación, a veces funciona, a veces no. lexical_tokens = [t.lower() for t in lemmas if (len(t) \u0026gt; 3 or t ==\u0026quot;no\u0026quot;) and t.isalpha()] lexical_tokens = [porter.stem(t) for t in lexical_tokens] return lexical_tokens  Esta funcion tiene como input una frace y escupe objeto tipo list() asique lo que haré es aplicarla a cada texto y despues volverla a unir para que cada fila tenga un texto y no un array\nnorm_token = [] for i in range(len(cc.comentarios)): try: a = normalize(cc.comentarios[i]) except: a = '' norm_token.append(a) norm_text = [' '.join(x) for x in norm_token] cc['norm_text'] = norm_text  Doc2Vec: Generando el embedding numérico ¿Porque no tratar directo con los tokens? Por la sencilla razón hay un paquete que permite aplicar Doc2Vec, asociando un texto a una clase, gensim nos va a venir bien para esto:\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument #Primera la separacion entre test y train train, test = train_test_split(cc, test_size=0.2, random_state=42) train_tagged = train.apply( lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1) test_tagged = test.apply( lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1)  Una vez que tenemos los elementos de train y test, hay que entrenar el Doc2Vec, para, así pasar el texto a un vector númerico que pueda ser el input del algoritmo de clasificación\nfor epoch in range(30): model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1) model_dbow.alpha -= 0.002 model_dbow.min_alpha = model_dbow.alpha  Con el modelo Doc2Vec entrenado, podemos darle pasar los textos y obtener el vector numerico deseado. Ahora bien, para facilitar la implementación del modelo después, generemos una función con dos ouputs, el vector numerico por un lado, y el rating asociado a ese vector numérico\ndef vec_for_learning(model, tagged_docs): sents = tagged_docs.values targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents]) return targets, regressors  Ahora que tenemos la función que nos genera el vector númerico terminamos con el proceso de preparación de los datos. Es curioso notar que siempre se dice que el 80% del trabajo consiste en la preparación y limpieza de datos, y el 20% el modelado. Hasta ahora se puede ver que no es una distinción tan discreta, sino que es continua. ¿A qué me refiero? Bueno, para preparar los datos hizo falta algoritmos de embedding (Doc2Vec). Y no es poco común que ocurran estas cosas.\nAhora bien, volvamos a lo nuestro, es hora de correr los algoritmos de regresión:\nRegresión from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LinearRegression y_train, X_train = vec_for_learning(model_dbow, train_tagged) y_test, X_test = vec_for_learning(model_dbow, test_tagged) lr = LinearRegression() lr.fit(X_train, y_train) y_pred = lr.predict(X_test) print(np.sqrt(mean_squared_error(y_test, y_pred))) #1.15  Este resultado no me convence demasiado si consideramos al puntaje como una regresión. El error cuadratico medio es del 1.15pts, para los que no recuerdan, el error cuadratico medio toma la diferencia entre el valor predecido y el valor real, lo eleva al cuadrado y de ello toma el promedio. Les dejo un video  de mi canal de Youtube con la visualización de lo que significa\nClasificación Otra forma de entender el problema es como uno de clasificación. ¿Pero como pasamos de un target continuo a uno discreto? Podríamos pensar que los puntajes de 4 ó 5 son \u0026ldquo;buenos\u0026rdquo;, y asignarles un 1, y los de menos de 4 son \u0026ldquo;malos\u0026rdquo;, y asignarles un 0, y nos quedamos con un problema de clasificación binaria.\nAdemás, con estas conceptualización, tenes que volver a correr el embedding porque los \u0026ldquo;tags\u0026rdquo; no son ahora los puntajes del 1 al 5 sino que son {1,0}\ncl = [] for i in cc.calificacion: if i \u0026lt;4: cl.append(0) else: cl.append(1) cc.calificacion = cl train, test = train_test_split(cc, test_size=0.2, random_state=42) train_tagged = train.apply( lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1) test_tagged = test.apply( lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1) model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0) model_dbow.build_vocab([x for x in tqdm(train_tagged.values)]) for epoch in range(30): model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1) model_dbow.alpha -= 0.002 model_dbow.min_alpha = model_dbow.alpha  Ahora si, sacamos los vectores del modelo Doc2Vec y lo fiteamos a un randomForest clasificador:\nfrom sklearn.ensemble import RandomForestClassifier y_train, X_train = vec_for_learning(model_dbow, train_tagged) y_test, X_test = vec_for_learning(model_dbow, test_tagged) rfr = RandomForestClassifier(n_estimators = 500, random_state = 42) rfr.fit(X_train, y_train)  ¿Cómo evaluamos esta clasificación? Bueno, primero nos fijamos cuanto coincide la predicción respecto al valor real, para eso se usa la matriz de confusión\nfrom sklearn.metrics import confusion_matrix y_pred = rfr.predict(X_test) tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0,1]).ravel() np.mean(y_test == y_pred)  Nuestra precisión es del 72,4%, nada mal, solo un detalle. Si mandamos un modelo que siempre diga \u0026ldquo;1\u0026rdquo;, tendremos una precisión equivalente a la proporción de \u0026ldquo;1\u0026rdquo; en el set. Que en la base completa es de 72,8%. Es decir, este modelo no mejor que decir que todas son igual a \u0026ldquo;1\u0026rdquo;.\nPolaridad como métrica Otra alternativa puede ser extraer la polaridad del texto, herramienta muy útil en los procesos de sentiment analysis. Y podríamos pensar que, cuan más positivo sea la polaridad, estará asociado a un mejor puntaje\nfrom textblob import TextBlob pls = [] sbj = [] for i in range(len(cc.comentarios)): try: senti = TextBlob(cc.comentarios[i]) polarity = senti.sentiment pls.append(polarity[0]) sbj.append(polarity[1]) except: pls.append(0) sbj.append(0) polscore = [int(x \u0026gt; 0) for x in pls] # Acá 0 es un valor arbitrario de corte, #un ejericio podría incluir la optimización de este valor como un hiperparametro. #La polaridad genera un indice de -1, 1. Siendo -1 cuan más negativo es, y 1 cuan más #positivo es, y polscore dice que aquellos que tienen valoracion positiva sean 1 y los demás 0 np.mean(polscore == np.array(cc.calificacion))  Sirve esto? Y, esto esta dando un resultado de 78.55%, es, a mi sorpresa, una mejora respecto al punto anterior. Aunque todavía no es satisfactorio.\nEvidentemente el score de polarización agrega información.\nRedes neuronales y GloVe Global Vectors ó GloVe  es una tecnica que, a diferencia de Doc2Vec, que es un algortimo supervisado, es no-supervisado y obtiene embedings númericos de palabras según estadisticas de co-ocurrencia. De esta manera puede encontrar analogías tales como \u0026ldquo;los que varón es a mujer, rey es a reina\u0026rdquo;.\nMás allá las controversias , es una herramienta bastante útil para muchos casos\nEl objetivo de esta sección es ver como, haciendo uso de un modelo de lenguage pre-existente, se puede usar el proceso de transfer-learning para incorporar nuestros textos y sus calificaciones y adaptarlo a nuestras necesidades.\nEsto es interesante e importante porque hay muchos modelos de lenguage que han hecho uso de datasets enormes en hardwares mucho más potentes que los que podría pagar, que se puede aprovechar\nimport os import sys import numpy as np from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.utils import to_categorical from keras.layers import Dense, Input, GlobalMaxPooling1D from keras.layers import Conv1D, MaxPooling1D, Embedding from keras.models import Model from keras.initializers import Constant BASE_DIR = os.getcwd() GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B') #Este es el modelo pre-entrenado TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup') MAX_SEQUENCE_LENGTH = 1000 #entrenar sobre oraciones de hasta estas palabras MAX_NUM_WORDS = 20000 #tamaño máximo del vocabulario EMBEDDING_DIM = 300 #dimensión del vector númerico resultante VALIDATION_SPLIT = 0.2 #Volvemos a cargar la información import pandas as pd cc = pd.read_csv('./hotel-reviews/Datafiniti_Hotel_Reviews.csv') cc = cc[['reviews.title','reviews.text','reviews.rating']] cc.columns = ['titulo','comentarios','calificacion'] cl = [] for i in cc.calificacion: if i \u0026lt;4: cl.append(0) else: cl.append(1) cc.calificacion=cl #Preparación de los datos TEXT_DATA_DIR = cc.comentarios embeddings_index = {} with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f: for line in f: word, coefs = line.split(maxsplit=1) coefs = np.fromstring(coefs, 'f', sep=' ') embeddings_index[word] = coefs texts = [x for x in cc.comentarios] # listado de muestras de texto labels_index = {1:1, 2:2, 3:3, 4:4, 5:5} # diccionario mapeando los revies a los target labels = [int(x) for x in cc.calificacion] # target # Tokenizar las palabras texts = np.array(texts) tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) tokenizer.fit_on_texts(texts) sequences = tokenizer.texts_to_sequences(texts) word_index = tokenizer.word_index #Con las palabras tokenizadas, se hace el padding, para que todos tengan la misma longitud, para eso se agrega 0 hasta que se llene data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) labels = to_categorical(np.asarray(labels)) indices = np.arange(data.shape[0]) np.random.shuffle(indices) data = data[indices] labels = labels[indices] num_validation_samples = int(VALIDATION_SPLIT * data.shape[0]) x_train = data[:-num_validation_samples] y_train = labels[:-num_validation_samples] x_val = data[-num_validation_samples:] y_val = labels[-num_validation_samples:] num_words = min(MAX_NUM_WORDS, len(word_index) + 1) embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) for word, i in word_index.items(): if i \u0026gt;= MAX_NUM_WORDS: continue embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[i] = embedding_vector #Este es el proceso que genera los embeddings embedding_layer = Embedding(num_words, EMBEDDING_DIM, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_SEQUENCE_LENGTH, trainable=False)  Ok, hasta ahi la preparación de los datos, es hora de entrenar una red neuronal convolutiva con los embeddings realizados para obtener el modelo que clasifique las reiews:\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') embedded_sequences = embedding_layer(sequence_input) x = Conv1D(128, 6, activation='relu')(embedded_sequences) x = MaxPooling1D(5)(x) x = Conv1D(128, 6, activation='relu')(x) x = MaxPooling1D(6)(x) x = Conv1D(128, 6, activation='relu')(x) x = GlobalMaxPooling1D()(x) x = Dense(128, activation='relu')(x) preds = Dense(2, activation='softmax')(x) model = Model(sequence_input, preds) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])  Hay mucho código repetido en este caso, pero eso es para simplicidad de exposición, lo relevante a enteder es que así se define la capa convolutiva, que esla que se repite:\nx = Conv1D(128, 6, activation='relu')(embedded_sequences) x = MaxPooling1D(5)(x)  Y así la ultima capa, que como terminamos con las \u0026ldquo;buenas\u0026rdquo; y \u0026ldquo;malas\u0026rdquo; reviews, tiene una función de activación binaria en el output\nx = Dense(128, activation='relu')(x) preds = Dense(2, activation='softmax')(x)  Una vez preparados los datos, y una vez definidas las capas de la red neuronal se entrena:\nmodel.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val))  Este fit, obtiene un accuracy que supera el 90%. Un gran paso adelante.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":-62135596800,"objectID":"27713a1cc1ef4d73c5a0fce0e8b8c1b9","permalink":"/post/review_rating/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/review_rating/","section":"post","summary":"Review Rating ¿Cómo generar un puntaje númerico en base a un texto?\nMucho de lo expuesto es en realidad distintas formas de pensar el problema y quedarse con la mejor solución.\nPaseo por Doc2Vec, Regresiones lineales, randomForests y redes convolutivas con GloVe\nRead more \n","tags":null,"title":"","type":"post"}]