[{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1567641600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/nelson-bighetti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nelson-bighetti/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Nelson Bighetti","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1461110400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1555459200,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\\\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$  renders as\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\n1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ```  renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2]  An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```  renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d  An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ```  renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() }  An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ```  renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*]  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a shortcode for asides, also referred to as notices, hints, or alerts. By wrapping a paragraph in {{% alert note %}} ... {{% /alert %}}, it will render as an aside.\n{{% alert note %}} A Markdown aside is useful for displaying notices, hints, or definitions to your readers. {{% /alert %}}  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.   Spoilers Add a spoiler to a page to reveal text, such as an answer to a question, after a button is clicked.\n{{\u0026lt; spoiler text=\u0026quot;Click to view the spoiler\u0026quot; \u0026gt;}} You found me! {{\u0026lt; /spoiler \u0026gt;}}  renders as\n Click to view the spoiler  You found me!    Icons Academic enables you to use a wide range of icons from Font Awesome and Academicons in addition to emojis.\nHere are some examples using the icon shortcode to render icons:\n{{\u0026lt; icon name=\u0026quot;terminal\u0026quot; pack=\u0026quot;fas\u0026quot; \u0026gt;}} Terminal {{\u0026lt; icon name=\u0026quot;python\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} Python {{\u0026lt; icon name=\u0026quot;r-project\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} R  renders as\n  Terminal\n Python\n R\nDid you find this page helpful? Consider sharing it 🙌 ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"93c49b4147975799280493e66c0399cb","permalink":"/post/prev/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/prev/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Nelson Bighetti"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["Nelson Bighetti"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab  Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata ( front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"7965b5e760ebabd8f66c4a38ca3859d3","permalink":"/post/prev/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/prev/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Nelson Bighetti","吳恩達"],"categories":["Demo","教程"],"content":"Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\n Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n 👉 Get Started 📚 View the documentation 💬 Ask a question on the forum 👥 Chat with the community 🐦 Twitter: @source_themes @GeorgeCushen #MadeWithAcademic 💡 Request a feature or report a bug ⬆️ Updating? View the Update Guide and Release Notes ❤️ Support development of Academic:  ☕️ Donate a coffee 💵 Become a backer on Patreon 🖼️ Decorate your laptop or journal with an Academic sticker 👕 Wear the T-shirt 👩‍💻 Contribute      Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.   Key features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\n Choose a stunning theme and font for your site. Themes are fully customizable.\nEcosystem   Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site  Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n  one-click install using your web browser (recommended)  install on your computer using Git with the Command Prompt/Terminal app  install on your computer by downloading the ZIP files  install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating  View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"68eb90119218558e98e7f0341a7256b5","permalink":"/post/prev/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/prev/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["Academic","开源"],"title":"Academic: the website builder for Hugo","type":"post"},{"authors":["Nelson Bighetti","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":null,"categories":["R"],"content":" R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932  Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA )  Figure 1: A fancy pie chart.   ","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"b74d7186ef8a9f9f98d01652d5751dd2","permalink":"/post/prev/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/post/prev/2015-07-23-r-rmarkdown/","section":"post","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":["Nelson Bighetti","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"Clusterize design patterns ¿How can I find patterns in design?\nWith so many adds around, one would think that that process is quite standardized but it remains, most times, a manual labor. This is, I believe mostly, because of how competitive that market is. To stand out, graphic designers have still a lot of work\nBut that doesn\u0026rsquo;t mean that we can\u0026rsquo;t find design patterns, after all, after a few thousend design even the best designer tend to have trends.\nRead more \nInput data\nThis problem had, originally, many houndred of designs, in this example I will use just a few made ad hoc for this purposes because in the end, clustering algorithms doesn\u0026rsquo;t need that many data to be effective.\nGiven these are manual examples, it\u0026rsquo;s all about sizes and shapes. I left behind fonts, content of the images and other. What I look for is pattern in the layout of the elements in the banner\nOnce extracted, data came in this form:\nWhere:\n  y : Distance from the top \n  x : Distance from the left \n  w : Width \n  h : Height \n  If we have 50 exameples, with 3 elements each, and 4 variables per element the shape of the input file is 50 × 3 × 4. Algorithms like I used like 2D data better, so I spread them to 50 × 12 for which:\nFirst, iterate file by file and transform each from 1 × 3 × 4 to 1 × 12. In R code:\nlibrary(tidyverse) cols_used = c('element_top', 'element_left', 'element_width', 'element_height') spread_file\u0026lt;-function(data, cols_used){ cols_used_a = c('element_name',cols_used) y=data[cols_used] h = data[cols_used_a] z=c(1,1,1,1) for(i in 1:nrow(y)) { z = cbind(z,y[i,]) } z = z[1,-1] newcols \u0026lt;- c() for (i in h['element_name']){ newcols\u0026lt;-cbind(newcols,paste(i,cols_used[1], sep = '.')) newcols\u0026lt;-cbind(newcols,paste(i,cols_used[2], sep = '.')) newcols\u0026lt;-cbind(newcols,paste(i,cols_used[3], sep = '.')) newcols\u0026lt;-cbind(newcols,paste(i,cols_used[4], sep = '.')) } newcols2\u0026lt;-c() for(i in 1:nrow(newcols)) { for(j in 1:4){ newcols2\u0026lt;-c(newcols2,newcols[i,j]) } } colnames(z)\u0026lt;-newcols2 n\u0026lt;-as_vector(data['id']) z['id']\u0026lt;-n[1] z }  And then iterate to transform this\n$$\\begin{bmatrix} elem1 \u0026amp; y_1 \u0026amp; x_1 \u0026amp; w_1 \u0026amp; h_1 \\\\ elem2 \u0026amp; y_2 \u0026amp; x_2 \u0026amp; w_2 \u0026amp; h_2 \\\\ \\vdots \\\\ elemk \u0026amp; y_2 \u0026amp; x_k \u0026amp; w_k \u0026amp; h_k \\end{bmatrix}$$ in this:\n$$\\begin{bmatrix} id.1 \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\end{bmatrix}$$ This way, you can stack them into:\n$$\\begin{bmatrix} id.1 \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\\\ id.2 \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\\\ \\vdots \\\\ id.N \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\end{bmatrix}$$\n Dimentionality reduction + Clustering   This most direct form of clusterization for this porpuses is dbscan and run it on our transform base. This didn\u0026rsquo;t work as expected so first I used a technique to reduce dimensionality and then do the clustering.\nPCA and t-SNE are the most popular algorithms in dimensionality reduction but UMAP is the new kid in the block (well it has almost 2 years now) with some fancy math behind it, it preserves global and local structures. and works faster using graphs. One thing to keep in mind when using it is that at one point uses a random procedure which makes it that each time you run it the mapping into 2D will look slightly different, but every point is similary close to others in each iteration. To prevent this, set.seed() is the way to go.\nAnd finally:\nlibrary(umap) library(dbscan) umap_data\u0026lt;- umap(data) cl \u0026lt;-hdbscan(x = umap_data, minPts = 3)  This worked better than with the full dimentions, but not quite as needed. Its time to\u0026hellip;\nTransform and normalice\nMost common option \n  Standarization (z-score): Represents the number of standard deviations up or down of resulting value. Useful for normally distributed variables \n  Normalization (min-max scaler): It allows to transform the data into values between 0 and 1. Useful when working with variables with different orders of magnitude \n  Can I use this transformations in this data?\n Not really, what this variables describe are absolute positions in space and are quite linked to one-another.    What can I do? Instead of using absolute positions and dimentions, lets use its relative, what I\u0026rsquo;ll call \u0026ldquo;geometric normalization\u0026rdquo;  normalize_geometric\u0026lt;-function(df){ df['total_area']\u0026lt;-max(df['element_height'])*max(df['element_width']) df['rel_area']\u0026lt;-df['element_height']*df['element_width']/df['total_area'] df['orientation']\u0026lt;-df['element_height']/df['element_width'] df['element_top_relative']\u0026lt;-df['element_top']/max(df['element_height']) df['element_left_relative']\u0026lt;-df['element_left']/max(df['element_width']) df }    x\u0026rsquo; is now the proportion of x in respecto the total width of the canvas  My new variable is x\u0026rsquo;, red line divided the blue one \n y\u0026rsquo; is now the proportion of x in respecto the total height of the canvas  My new variable is y\u0026rsquo;, red line divided the blue one \n areaRelativa is the proportion of the canvas the element occupies  My new variable areaRelativa is: the are of the small rectangle divided divided the area of the big one \n disposition is to know if the elmenet in horizontal position, vertical or if it is a square  My new variable disposicion es: heigth/width \nResults To begin to analize the results, every \u0026ldquo;spread\u0026rdquo; has to have a \u0026ldquo;gather\u0026rdquo;\ngather_file\u0026lt;-function(gdf){ x\u0026lt;-strsplit(colnames(gdf), '\\\\.') element_name=unique(unlist(map(x, 1))) original_cols=unique(unlist(map(x, 2))) gdf1\u0026lt;-data.frame(element_name) gdf1[original_cols[1]]\u0026lt;-0 gdf1[original_cols[2]]\u0026lt;-0 gdf1[original_cols[3]]\u0026lt;-0 gdf1[original_cols[4]]\u0026lt;-0 rel_area\u0026lt;-c() orientation\u0026lt;-c() element_top_relative\u0026lt;-c() element_left_relative\u0026lt;-c() for(i in seq(from=1, to=length(gdf), by=4)){ # stuff, such as rel_area=c(rel_area,gdf[i]) orientation=c(orientation,gdf[i+1]) element_top_relative = c(element_top_relative,gdf[i+2]) element_left_relative = c(element_left_relative,gdf[i+3]) } gdf1['rel_area']=as_vector(unlist(rel_area)) gdf1['orientation']=as_vector(unlist(orientation)) gdf1['element_top_relative']=as_vector(unlist(element_top_relative)) gdf1['element_left_relative']=as_vector(unlist(element_left_relative)) gdf1}  Let\u0026rsquo;s first see how the clustering works with and without geometric normalization\nAnd finally, examples of each group:\nFirst cluster:\n000003.png\nSecond cluster:\nThird:\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e6926e1132084f9b399c66235ab67294","permalink":"/post/clusterizar-disenos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/clusterizar-disenos/","section":"post","summary":"Clusterize design patterns ¿How can I find patterns in design?\nWith so many adds around, one would think that that process is quite standardized but it remains, most times, a manual labor. This is, I believe mostly, because of how competitive that market is. To stand out, graphic designers have still a lot of work\nBut that doesn\u0026rsquo;t mean that we can\u0026rsquo;t find design patterns, after all, after a few thousend design even the best designer tend to have trends.\nRead more \n","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"De-duplicación de Avisos Cualquiera que haya hecho un curso introductorio o leído un libro de Ciencia de Datos sabe que se dice mucho que la limpieza de un dataset es, por lo menos el 80% del trabajo y luego el modelado. Pero eso no quiere decir que no hagan falta técnicas propias del modelado para limpiar un dataset.\nRead more \nCon un scrap hecho a ZonaProp, de las unidades en alquiler en Recoleta en febrero 2020, luego de una exahustiva limpieza de unificación de la moneda, de corrección de datos implícitos (NA\u0026rsquo;s que implicaban un dato, por ejemplo, cocheras-si es NA, cocheras=0).\nEl problema que quedó fue que el resultado es una base de datos de avisos, pero, ¿que pasa si yo quiero que mi base de datos sea de inmuebles? El problema que me encuentro es que hay avisos duplicados, ya sea porque vuelven a estar publicados o porque un mismo inmueble es publicado por más de una inmobiliaria.\n¿Cómo se pueden detectar sistematicamente esas duplicaciones? Debajo hay un método posible\nLos Datos library(tidyverse) alqs\u0026lt;-read_csv(\u0026quot;alq_feb20_recoleta.csv\u0026quot;)  Bien, este es el dataset. Una primera idea sería: si dos publicaciones se parecen lo suficiente, sospechamos que son la misma.\nLas columnas son\n  Columna Descrpción    WEB La url de la publicación  Provincia La provincia de la publicación  Tipo_Op Si corresponde a venta o alquiler  Tipo Si es casa, comercio,oficina, PH, departamento, etc  Zona El barrio, para CABA, el municipio para las provincias  Dirección La dirección de la publicación  Latitud y Longittud Georreferencia  Inmobiliaria Quien está publicando  Tiempo Cuanto tiempo lleva publicado en días  Cochera Si tiene cochera, boolean  Expensas Cuanto se paga de expensas  Prices El precio de venta/alquiler  Antigüedad Cuantos años tiene de construido  Metros Tamaño en metros cuadrados  Ambientes Cantidad de Ambientes  Descripción El texto de la descripción  Baños Cantidad de baños    Ahora bien, cualquier procedimiento de detección de duplicados requiere necesariamente cierta flexibilidad, sino buscamos aquellos identicos y listo; pero la realidad es que la duplicación en general va a tener un motivo en concreto, sea por corrección de algún dato en particular, sea por ponerlo en otra inmobiliaria, sea por cambio de la descripción para que sea más atractiva. Pero por otro lado, revisar todos los pares posible nos va a llevar a que, si N es la cantidad de publicaciones en el dataset, los chequeos sean N2. Manualmente, esto, es inviable.\nEntonces, lo que se puede hacer es:\n  Ver la tasa de variación de las variables numéricas.\n  Verificar la distancia (aprovechando que estan georreferenciados)\n  Ver cuanto se parecen las descripciones\n  Tasa de variacion de las variables numericas Una opción sería considerar solo las variables que son iguales, pero eso descarta la posibilidad que haya una corrección en los datos, ni hablar si alguno de los datos se considera missing (N**A).\nUna cuestión antes de arrancar con esta sección, se estará generando una matriz simétrica para cada una de estas variables númericas. En la cual xi**j va a ser la tasa de variación entre las observaciones; y por lo tanto la diagonal cuando i = j, xi**j = 0\nEmpecemos\npmx\u0026lt;-matrix(nrow=nrow(alqs1), ncol=nrow(alqs1)) for(i in 1:nrow(alqs)){ for(j in 1:nrow(alqs)){ h= abs(alqs3$Prices[i]-alqs3$Prices[j])/max(alqs3$Prices[i],alqs3$Prices[j], na.rm = T) pmx[i,j] = h } }  ¿Qué está mal con esta aproximación? Que es un loop anidado, por lo que el Big O, es cuadrático, la forma menos eficiente de llenar una matriz. Esto quiere decir, que el tiempo que tarda el terminar este loop depende cuadráticamente del tamaño de las filas.\nSe puede aprovechar las operaciones vectorizadas que tiene R-Base para quedarnos con una Big O lineal.\npmx\u0026lt;-matrix(nrow=nrow(alqs1), ncol=nrow(alqs1)) for(i in 1:nrow(alqs)){ pmx[i,] = abs(alqs3$Prices[i]-alqs3$Prices)/max(alqs3$Prices[i],alqs3$Prices, na.rm = T) }  Esto es mejor, pero R tiene su propio diálecto para tratar con loops y es la facilidad que tiene para trabajar vectorialmente y la familia de funciones de apply, y de paso ponemos en práctica la máxima de evitar los loops\nrel_dif = function(a,b){ abs(a-b)/max(a,b, na.rm = T) } pmx\u0026lt;-sapply(FUN = rel_dif, alqs1$Prices,alqs1$Prices)  Ahora que podemos obtener la matriz de diferencias con una sola linea de código, hagamoslo para todoas las variables númericas.\nmts.mx\u0026lt;-sapply(FUN = rel_dif, alqs1$Metros,alqs1$Metros) #Metros cuadrados antig.pmx\u0026lt;-sapply(FUN = rel_dif, alqs1$Antiguedad,alqs1$Antiguedad) #Antigüedad  Solo una cosa, lo que queremos es ver cuan cerca están, pero hasta ahora vimos la variación, por lo que la cercacia va a estar dada por:\npmx\u0026lt;-1-as.matrix(pmx) pmts\u0026lt;-1-as.matrix(pmts) pantg\u0026lt;-1-as.matrix(pantg)  Distancia geográfica Para calcular la distancia geográfica vamos a estar necesitando el paquete geosphere para lo cual se usa el criterio de la distancia entre dos puntos en una superficie esférica, llamada la distancia Haversine.\nlatlongs\u0026lt;-alqs1 %\u0026gt;% select(Latitud,Longitud) dm \u0026lt;- distm(latlongs,latlongs, fun=distHaversine)  La función llamada distm es realmente útil porque viene con una implementación vectorizada,lo cual soluciona de entrada el problema de los potenciales loops anidados que teniamos antes.\nDescripción Bueno, llegamos al punto álgido, y por esto me refiero a que el criterio para determinar que tan cerca están dos descripciones no es tan obvio como la distancia o la variación relativa de alguna variable, porque estamos tratando ahora con variables no numéricas; por lo tanto antes de realizar la comparación necesitamos convertir a ese texto en un vector numérico, para lo cual existen varias vías.\nLa utilizada en este caso fue T**F − IDF que significa \u0026ldquo;term frequency–inverse document frequency,\u0026quot;, el cual genera un vector numérico ponderado por la importancia de cada palabra en el texto (term frequency) y la frecuencia de la palabra en todos los textos (inverse document frequency)\nEs decir, la ponderación indica que no todas la palabras tienen el mismo peso para describir una descripción, sino que hay algunas que deben ponderarse más y otras que deben ponderarse menos. En el caso de una descripción de un inmueble, las palabras que son más comunes a todas las descripciones, como pueden ser \u0026ldquo;baño\u0026rdquo; ó \u0026ldquo;cocina\u0026rdquo;, son ponderadas menos que las palabras que son menos frecuentes a cada descripción, como \u0026ldquo;a tres cuadras de la estacion Primera Junta\u0026rdquo;.\nEste score esta definido por $$ TFIDF_{xy} = TF_{xy}*log\\frac{N}{df} $$ donde:\n  T**Fx**y es la frecuencia de la palabra x en la descripción y\n  N es el número total de descripciones\n  d**f es el número total de documentos que contienen la palabra x\n  Por suerte, python tienen una libreria para (casi)todo, por lo que esta vectorización require solo unas pocas lineas de código; desafortunadamente, el texto está \u0026ldquo;sucio\u0026rdquo; y debe ser limpiado antes de la vectorización.\n#Eliminar caracteres especiales y espacios innecesarios cleanFun \u0026lt;- function(htmlString) { #Saco los tags de html t=(gsub(\u0026quot;\\t\u0026quot;,\u0026quot;\u0026quot;,gsub(\u0026quot;\\n\u0026quot;,\u0026quot;\u0026quot;,gsub(\u0026quot;\u0026lt;.*?\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, htmlString)))) #Lo paso a minuscula t=tolower(t) #Le saco los caracteres no alfanumericos t=str_replace_all(t, \u0026quot;[[:punct:]]\u0026quot;, \u0026quot; \u0026quot;) t } #Tokenización y eliminación de stopwords prepTx \u0026lt;- function(tx){ t1=word_tokenizer(cleanFun(tx)) t2 = t1[!(t1%in%stopwords(kind=\u0026quot;es\u0026quot;))] t2 = unlist(t2)[nchar(unlist(t2))\u0026gt;2] t2 }  Ok, ahora que el text está limpio, es hora de generar la conversión a numeros\nlibrary(tcR) prep_fun \u0026lt;- cleanFun tok_fun \u0026lt;- word_tokenizer smp_size\u0026lt;-floor(0.75*length(descripciones)) set.seed(123) train_ind \u0026lt;- sample(seq_len(length(descripciones)), size = smp_size) train \u0026lt;- descripciones it_train \u0026lt;- itoken(train, preprocessor = prep_fun, tokenizer = tok_fun, progressbar = TRUE) vocab \u0026lt;- create_vocabulary(it_train) pruned_vocab = prune_vocabulary(vocab, term_count_min = 100, doc_proportion_max = 0.5, doc_proportion_min = 0.001) vectorizer \u0026lt;- vocab_vectorizer(pruned_vocab) dtm_train \u0026lt;- create_dtm(it_train, vectorizer) tfidf = TfIdf$new() dtm_transformed = tfidf$fit_transform(dtm_train)  Ahora dtm_transformed es nuestra variable que contiene los vectores numericos para cada descripción. Como son vectores, una forma sencilla de compararlos es usando la distancia coseno.\nd1_d2_tfidf_cos_sim = sim2(x = dtm_transformed, method = \u0026quot;cosine\u0026quot;, norm = \u0026quot;l2\u0026quot;)  Y ahora si, nuestra matriz de similitud d1_d2_tfidf_cos_sim nos permite comparar las descripciones.\nUnificación de las matrices de similitud Ahora que tenemos todas las matrices que comparan todos los registros con todos los demás, es hora de unificarlas.\nA\u0026lt;-pmx B\u0026lt;-pmts C\u0026lt;-pantg D\u0026lt;-d1_d2_tfidf_cos_sim E\u0026lt;-dm colnames(A)\u0026lt;-alqs3$id rownames(A)\u0026lt;-alqs3$id colnames(B)\u0026lt;-alqs3$id rownames(B)\u0026lt;-alqs3$id colnames(C)\u0026lt;-alqs3$id rownames(C)\u0026lt;-alqs3$id colnames(D)\u0026lt;-alqs3$id rownames(D)\u0026lt;-alqs3$id colnames(E)\u0026lt;-alqs3$id rownames(E)\u0026lt;-alqs3$id Aa\u0026lt;-as.data.frame(as.table(A)) %\u0026gt;% distinct(Var1,Var2, .keep_all = T) Bb\u0026lt;-as.data.frame(as.table(B)) %\u0026gt;% distinct(Var1,Var2, .keep_all = T) Cc\u0026lt;-as.data.frame(as.table(C)) %\u0026gt;% distinct(Var1,Var2, .keep_all = T) Dd\u0026lt;-as.data.frame(as.table(D)) %\u0026gt;% distinct(Var1,Var2, .keep_all = T) Ee\u0026lt;-as.data.frame(as.table(E)) %\u0026gt;% distinct(Var1,Var2, .keep_all = T) colnames(Aa)[3]='precios' colnames(Bb)[3]='metros' colnames(Cc)[3]='antiguedad' colnames(Dd)[3]='descripcion' colnames(Ee)[3]='dist_mts' AB\u0026lt;-right_join(Aa,Bb, by = c('Var1','Var2')) CD\u0026lt;-right_join(Cc,Dd, by = c('Var1','Var2')) ABCD\u0026lt;-right_join(AB,CD,by = c('Var1','Var2')) ABCDE\u0026lt;-right_join(ABCD,Ee,by = c('Var1','Var2')) ABCDE\u0026lt;-ABCDE %\u0026gt;% filter(Var1!=Var2) %\u0026gt;% as_tibble()  Ahora ABCDE es nuestra dataframe con todos los pares de i**d′s, y las columnas con cada criterio de similitud se agregan en la misma fila\nLo que sigue es un poco arbitrario y seguro hay mejores métodos para hacerlo. Pero nuestros candidatos a duplicados son aquellos que, decimos, cumplen los siguientes criterios (en simultaneo):\n  Estan a menos de 300mts entre sí.\n  Tienen una variación en Metros menor al 10%\n  Tienen una variación en Precio menor al 25%\n  Tienen una variación en la descripcion menor al 30%\n  Tienen una variación en antigüedad menor al 10%\n  En todos los casos se mantienen los resultados NA porque eso nos indica que podría haberse agregado el dato.\ncand_dupls\u0026lt;-ABCDE %\u0026gt;% filter(dist_mts\u0026lt;=300 | is.na(dist_mts)) %\u0026gt;% filter(metros\u0026gt;0.9| is.na(metros)) %\u0026gt;% filter(precios\u0026gt;0.75| is.na(precios)) %\u0026gt;% filter(descripcion\u0026gt;0.7| is.na(descripcion))%\u0026gt;% filter(antiguedad\u0026gt;0.90| is.na(antiguedad))  El resultado de esto es de 840 pares que son potencialmente duplicados. Lo cual es una reducción bastante drástica de los 17162 que llevaría revisar todos los registros de una base de 1716 registros.\nFinalmente, hay un paso más que puede hacerse para que el proceso sea más robusto y es considerar la propiedad transitiva de los pares, realizar un network analysis, lo que significa agrupar todos aquellos id\u0026rsquo;s que tienen suficiente similitud, para eso generamos una variable que sea 1 para aquellos pares que cumplen los criterios y 0 para aquellos que no:\ncand_dupls\u0026lt;-cand_dupls %\u0026gt;% mutate( potdup = case_when( (precios \u0026gt; 0.75 | is.na(precios)) \u0026amp; (metros \u0026gt; 0.9 | is.na(metros)) \u0026amp; (antiguedad \u0026gt; 0.9 | is.na(antiguedad)) \u0026amp; (descripcion \u0026gt; 0.9 | is.na(descripcion)) \u0026amp; (dist_mts\u0026lt;300 | is.na(dist_mts)) ~ 1, T ~ 0 ) )  Lo que tenemos ahora, es un dataframe que tiene los pares que creemos que son duplicados, estos van a ser nuestros vinculos en el analisis de red, los edges:\nedges\u0026lt;-cand_dupls %\u0026gt;% select(Var1, Var2, potdup)  Que tienen esta forma\n  ID1 ID2 match    45557690 44375852 1  44349226 45589067 1  45620618 44714730 1    Que es la forma que acepta el paquete igraph para generar el grafo que va a vincular el id como en la imagen debajo, donde\n 6,9,10 y 4 serían un anuncio separado del resto, por ejemplo  library(igraph) g \u0026lt;- graph_from_data_frame(edges) fc \u0026lt;- fastgreedy.community(as.undirected(g))  Ahora fc es una lista en el que cada elemento es un vector de id\u0026rsquo;s que corresponderían al mismo inmueble para ver por ejemplo el grupo 3 podemos hacer:\nalqs %\u0026gt;% filter(id%in% as.numeric(fc[[3]])) %\u0026gt;% View()  Los links asociados al grupo 3, entonces, son:\n[1] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/arenales-y-callao-excelente-edificio-de-estilo-44714730.html\u0026rdquo;\n[2] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/arenales-1700-43499494.html\u0026rdquo; [3] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/petit-hotel-arenales-y-callao-700-m-sup2--lote-propio-45138045.html\u0026rdquo; [4] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/petit-hotel-arenales-y-callao-700-m-sup2--lote-propio-45564366.html\u0026rdquo; [5] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/petit-hotel-arenales-y-callao-700-m-sup2--lote-propio-45564348.html\u0026rdquo;\nAl grupo 10:\n[1] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/piso-pueyrredon-y-guido-191-m-sup2--una-o-dos-44047718.html\u0026rdquo;\n[2] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/pueyrredon-y-guido-piso-191-m-sup2--1-o-2-cocheras-44748468.html\u0026rdquo; [3] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/piso-pueyrredon-y-guido-191-m-sup2--una-o-dos-45455991.html\u0026rdquo;\n[4] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/pueyrredon-2400-45078667.html\u0026rdquo;\nPero no todo es un éxito, tambien existen grupos tales como el 1, en el que hay más de uno:\n[1] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/en-alquiler-temporario-departamentos-tipo-lofts-de-47-40313161.html\u0026rdquo; [2] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/departamentos-en-alquiler-temporario-posadas-1300-40075639.html\u0026rdquo;\n[3] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/alquiler-loft-m-sup2--47-posadas-1323-recoleta-amobl-y-41937518.html\u0026rdquo; [4] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/gran-oport-recoleta-1-amb-47-m-sup2--piso-alto-en-41324130.html\u0026rdquo;\n[5] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/venta-1amb-al-frente-m-sup2--47-balcon-vista-a-los-41324195.html\u0026rdquo;\n[6] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/venta-gran-oport-posadas-1323-1-amb-m-sup2--47-balcon-41163069.html\u0026rdquo; [7] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/departamento-tipo-lofts-m-sup2--47-amoblados-y-41242674.html\u0026rdquo;\n[8] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/posadas-1323-edificio-alquiler-temporal-departamentos-41683158.html\u0026rdquo; [9] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/loft-al-frente.-amueblado-y-decoracion-de-diseno-42984563.html\u0026rdquo;\n[10] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/posadas-1323-alquiler-depto-amobl.-y-equip-lofts-41775461.html\u0026rdquo;\n[11] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/venta-y-alquiler-temporal-apartments-amoblados-41242680.html\u0026rdquo;\n[12] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/venta-loft-m-sup2--47-balcon-al-frente.-o-en-alquiler-40116759.html\u0026rdquo; [13] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/posadas-1323-lofts-m-sup2--47-amobl-y-equip-confort-41768916.html\u0026rdquo;\n[14] \u0026ldquo;https://www.zonaprop.com.ar/propiedades/posadas-1323-apart-hotel-departamento-41682584.html\u0026rdquo;\nPero en todo caso, esto efectivamente detecta gran parte de los duplicados y puede asistir a que un ser humano genere la identificación de duplicados.\nEsto es un ejemplo de aprendizaje no-supervisado, pero si este analisis se llevara a fondo obtendríamos un set de datos de duplicaciones etiquetadas, lo cual podría ser la base para el analisis con procedimientos de analisis supervisados, y así encontrar patrones de que causa la duplicaciones; lo cual mejoraría el proceso de identificación de duplicados y podría usarse también para optimizar los criterios subjetivos que utilicé mas arriba.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c7eaea7cef7971cf0f2877202916ca7a","permalink":"/post/dedupl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/dedupl/","section":"post","summary":"De-duplicación de Avisos Cualquiera que haya hecho un curso introductorio o leído un libro de Ciencia de Datos sabe que se dice mucho que la limpieza de un dataset es, por lo menos el 80% del trabajo y luego el modelado. Pero eso no quiere decir que no hagan falta técnicas propias del modelado para limpiar un dataset.\nRead more \n","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"ANALYSIS OF SUPPLY AND EVOLUTION OF THE REAL STATE MARKET The real state market is composed of a series of goods and sevices that are heterogeneous in their characteristics and in their localization\nThe features of each of the housing units in supply are, together, the features of the market in general in a given timeframe\nUsually, what is obvserved is the supply stock and it is assumed that the price is in equilibrium, in this sense, the supply is fixing a price which will allow them to sell or rent at a price in the least amount of time. Nevertheless, the flux of actual sales is not observable in this sample, but it might be yet possible to determine by analysing the changes in supply over time\nThe scrapping methods takes the data monthly in order to get the necessary data to perform such analysis.\nIn this presentation, we\u0026rsquo;ll give a view of the supply. Leaving the demand analysis for future oportunity\nRead more \nStudy of supply The total amount of registers we have for the third trimester of 2019 in our scrapping, once deleted repeated adds, is roughly 25675; of which 15562 are apartments and 5489 to houses.\nThe next table is a summary of total and average squared meters, and their values in US Dollars for Mar del Plata\nreferences Departamento = Apartment Casa = House PH = Horizontal Property, a building with housing units much bigger that apartments but has less than 3 floors Terreno = Land Local Comercial = Comerce shop Local Comercial = Comerce office Garage = Garage Fondo de Comercio = On-going business in sale Bodega-Galpon = shed (as big as a block usually) Edificio = Building\n Features of Sales dataset  Tipo Cantidad Precio_M2 Tamaño Precio    Departamento 15562 2066 70 138747  Casa 5489 811 434 245960  PH 1645 1207 101 99352  Terrenos 1451 743 700 316334  Local comercial 659 1751 208 197579  Oficina comercial 287 1569 98 125887  Garage 245 912 1932 803429  Fondo de Comercio 132 1221 968 1340016  Bodega-Galpón 99 611 875 336943  Edificio 51 1737 759 887350    From this table it came out that 82.16% of the supply is composed by Departamento (60.74%) y Casa (21.42%) and that the apartments have the highest prices by squared meter and the least change in supply\nLikewise the data reveal the variation in quantities, prices and sizes of each type of housing unit\nIn this case we can see, for example, that, for apartments, the rise in size (total squared meters) causes that, even though the total price risses, the price by squared meters falls. The oposite occurs for the PH\u0026rsquo;s\nThe dataset also provides housing units that have activaly reduce it\u0026rsquo;s prices in the last three months\n Bajaron de Precio  Tipo Cantidad Bajó Precio_M2 var Precio m2 Tamaño var Tamaño Precio var Precio    Departamento 207 8.63% 1891 -13.51% 54 -33.65% 100875 -35.72%  PH 10 9.80% 1173 12.05% 81 -50.80% 91780 -27.27%    In the table we can see that the average price of apartments drop 8.63%, meanwhile the prices of the PHs dropped even more, an average of 9.8%.\nThe housing units that lowered the price also present a series of features we can exam.\nThe prices for squared meter that lowered the price are 13.5% cheaper than the overall average, meaning that those which lower the price were already cheaper than the overall sample. The same for sizes\nThe PH\u0026rsquo;s have a peculiar feature. Even though, those which lowered the price are smaller than the general sample, the price by squared meter is higher. This is because the difference between the average size si higher than the difference between the price of the housing unit\nReal State brokers The dataset provides as well, the real state broker since november 2019. This allows us to evaluate better who is offering.\nTo begin with, we can see that the majority of the brokers have a few publications, 77% have less than 22 in this trimester. But there is also brokers with a lot of publications; 1% of the brokers have between 276 and 419 publications\nThis data can also be used to track the market-share of each broker, and analyse the evolution\nZones of Supply Density The data is geolocated in 97%, which permits us to map it and visualize the density of the real state supply. As expected, the major density are near the center of town, near the bus terminal and near the cost; given that Mar del Plata is a major tourist center\n A further anlysis of each zone can be made, so we can describe them\nOverall, how is the supply distributed in each zone?\nEsto nos dice que si bien la zona 6 ocupa el 10% de los m² ofrecidos, constituye el 26% de las publicaciones. Y, conjuntamente con la zona 5, son el 25% de los m² ofrecidos, pero superan el 52% del total de publicaciones. This tell us that, even though zone 6 occupies 10% the m² offered, is 26% of the publications. And, toghether with zone 5, they are more than 52% of the publications\nThe variables than will be described for each zone are:\n Type of housing unit Size in squared meters Price by squared meter Total price  Type Below, we can see that the zone of least density (zone 0), the market is composed mainly with land and houses. And as we advance to the zone of major density (zone 6) the composition of the supply varies. In the first place, the land are no longer offered, the apartments takes over as the main type and in the center of town also appears comercial shops and garages\nThis is exactly what one would expect, if the housing unit is smaller, more units fit in a given space\nSizes The sizes of each unit play a fundamental role in the geographical concentration of the supply\nAs we saw in the \u0026ldquo;Type\u0026rdquo; section, we can quantify how the types which are traditionaly the largest (houses, land) affect the concentration\nPrices The prices in each area are more homogeneous than the sizes or any other variable previously analyse. There is still a tendency that more supply means less prices. For example, the least average price is in zone 6\nThis information, doesn\u0026rsquo;t reflect how the prices varies in each zone. Prices in zone 6 may be the least, in average, but this prices hide a major variabilty. Prices in zone 6 have 50% variation than in zone 0. Meaning that, even though the prices are lower, in average, there are a lot with higher prices, and with lower prices. And in zones where houses are the main unit, the prices are more steady\nPrice by squared meter This a sort of conclution of the two previous parts. We saw that the units gets larger in less concentrated areas, and the total prices doesn\u0026rsquo;t change that much. This means that the prices by squared meter will be higher there where there is a major supply concentration\nAs price zones\u0026hellip; The same methology can be used to extract different zones, Prices, number of rooms, etc which can be made ad hoc\n Some of the alternative maps can be seen in: mapa interactivo\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3ca8b3c35f2da3cfb1d34d4a015e35a1","permalink":"/post/mardel_realstate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/mardel_realstate/","section":"post","summary":"ANALYSIS OF SUPPLY AND EVOLUTION OF THE REAL STATE MARKET The real state market is composed of a series of goods and sevices that are heterogeneous in their characteristics and in their localization\nThe features of each of the housing units in supply are, together, the features of the market in general in a given timeframe\nUsually, what is obvserved is the supply stock and it is assumed that the price is in equilibrium, in this sense, the supply is fixing a price which will allow them to sell or rent at a price in the least amount of time. Nevertheless, the flux of actual sales is not observable in this sample, but it might be yet possible to determine by analysing the changes in supply over time\nThe scrapping methods takes the data monthly in order to get the necessary data to perform such analysis.\nIn this presentation, we\u0026rsquo;ll give a view of the supply. Leaving the demand analysis for future oportunity\nRead more \n","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"Twitter y las Paso Esta fue mi primera aproximacion a minar datos de redes sociales, amigarme con las APIs, y algún analysis rudimentario. Siguiendo metodologias propuestas por otros.\nEn principio vamos a comenzar con lo primero ¿como hice para minar los datos de twitter? Bueno para eso use tweepy (http://www.tweepy.org/) Asique la primera parte va a estar en Python.\nRead more \nImportando lo importante\nimport tweepy from tweepy.streaming import StreamListener from tweepy import Stream import time from slistener import SListener import os import matplotlib.pyplot as plt import json import requests  por slistener es un script cortesia de https://github.com/alexhanna/hse-twitter/blob/master/bin/slistener.py que permite crear un objeto que va a ser el \u0026ldquo;listener\u0026rdquo; o \u0026ldquo;escuchante\u0026rdquo; de twitter. Para tomar los datos en tiempo real y poder ir guardandolos.\nInstanciando lo instanciable y setiando los paths donde van las cosas\nauth = tweepy.OAuthHandler('zarazatoken', 'zarazatoken) auth.set_access_token('zarazatoken', 'zarazatoken') api = tweepy.API(auth) datapath = os.path.join(os.getcwd(), 'data') datafiles = os.listdir(datapath) Y vamos a poder el escuchante a escuchar twitter keywords_to_track = ['EleccionesPASO2019', 'FrenteDeTodos','Frente Todos', 'Juntos por el Cambio','Juntos Cambio','Elecciones','PASO', 'YoTeVotoAlberto','NoVuelvenNuncaMas', 'ArgentinaVota','Macri','YoLoVoto','Fernandez','Kirchner'] stream.filter(track = keywords_to_track)  Los keywords a trackear se eligieron tomando los trending topic en argentina referidos a las elecciones y algunos elegidos por mi, a mano\nstream.filter() lo que se encarga de hacer es ir tomando la muestra en tiempo real de datos de twitter que se ajusten al filtro. Mientras corra (es decir, mientras no se interrumpa) va a ir juntando los datos. Esto lo empece a correr el domingo de las paso a las 7am y lo frené el mismo día a las 17hs.\nLa siguiente parte me fue bastante mas dificil de lo que habia anticipado, porque estas muestras se guardan en formato \u0026ldquo;.json\u0026rdquo; lo cual tenia que convertir a \u0026ldquo;.csv\u0026rdquo; para poder trabajar mejor\nimport pandas as pd import numpy as np tweets = [] with open(os.path.join(datapath,datafiles[1]), 'r',encoding='utf-8', errors='ignore') as t: tw_json = t.read().split('\\n') for tw in tw_json: #print(tw) #print('\\t\\t') try: tweet_obj = json.loads(tw) except: pass if 'extended_tweet'in tweet_obj: tweet_obj['extended_tweet-text'] = tweet_obj['extended_tweet']['full_text'] if tw != '': tweets.append(tw) pd.DataFrame(tweets)  ¡Ahora si! Ya tenemos bonito el data_frame en pandas para guardarlo y seguir desde allí\nLa siguiente tarea seria el topic extraction, pero la realidad es que cuando lo hice no llegue a ningun lado, porque, obviamente y como es de esperar, estaba todo referido a as elecciones. Lo que si termine haciendo fue filtrar el dataset que me quedo por las keywords que nombrar a los dos principales candidatos\n{'YoVotoMM','juntosporelcambio', 'votomm','NoVuelvenNuncaMas', 'yolovoto','Macri'} ## Quedarón 6,320 registros {'FrenteDeTodos','futurocontodos','YoTeVotoAlberto','FernandezFernandez','CFK,'cristina kirchner''Alberto Fernandez'} ## Quedarón 5,554 registros  Ahora bien, ¿de quien se hablaba mas en twitter?\nPrimero al dataframe de cada topic se agrega la variable correspondiente, se generan las dummies y luego se saca el promedio por hora, lo que resulta en la proporcion de tuits de cada uno\nmacri['p'] = 'MM' frente_de_todos['p'] = 'FF' df1 = pd.concat([macri, frente_de_todos]) df2 = pd.get_dummies(df1.p) mean_mm = df2['MM'].resample('1 h').mean() mean_ff = df2['FF'].resample('1 h').mean()  Sacando el plot:\nSe ve que salvo a la mañana y bien entrada la tarde, se hablo mas de Macri.\nBueno, habiendo hecho la primera parte en Python, es hora de continuar con la parte de sentiment analysis de los tuits de las PASO. Esta vez, en R. Vamos a empezar por las bibliotecas que necesitamos:\nlibrary(tidyverse) library(tidytext) library(stopwords) library(syuzhet) library(stopwords)  Luego traemos los datos:\npaso\u0026lt;-read_csv('paso.csv') paso\u0026lt;-paso[colnames(paso)!=\u0026quot;X1\u0026quot;] paso_unique\u0026lt;-unique(paso$`extended_tweet-full_text`) paso_unique2\u0026lt;-as_tibble(paso_unique)  El unique() nos sirve para filtrar tuits duplicados. Que pueden ocurrir por que un usuario citó a otro.\nlength(paso$`extended_tweet-full_text`) #Quedan 44423 registros length(unique(paso$`extended_tweet-full_text`)) #Quedan 43996 registros  Vamos a tokenizar las palabras:\ntweet_token\u0026lt;-paso_unique2%\u0026gt;% unnest_tokens(word, txt) tweet_token\u0026lt;-tweet_token%\u0026gt;% count(word, sort = T)%\u0026gt;% filter(!word%in% stopwords('es'))%\u0026gt;% filter(!word%in% stopwords('en'))%\u0026gt;% filter(str_detect(word, \u0026quot;^[a-zA-z]|^#|^@\u0026quot;))%\u0026gt;% ungroup()%\u0026gt;% arrange(desc(n))%\u0026gt;% mutate(w = word, freq = n)%\u0026gt;% select(w, freq) ## Resultado w freq \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; 1 t.co 18839 2 https 18834 3 paso 16293 4 elecciones 8401 5 macri 7982 6 si 5853 7 eleccionespaso2019 5498 8 votar 4950 9 q 4318 10 hoy 3304  Esto no es muuy bueno, hay tokens que hay que sacar.\ntweet_token_2\u0026lt;-tweet_token%\u0026gt;%filter(w!='t.co')%\u0026gt;%filter(w!='https')%\u0026gt;% filter(w!='q')%\u0026gt;%filter(w!='to')%\u0026gt;%filter(w!='si')%\u0026gt;%filter(w!='and')%\u0026gt;% filter(w!='rt') w freq \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; 1 paso 16293 2 elecciones 8401 3 macri 7982 4 eleccionespaso2019 5498 5 votar 4950 6 hoy 3304 7 argentinavota 3043 8 eleccionesargentina 2878 9 voto 2588 10 trump 2409  Ahora si, mira que loco lo de Trump. Igual esto se ve mucho mejor con un gráfico, además, no filtre todavia los stopwords y no filtre por tuits en español, asique probablemente sean tuits colados de otro tema\ntweet_token_2 [ 1 : 25 , ] %\u0026gt;% mutate ( w = forcats :: fct_inorder ( w ) ) %\u0026gt;% ggplot ( aes ( x = w , y = freq ) ) + geom_segment ( aes ( x = w , xend = w , y = 0 , yend = freq ) , color= \u0026quot;grey\u0026quot; )+ geom_point(size = 3, color = \u0026quot;#009A44\u0026quot;)+ coord_flip()  Bueno, el termino \u0026ldquo;paso\u0026rdquo; es evidentemente el mas frecuente, lo cual es mas que esperable. Luego, nos quedaria ver como se sentia la gente respecto a esto. Para esto se uso la libreria syuzhet\nbase_emocion\u0026lt;-get_nrc_sentiment(unlist(paso_unique2)) base_emocion \u0026lt;- data.frame(t(base_emocion)) base_emocion \u0026lt;- data.frame ( rowMeans ( base_emocion ) ) names ( base_emocion ) [ 1 ] \u0026lt;- \u0026quot;Proporcion\u0026quot; base_emocion \u0026lt;- cbind ( 'Sentimiento' = rownames ( base_emocion ) , base_emocion ) base_emocion%\u0026gt;% ggplot()+geom_bar(aes(x = Sentimiento, y = Proporcion), stat = 'identity', fill = 'green', alpha = 0.8)+ theme(axis.text.x = element_text(angle = 45, hjust = 1))  Esta es la parte mas relevante que tome de Hernan, la diferencia que tome, fue que él tomo la suma de cada una de los casos de cada sentimiento, y yo la proporcion. Creo que eso puede reflejar de otra forma cual es la emocion predominante en cada caso:\nEn este caso general, se ve que entre \u0026ldquo;positivo\u0026rdquo; y \u0026ldquo;negativo\u0026rdquo; son los predominantes, seguidos por \u0026ldquo;confianza\u0026rdquo; y \u0026ldquo;enojo\u0026rdquo;.\nEsta misma metodologia se puede usar para los dos datasets separados para cada topic pre-seleccionado, los referidos al frente de todos y a juntos por el cambio\nTodos se quejan, pero de Cristina hablan todos. Igual hay que considerar que de este conteo, se filtraron los nombres y apellidos de los candidatos a la presidencia ya que es lo que se uso de filtro.\n¿Como se sienten? Un poco de esto confirma no solo los resultado de la eleccion sino tambien la lectura del voto \u0026ldquo;enojo\u0026rdquo;. Porque los sentimientos asociados a cambiemos tienen mayor participacion de enojo y sentimientos negativos. Mientras que los asociados al frente de todos tiene mucha mayor participacion los tuits positivos.\nUn bonus track de python nada mas (esbozo de network analysis) ¿A quien se le contestaba mas para cada grupo?\nLas libreras de python son las mismas que el post anterior solo con la adicional de Networkx que permite hacer el analisis de redes.\nimport networkx as nx frente_de_todos = pd.read_csv('frente_de_todos.csv') cambiemos = pd.read_csv('cambiemos.csv')  Una vez leidos, filtramos los tuits que \u0026ldquo;son respuesta a\u0026rdquo;\ncambiemos_nx = cambiemos_nx[-cambiemos_nx['in_reply_to_screen_name'].isnull()] cambiemos_nx['in_reply_to_screen_name'] frente_de_todos_nx = frente_de_todos_nx[-frente_de_todos_nx['in_reply_to_screen_name'].isnull()] frente_de_todos_nx['in_reply_to_screen_name']  Se generan las redes de c/u; esto genera que cada usuario sea un nodo y que la relacion entre los usuarios se dá, en este caso particular, si responden a un tuit es decir: si yo te respondo un tuit, nosotros dos generamos una red que tiene mi nombre (mi usuario) como nodo de inicio y tu nodo (tu usuario) como nodo destino. Cada objeto, entonces, va a tener tantas salidas como respuestas haya hecho y tantas entradas como respuestas haya recibido:\nG_reply_c = nx.from_pandas_edgelist( cambiemos_nx, source = 'user-screen_name', target = 'in_reply_to_screen_name', create_using = nx.DiGraph()) G_reply_f = nx.from_pandas_edgelist( frente_de_todos_nx, source = 'user-screen_name', target = 'in_reply_to_screen_name', create_using = nx.DiGraph())  Y vemos la centralidad de cada tuitero (\u0026ldquo;in-degree-centrality\u0026rdquo;), que en realidad seria la respuesta a la pregunta \u0026ldquo;¿A quien se le esta contestando más?\u0026rdquo;\n#Para Frente de Todos - ¿a quien se le contesta cuando se habla de estse #tema? G_reply_f = nx.from_pandas_edgelist( frente_de_todos_nx, source = 'user-screen_name', target = 'in_reply_to_screen_name', create_using = nx.DiGraph()) bc = nx.in_degree_centrality(G_reply_f) indg = pd.DataFrame(list(bc.items()), columns =[\u0026quot;Name\u0026quot;,'Cent']) indg.sort_values('Cent', ascending=False) Name Cent 153 alferdez 0.018328 18 ierrejon 0.017182 137 LotusHerbals 0.017182 115 todonoticias 0.016037 113 fllorenteantoni 0.013746 746 AlbertoRavell 0.010309 159 FernandezAnibal 0.010309 147 LeonelFernandez 0.006873 236 mirthalegrand 0.006873  Y en el caso del Juntos Por el Cambio\nG_reply_c = nx.from_pandas_edgelist( cambiemos_nx, source = 'user-screen_name', target = 'in_reply_to_screen_name', create_using = nx.DiGraph()) bc = nx.in_degree_centrality(G_reply_c) indg = pd.DataFrame(list(bc.items()), columns =[\u0026quot;Name\u0026quot;,'Cent']) indg.sort_values('Cent', ascending=False) Name Cent 36 mauriciomacri 0.021858 148 fllorenteantoni 0.012610 131 gabicerru 0.011349 84 juansolervalls 0.008827 119 EsmeraldaMitre 0.007566 3 todonoticias 0.005885 99 CamiSolovitas 0.004624 17 Alfredo5019 0.004624 23 SantoroLeandro 0.004203  FIN! Gracias por leer hasta acá! Si tienen alguna recomendacion para tener en cuenta futuros analisis se los agradece!\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3e36cc2807c52c89acc7d62eff61f9a5","permalink":"/post/paso2019/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/paso2019/","section":"post","summary":"Twitter y las Paso Esta fue mi primera aproximacion a minar datos de redes sociales, amigarme con las APIs, y algún analysis rudimentario. Siguiendo metodologias propuestas por otros.\nEn principio vamos a comenzar con lo primero ¿como hice para minar los datos de twitter? Bueno para eso use tweepy (http://www.tweepy.org/) Asique la primera parte va a estar en Python.\nRead more \n","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"Review Rating ¿Cómo generar un puntaje númerico en base a un texto?\nMucho de lo expuesto es en realidad distintas formas de pensar el problema y quedarse con la mejor solución.\nPaseo por Doc2Vec, Regresiones lineales, randomForests y redes convolutivas con GloVe\nRead more \nEn principio, lo que se va a ver es las calificaciones haciendo uso de Doc2Vec para convertir el texto en un vector numerico y poder, con esos vectores numericos como input, realizar la predicción de cual sería la calificación que hubiera tenido según el texto. Haciendo uso de algortimos de aprendizaje supervisado.\nPor otro lado, dado que los datos no son tantos, lo que perjudica la construcción del vector numerico a partir de los textos, se hará uso de un \u0026ldquo;word embedding\u0026rdquo; ya entrenado y posteriormente se verá como mejora el poder predictivo.\nLos datos y la limpieza Para empezar, veamos como se ven los datos:\n   review.rating review.text    5 Our experience at Rancho Valencia was absolutely perfect from beginning to end!!!! We felt special and very happy during our stayed. I would come back in a heart beat!!!  5 Amazing place. Everyone was extremely warm and welcoming. We've stayed at some top notch places and this is definitely in our top 2. Great for a romantic getaway or take the kids along as we did. Had a couple stuffed animals waiting for our girls upon arrival. Can't wait to go back.  2 We booked a 3 night stay at Rancho Valencia to play some tennis, since it is one of the highest rated tennis resorts in America. This place is really over the top from a luxury standpoint and overall experience. The villas are really perfect, the staff is great, attention to details (includes fresh squeezed orange juice each morning), restaurants, bar and room service amazing, and the tennis program was really impressive as well. We will want to come back here again.    En lugar de importar todos los paquetes juntos, vamos a ir importando a medida que vayamos necesitando.\nimport pandas as pd cc = pd.read_csv('./hotel-reviews/Datafiniti_Hotel_Reviews.csv')  Vamos a seleccionar las columnas necesarias y cambiarle el nombre para que sea mas facil luego seleccionarlas.\ncc = cc[['reviews.title','reviews.text','reviews.rating']] cc.columns = ['titulo','comentarios','calificacion']  El primer paso a la hora de trabajar con estos textos, es reducir a la maxima expresión la cardinalidad del vocabulario. ¿Que significa esto? Si tenemos una gran cantidad de usos de un verbo, como por ejemplo, \u0026ldquo;correr\u0026rdquo; en sus distintas conjugaciones, \u0026ldquo;corría\u0026rdquo;,\u0026ldquo;corriendo\u0026rdquo;,\u0026ldquo;corrian\u0026rdquo; y queremos armar un listado de frecuencias de palabras, esto daria como resultado que cada uno de esas palabras aparezca una sola vez; pero si logramos que la referencia a la acción concreta de \u0026ldquo;correr\u0026rdquo; sume independientemente de su conjugación, reduciríamos la cardinalidad de nuestro diccionario, eso es para lo que se usa la lematización.\n  Original Lemmatizacion    corrian correr  corriendo correr  corrio correr    Luego, hay sustantivos y otras palabras que tienen la misma raiz y dependiendo del sujeto, se puede reducir el tamaño del diccionario cortando de la raiz la palabra, lo que se conoce como stemmización\n  Original Stemmización    niña niñe  niño niñe  niñe niñe    El proceso de lematización y stemización, en su conjunto, se puede entender como normalizar el vocabulario y por lo tanto, una función que se encarge de hacer estas dos cosas, puede llamarse normalize(text):\ndef normalize(text): from nltk.stem import PorterStemmer from nltk.tokenize import word_tokenize import unidecode import spacy nlp = spacy.load('en_core_web_sm') porter = PorterStemmer() doc = nlp(text) lemmas = [unidecode.unidecode(tok.lemma_.lower()) for tok in doc if not tok.is_punct ] #En este caso, estoy eliminando palabras con menos de 3 letras y las negaciones, esto no es necesario estrictamente, y depende mucho del caso de aplicación, a veces funciona, a veces no. lexical_tokens = [t.lower() for t in lemmas if (len(t) \u0026gt; 3 or t ==\u0026quot;no\u0026quot;) and t.isalpha()] lexical_tokens = [porter.stem(t) for t in lexical_tokens] return lexical_tokens  Esta funcion tiene como input una frace y escupe objeto tipo list() asique lo que haré es aplicarla a cada texto y despues volverla a unir para que cada fila tenga un texto y no un array\nnorm_token = [] for i in range(len(cc.comentarios)): try: a = normalize(cc.comentarios[i]) except: a = '' norm_token.append(a) norm_text = [' '.join(x) for x in norm_token] cc['norm_text'] = norm_text  Doc2Vec: Generando el embedding numérico ¿Porque no tratar directo con los tokens? Por la sencilla razón hay un paquete que permite aplicar Doc2Vec, asociando un texto a una clase, gensim nos va a venir bien para esto:\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument #Primera la separacion entre test y train train, test = train_test_split(cc, test_size=0.2, random_state=42) train_tagged = train.apply( lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1) test_tagged = test.apply( lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1)  Una vez que tenemos los elementos de train y test, hay que entrenar el Doc2Vec, para, así pasar el texto a un vector númerico que pueda ser el input del algoritmo de clasificación\nfor epoch in range(30): model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1) model_dbow.alpha -= 0.002 model_dbow.min_alpha = model_dbow.alpha  Con el modelo Doc2Vec entrenado, podemos darle pasar los textos y obtener el vector numerico deseado. Ahora bien, para facilitar la implementación del modelo después, generemos una función con dos ouputs, el vector numerico por un lado, y el rating asociado a ese vector numérico\ndef vec_for_learning(model, tagged_docs): sents = tagged_docs.values targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents]) return targets, regressors  Ahora que tenemos la función que nos genera el vector númerico terminamos con el proceso de preparación de los datos. Es curioso notar que siempre se dice que el 80% del trabajo consiste en la preparación y limpieza de datos, y el 20% el modelado. Hasta ahora se puede ver que no es una distinción tan discreta, sino que es continua. ¿A qué me refiero? Bueno, para preparar los datos hizo falta algoritmos de embedding (Doc2Vec). Y no es poco común que ocurran estas cosas.\nAhora bien, volvamos a lo nuestro, es hora de correr los algoritmos de regresión:\nRegresión from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LinearRegression y_train, X_train = vec_for_learning(model_dbow, train_tagged) y_test, X_test = vec_for_learning(model_dbow, test_tagged) lr = LinearRegression() lr.fit(X_train, y_train) y_pred = lr.predict(X_test) print(np.sqrt(mean_squared_error(y_test, y_pred))) #1.15  Este resultado no me convence demasiado si consideramos al puntaje como una regresión. El error cuadratico medio es del 1.15pts, para los que no recuerdan, el error cuadratico medio toma la diferencia entre el valor predecido y el valor real, lo eleva al cuadrado y de ello toma el promedio. Les dejo un video  de mi canal de Youtube con la visualización de lo que significa\nClasificación Otra forma de entender el problema es como uno de clasificación. ¿Pero como pasamos de un target continuo a uno discreto? Podríamos pensar que los puntajes de 4 ó 5 son \u0026ldquo;buenos\u0026rdquo;, y asignarles un 1, y los de menos de 4 son \u0026ldquo;malos\u0026rdquo;, y asignarles un 0, y nos quedamos con un problema de clasificación binaria.\nAdemás, con estas conceptualización, tenes que volver a correr el embedding porque los \u0026ldquo;tags\u0026rdquo; no son ahora los puntajes del 1 al 5 sino que son {1,0}\ncl = [] for i in cc.calificacion: if i \u0026lt;4: cl.append(0) else: cl.append(1) cc.calificacion = cl train, test = train_test_split(cc, test_size=0.2, random_state=42) train_tagged = train.apply( lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1) test_tagged = test.apply( lambda r: TaggedDocument(words=word_tokenize(r.norm_text), tags=[r.calificacion]), axis=1) model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0) model_dbow.build_vocab([x for x in tqdm(train_tagged.values)]) for epoch in range(30): model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1) model_dbow.alpha -= 0.002 model_dbow.min_alpha = model_dbow.alpha  Ahora si, sacamos los vectores del modelo Doc2Vec y lo fiteamos a un randomForest clasificador:\nfrom sklearn.ensemble import RandomForestClassifier y_train, X_train = vec_for_learning(model_dbow, train_tagged) y_test, X_test = vec_for_learning(model_dbow, test_tagged) rfr = RandomForestClassifier(n_estimators = 500, random_state = 42) rfr.fit(X_train, y_train)  ¿Cómo evaluamos esta clasificación? Bueno, primero nos fijamos cuanto coincide la predicción respecto al valor real, para eso se usa la matriz de confusión\nfrom sklearn.metrics import confusion_matrix y_pred = rfr.predict(X_test) tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0,1]).ravel() np.mean(y_test == y_pred)  Nuestra precisión es del 72,4%, nada mal, solo un detalle. Si mandamos un modelo que siempre diga \u0026ldquo;1\u0026rdquo;, tendremos una precisión equivalente a la proporción de \u0026ldquo;1\u0026rdquo; en el set. Que en la base completa es de 72,8%. Es decir, este modelo no mejor que decir que todas son igual a \u0026ldquo;1\u0026rdquo;.\nPolaridad como métrica Otra alternativa puede ser extraer la polaridad del texto, herramienta muy útil en los procesos de sentiment analysis. Y podríamos pensar que, cuan más positivo sea la polaridad, estará asociado a un mejor puntaje\nfrom textblob import TextBlob pls = [] sbj = [] for i in range(len(cc.comentarios)): try: senti = TextBlob(cc.comentarios[i]) polarity = senti.sentiment pls.append(polarity[0]) sbj.append(polarity[1]) except: pls.append(0) sbj.append(0) polscore = [int(x \u0026gt; 0) for x in pls] # Acá 0 es un valor arbitrario de corte, #un ejericio podría incluir la optimización de este valor como un hiperparametro. #La polaridad genera un indice de -1, 1. Siendo -1 cuan más negativo es, y 1 cuan más #positivo es, y polscore dice que aquellos que tienen valoracion positiva sean 1 y los demás 0 np.mean(polscore == np.array(cc.calificacion))  Sirve esto? Y, esto esta dando un resultado de 78.55%, es, a mi sorpresa, una mejora respecto al punto anterior. Aunque todavía no es satisfactorio.\nEvidentemente el score de polarización agrega información.\nRedes neuronales y GloVe Global Vectors ó GloVe  es una tecnica que, a diferencia de Doc2Vec, que es un algortimo supervisado, es no-supervisado y obtiene embedings númericos de palabras según estadisticas de co-ocurrencia. De esta manera puede encontrar analogías tales como \u0026ldquo;los que varón es a mujer, rey es a reina\u0026rdquo;.\nMás allá las controversias , es una herramienta bastante útil para muchos casos\nEl objetivo de esta sección es ver como, haciendo uso de un modelo de lenguage pre-existente, se puede usar el proceso de transfer-learning para incorporar nuestros textos y sus calificaciones y adaptarlo a nuestras necesidades.\nEsto es interesante e importante porque hay muchos modelos de lenguage que han hecho uso de datasets enormes en hardwares mucho más potentes que los que podría pagar, que se puede aprovechar\nimport os import sys import numpy as np from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.utils import to_categorical from keras.layers import Dense, Input, GlobalMaxPooling1D from keras.layers import Conv1D, MaxPooling1D, Embedding from keras.models import Model from keras.initializers import Constant BASE_DIR = os.getcwd() GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B') #Este es el modelo pre-entrenado TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup') MAX_SEQUENCE_LENGTH = 1000 #entrenar sobre oraciones de hasta estas palabras MAX_NUM_WORDS = 20000 #tamaño máximo del vocabulario EMBEDDING_DIM = 300 #dimensión del vector númerico resultante VALIDATION_SPLIT = 0.2 #Volvemos a cargar la información import pandas as pd cc = pd.read_csv('./hotel-reviews/Datafiniti_Hotel_Reviews.csv') cc = cc[['reviews.title','reviews.text','reviews.rating']] cc.columns = ['titulo','comentarios','calificacion'] cl = [] for i in cc.calificacion: if i \u0026lt;4: cl.append(0) else: cl.append(1) cc.calificacion=cl #Preparación de los datos TEXT_DATA_DIR = cc.comentarios embeddings_index = {} with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f: for line in f: word, coefs = line.split(maxsplit=1) coefs = np.fromstring(coefs, 'f', sep=' ') embeddings_index[word] = coefs texts = [x for x in cc.comentarios] # listado de muestras de texto labels_index = {1:1, 2:2, 3:3, 4:4, 5:5} # diccionario mapeando los revies a los target labels = [int(x) for x in cc.calificacion] # target # Tokenizar las palabras texts = np.array(texts) tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) tokenizer.fit_on_texts(texts) sequences = tokenizer.texts_to_sequences(texts) word_index = tokenizer.word_index #Con las palabras tokenizadas, se hace el padding, para que todos tengan la misma longitud, para eso se agrega 0 hasta que se llene data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) labels = to_categorical(np.asarray(labels)) indices = np.arange(data.shape[0]) np.random.shuffle(indices) data = data[indices] labels = labels[indices] num_validation_samples = int(VALIDATION_SPLIT * data.shape[0]) x_train = data[:-num_validation_samples] y_train = labels[:-num_validation_samples] x_val = data[-num_validation_samples:] y_val = labels[-num_validation_samples:] num_words = min(MAX_NUM_WORDS, len(word_index) + 1) embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) for word, i in word_index.items(): if i \u0026gt;= MAX_NUM_WORDS: continue embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[i] = embedding_vector #Este es el proceso que genera los embeddings embedding_layer = Embedding(num_words, EMBEDDING_DIM, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_SEQUENCE_LENGTH, trainable=False)  Ok, hasta ahi la preparación de los datos, es hora de entrenar una red neuronal convolutiva con los embeddings realizados para obtener el modelo que clasifique las reiews:\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') embedded_sequences = embedding_layer(sequence_input) x = Conv1D(128, 6, activation='relu')(embedded_sequences) x = MaxPooling1D(5)(x) x = Conv1D(128, 6, activation='relu')(x) x = MaxPooling1D(6)(x) x = Conv1D(128, 6, activation='relu')(x) x = GlobalMaxPooling1D()(x) x = Dense(128, activation='relu')(x) preds = Dense(2, activation='softmax')(x) model = Model(sequence_input, preds) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])  Hay mucho código repetido en este caso, pero eso es para simplicidad de exposición, lo relevante a enteder es que así se define la capa convolutiva, que esla que se repite:\nx = Conv1D(128, 6, activation='relu')(embedded_sequences) x = MaxPooling1D(5)(x)  Y así la ultima capa, que como terminamos con las \u0026ldquo;buenas\u0026rdquo; y \u0026ldquo;malas\u0026rdquo; reviews, tiene una función de activación binaria en el output\nx = Dense(128, activation='relu')(x) preds = Dense(2, activation='softmax')(x)  Una vez preparados los datos, y una vez definidas las capas de la red neuronal se entrena:\nmodel.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val))  Este fit, obtiene un accuracy que supera el 90%. Un gran paso adelante.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"149707dc6ab3f739041fb53bc2fd6da0","permalink":"/post/review_rating/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/review_rating/","section":"post","summary":"Review Rating ¿Cómo generar un puntaje númerico en base a un texto?\nMucho de lo expuesto es en realidad distintas formas de pensar el problema y quedarse con la mejor solución.\nPaseo por Doc2Vec, Regresiones lineales, randomForests y redes convolutivas con GloVe\nRead more \n","tags":null,"title":"","type":"post"}]