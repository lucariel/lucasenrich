[{"authors":null,"categories":null,"content":"Clusterize design patterns ¿How can I find patterns in design?\nWith so many adds around, one would think that that process is quite standardized but it remains, most times, a manual labor. This is, I believe mostly, because of how competitive that market is. To stand out, graphic designers have still a lot of work\nBut that doesn\u0026rsquo;t mean that we can\u0026rsquo;t find design patterns, after all, after a few thousend design even the best designer tend to have trends.\nRead more \nInput data\nThis problem had, originally, many houndred of designs, in this example I will use just a few made ad hoc for this purposes because in the end, clustering algorithms doesn\u0026rsquo;t need that many data to be effective.\nGiven these are manual examples, it\u0026rsquo;s all about sizes and shapes. I left behind fonts, content of the images and other. What I look for is pattern in the layout of the elements in the banner\nOnce extracted, data came in this form:\nWhere:\n  y : Distance from the top \n  x : Distance from the left \n  w : Width \n  h : Height \n  If we have 50 exameples, with 3 elements each, and 4 variables per element the shape of the input file is 50 × 3 × 4. Algorithms like I used like 2D data better, so I spread them to 50 × 12 for which:\nFirst, iterate file by file and transform each from 1 × 3 × 4 to 1 × 12. In R code:\nlibrary(tidyverse) cols_used = c('element_top', 'element_left', 'element_width', 'element_height') spread_file\u0026lt;-function(data, cols_used){ cols_used_a = c('element_name',cols_used) y=data[cols_used] h = data[cols_used_a] z=c(1,1,1,1) for(i in 1:nrow(y)) { z = cbind(z,y[i,]) } z = z[1,-1] newcols \u0026lt;- c() for (i in h['element_name']){ newcols\u0026lt;-cbind(newcols,paste(i,cols_used[1], sep = '.')) newcols\u0026lt;-cbind(newcols,paste(i,cols_used[2], sep = '.')) newcols\u0026lt;-cbind(newcols,paste(i,cols_used[3], sep = '.')) newcols\u0026lt;-cbind(newcols,paste(i,cols_used[4], sep = '.')) } newcols2\u0026lt;-c() for(i in 1:nrow(newcols)) { for(j in 1:4){ newcols2\u0026lt;-c(newcols2,newcols[i,j]) } } colnames(z)\u0026lt;-newcols2 n\u0026lt;-as_vector(data['id']) z['id']\u0026lt;-n[1] z }  And then iterate to transform this\n$$\\begin{bmatrix} elem1 \u0026amp; y_1 \u0026amp; x_1 \u0026amp; w_1 \u0026amp; h_1 \\\\ elem2 \u0026amp; y_2 \u0026amp; x_2 \u0026amp; w_2 \u0026amp; h_2 \\\\ \\vdots \\\\ elemk \u0026amp; y_2 \u0026amp; x_k \u0026amp; w_k \u0026amp; h_k \\end{bmatrix}$$ in this:\n$$\\begin{bmatrix} id.1 \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\end{bmatrix}$$ This way, you can stack them into:\n$$\\begin{bmatrix} id.1 \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\\\ id.2 \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\\\ \\vdots \\\\ id.N \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\end{bmatrix}$$\n Dimentionality reduction + Clustering   This most direct form of clusterization for this porpuses is dbscan and run it on our transform base. This didn\u0026rsquo;t work as expected so first I used a technique to reduce dimensionality and then do the clustering.\nPCA and t-SNE are the most popular algorithms in dimensionality reduction but UMAP is the new kid in the block (well it has almost 2 years now) with some fancy math behind it, it preserves global and local structures. and works faster using graphs. One thing to keep in mind when using it is that at one point uses a random procedure which makes it that each time you run it the mapping into 2D will look slightly different, but every point is similary close to others in each iteration. To prevent this, set.seed() is the way to go.\nAnd finally:\nlibrary(umap) library(dbscan) umap_data\u0026lt;- umap(data) cl \u0026lt;-hdbscan(x = umap_data, minPts = 3)  This worked better than with the full dimentions, but not quite as needed. Its time to\u0026hellip;\nTransform and normalice\nMost common option \n  Standarization (z-score): Represents the number of standard deviations up or down of resulting value. Useful for normally distributed variables \n  Normalization (min-max scaler): It allows to transform the data into values between 0 and 1. Useful when working with variables with different orders of magnitude \n  Can I use this transformations in this data?\n Not really, what this variables describe are absolute positions in space and are quite linked to one-another.    What can I do? Instead of using absolute positions and dimentions, lets use its relative, what I\u0026rsquo;ll call \u0026ldquo;geometric normalization\u0026rdquo;  normalize_geometric\u0026lt;-function(df){ df['total_area']\u0026lt;-max(df['element_height'])*max(df['element_width']) df['rel_area']\u0026lt;-df['element_height']*df['element_width']/df['total_area'] df['orientation']\u0026lt;-df['element_height']/df['element_width'] df['element_top_relative']\u0026lt;-df['element_top']/max(df['element_height']) df['element_left_relative']\u0026lt;-df['element_left']/max(df['element_width']) df }    x\u0026rsquo; is now the proportion of x in respecto the total width of the canvas  My new variable is x\u0026rsquo;, red line divided the blue one \n y\u0026rsquo; is now the proportion of x in respecto the total height of the canvas  My new variable is y\u0026rsquo;, red line divided the blue one \n areaRelativa is the proportion of the canvas the element occupies  My new variable areaRelativa is: the are of the small rectangle divided divided the area of the big one \n disposition is to know if the elmenet in horizontal position, vertical or if it is a square  My new variable disposicion es: heigth/width \nResults To begin to analize the results, every \u0026ldquo;spread\u0026rdquo; has to have a \u0026ldquo;gather\u0026rdquo;\ngather_file\u0026lt;-function(gdf){ x\u0026lt;-strsplit(colnames(gdf), '\\\\.') element_name=unique(unlist(map(x, 1))) original_cols=unique(unlist(map(x, 2))) gdf1\u0026lt;-data.frame(element_name) gdf1[original_cols[1]]\u0026lt;-0 gdf1[original_cols[2]]\u0026lt;-0 gdf1[original_cols[3]]\u0026lt;-0 gdf1[original_cols[4]]\u0026lt;-0 rel_area\u0026lt;-c() orientation\u0026lt;-c() element_top_relative\u0026lt;-c() element_left_relative\u0026lt;-c() for(i in seq(from=1, to=length(gdf), by=4)){ # stuff, such as rel_area=c(rel_area,gdf[i]) orientation=c(orientation,gdf[i+1]) element_top_relative = c(element_top_relative,gdf[i+2]) element_left_relative = c(element_left_relative,gdf[i+3]) } gdf1['rel_area']=as_vector(unlist(rel_area)) gdf1['orientation']=as_vector(unlist(orientation)) gdf1['element_top_relative']=as_vector(unlist(element_top_relative)) gdf1['element_left_relative']=as_vector(unlist(element_left_relative)) gdf1}  Let\u0026rsquo;s first see how the clustering works with and without geometric normalization\nAnd finally, examples of each group:\nFirst cluster:\n000003.png\nSecond cluster:\nThird:\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"97cb68bc399d0c03548aaade824fda5d","permalink":"/en/en/post/clusterizar-disenos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/en/post/clusterizar-disenos/","section":"en","summary":"Clusterize design patterns ¿How can I find patterns in design?\nWith so many adds around, one would think that that process is quite standardized but it remains, most times, a manual labor. This is, I believe mostly, because of how competitive that market is. To stand out, graphic designers have still a lot of work\nBut that doesn\u0026rsquo;t mean that we can\u0026rsquo;t find design patterns, after all, after a few thousend design even the best designer tend to have trends.\nRead more \n","tags":null,"title":"","type":"en"},{"authors":null,"categories":null,"content":"Clusterize design patterns ¿How can I find patterns in design?\nWith so many adds around, one would think that that process is quite standardized but it remains, most times, a manual labor. This is, I believe mostly, because of how competitive that market is. To stand out, graphic designers have still a lot of work\nBut that doesn\u0026rsquo;t mean that we can\u0026rsquo;t find design patterns, after all, after a few thousend design even the best designer tend to have trends.\nRead more \nInput data\nThis problem had, originally, many houndred of designs, in this example I will use just a few made ad hoc for this purposes because in the end, clustering algorithms doesn\u0026rsquo;t need that many data to be effective.\nGiven these are manual examples, it\u0026rsquo;s all about sizes and shapes. I left behind fonts, content of the images and other. What I look for is pattern in the layout of the elements in the banner\nOnce extracted, data came in this form:\nWhere:\n  y : Distance from the top \n  x : Distance from the left \n  w : Width \n  h : Height \n  If we have 50 exameples, with 3 elements each, and 4 variables per element the shape of the input file is 50 × 3 × 4. Algorithms like I used like 2D data better, so I spread them to 50 × 12 for which:\nFirst, iterate file by file and transform each from 1 × 3 × 4 to 1 × 12. In R code:\nlibrary(tidyverse) cols_used = c('element_top', 'element_left', 'element_width', 'element_height') spread_file\u0026lt;-function(data, cols_used){ cols_used_a = c('element_name',cols_used) y=data[cols_used] h = data[cols_used_a] z=c(1,1,1,1) for(i in 1:nrow(y)) { z = cbind(z,y[i,]) } z = z[1,-1] newcols \u0026lt;- c() for (i in h['element_name']){ newcols\u0026lt;-cbind(newcols,paste(i,cols_used[1], sep = '.')) newcols\u0026lt;-cbind(newcols,paste(i,cols_used[2], sep = '.')) newcols\u0026lt;-cbind(newcols,paste(i,cols_used[3], sep = '.')) newcols\u0026lt;-cbind(newcols,paste(i,cols_used[4], sep = '.')) } newcols2\u0026lt;-c() for(i in 1:nrow(newcols)) { for(j in 1:4){ newcols2\u0026lt;-c(newcols2,newcols[i,j]) } } colnames(z)\u0026lt;-newcols2 n\u0026lt;-as_vector(data['id']) z['id']\u0026lt;-n[1] z }  And then iterate to transform this\n$$\\begin{bmatrix} elem1 \u0026amp; y_1 \u0026amp; x_1 \u0026amp; w_1 \u0026amp; h_1 \\\\ elem2 \u0026amp; y_2 \u0026amp; x_2 \u0026amp; w_2 \u0026amp; h_2 \\\\ \\vdots \\\\ elemk \u0026amp; y_2 \u0026amp; x_k \u0026amp; w_k \u0026amp; h_k \\end{bmatrix}$$ in this:\n$$\\begin{bmatrix} id.1 \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\end{bmatrix}$$ This way, you can stack them into:\n$$\\begin{bmatrix} id.1 \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\\\ id.2 \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\\\ \\vdots \\\\ id.N \u0026amp; elem.1.x \u0026amp; elem.1.y \u0026amp; elem.1.h \u0026amp; elem.1.w \u0026amp; \u0026hellip; \u0026amp; elem.k.w \\end{bmatrix}$$\n Dimentionality reduction + Clustering   This most direct form of clusterization for this porpuses is dbscan and run it on our transform base. This didn\u0026rsquo;t work as expected so first I used a technique to reduce dimensionality and then do the clustering.\nPCA and t-SNE are the most popular algorithms in dimensionality reduction but UMAP is the new kid in the block (well it has almost 2 years now) with some fancy math behind it, it preserves global and local structures. and works faster using graphs. One thing to keep in mind when using it is that at one point uses a random procedure which makes it that each time you run it the mapping into 2D will look slightly different, but every point is similary close to others in each iteration. To prevent this, set.seed() is the way to go.\nAnd finally:\nlibrary(umap) library(dbscan) umap_data\u0026lt;- umap(data) cl \u0026lt;-hdbscan(x = umap_data, minPts = 3)  This worked better than with the full dimentions, but not quite as needed. Its time to\u0026hellip;\nTransform and normalice\nMost common option \n  Standarization (z-score): Represents the number of standard deviations up or down of resulting value. Useful for normally distributed variables \n  Normalization (min-max scaler): It allows to transform the data into values between 0 and 1. Useful when working with variables with different orders of magnitude \n  Can I use this transformations in this data?\n Not really, what this variables describe are absolute positions in space and are quite linked to one-another.    What can I do? Instead of using absolute positions and dimentions, lets use its relative, what I\u0026rsquo;ll call \u0026ldquo;geometric normalization\u0026rdquo;  normalize_geometric\u0026lt;-function(df){ df['total_area']\u0026lt;-max(df['element_height'])*max(df['element_width']) df['rel_area']\u0026lt;-df['element_height']*df['element_width']/df['total_area'] df['orientation']\u0026lt;-df['element_height']/df['element_width'] df['element_top_relative']\u0026lt;-df['element_top']/max(df['element_height']) df['element_left_relative']\u0026lt;-df['element_left']/max(df['element_width']) df }    x\u0026rsquo; is now the proportion of x in respecto the total width of the canvas  My new variable is x\u0026rsquo;, red line divided the blue one \n y\u0026rsquo; is now the proportion of x in respecto the total height of the canvas  My new variable is y\u0026rsquo;, red line divided the blue one \n areaRelativa is the proportion of the canvas the element occupies  My new variable areaRelativa is: the are of the small rectangle divided divided the area of the big one \n disposition is to know if the elmenet in horizontal position, vertical or if it is a square  My new variable disposicion es: heigth/width \nResults To begin to analize the results, every \u0026ldquo;spread\u0026rdquo; has to have a \u0026ldquo;gather\u0026rdquo;\ngather_file\u0026lt;-function(gdf){ x\u0026lt;-strsplit(colnames(gdf), '\\\\.') element_name=unique(unlist(map(x, 1))) original_cols=unique(unlist(map(x, 2))) gdf1\u0026lt;-data.frame(element_name) gdf1[original_cols[1]]\u0026lt;-0 gdf1[original_cols[2]]\u0026lt;-0 gdf1[original_cols[3]]\u0026lt;-0 gdf1[original_cols[4]]\u0026lt;-0 rel_area\u0026lt;-c() orientation\u0026lt;-c() element_top_relative\u0026lt;-c() element_left_relative\u0026lt;-c() for(i in seq(from=1, to=length(gdf), by=4)){ # stuff, such as rel_area=c(rel_area,gdf[i]) orientation=c(orientation,gdf[i+1]) element_top_relative = c(element_top_relative,gdf[i+2]) element_left_relative = c(element_left_relative,gdf[i+3]) } gdf1['rel_area']=as_vector(unlist(rel_area)) gdf1['orientation']=as_vector(unlist(orientation)) gdf1['element_top_relative']=as_vector(unlist(element_top_relative)) gdf1['element_left_relative']=as_vector(unlist(element_left_relative)) gdf1}  Let\u0026rsquo;s first see how the clustering works with and without geometric normalization\nAnd finally, examples of each group:\nFirst cluster:\n000003.png\nSecond cluster:\nThird:\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e6926e1132084f9b399c66235ab67294","permalink":"/en/post/clusterizar-disenos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/post/clusterizar-disenos/","section":"post","summary":"Clusterize design patterns ¿How can I find patterns in design?\nWith so many adds around, one would think that that process is quite standardized but it remains, most times, a manual labor. This is, I believe mostly, because of how competitive that market is. To stand out, graphic designers have still a lot of work\nBut that doesn\u0026rsquo;t mean that we can\u0026rsquo;t find design patterns, after all, after a few thousend design even the best designer tend to have trends.\nRead more \n","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"ANALYSIS OF SUPPLY AND EVOLUTION OF THE REAL STATE MARKET The real state market is composed of a series of goods and sevices that are heterogeneous in their characteristics and in their localization\nThe features of each of the housing units in supply are, together, the features of the market in general in a given timeframe\nUsually, what is obvserved is the supply stock and it is assumed that the price is in equilibrium, in this sense, the supply is fixing a price which will allow them to sell or rent at a price in the least amount of time. Nevertheless, the flux of actual sales is not observable in this sample, but it might be yet possible to determine by analysing the changes in supply over time\nThe scrapping methods takes the data monthly in order to get the necessary data to perform such analysis.\nIn this presentation, we\u0026rsquo;ll give a view of the supply. Leaving the demand analysis for future oportunity\nRead more \nStudy of supply The total amount of registers we have for the third trimester of 2019 in our scrapping, once deleted repeated adds, is roughly 25675; of which 15562 are apartments and 5489 to houses.\nThe next table is a summary of total and average squared meters, and their values in US Dollars for Mar del Plata\nreferences Departamento = Apartment Casa = House PH = Horizontal Property, a building with housing units much bigger that apartments but has less than 3 floors Terreno = Land Local Comercial = Comerce shop Local Comercial = Comerce office Garage = Garage Fondo de Comercio = On-going business in sale Bodega-Galpon = shed (as big as a block usually) Edificio = Building\n Features of Sales dataset  Tipo Cantidad Precio_M2 Tamaño Precio    Departamento 15562 2066 70 138747  Casa 5489 811 434 245960  PH 1645 1207 101 99352  Terrenos 1451 743 700 316334  Local comercial 659 1751 208 197579  Oficina comercial 287 1569 98 125887  Garage 245 912 1932 803429  Fondo de Comercio 132 1221 968 1340016  Bodega-Galpón 99 611 875 336943  Edificio 51 1737 759 887350    From this table it came out that 82.16% of the supply is composed by Departamento (60.74%) y Casa (21.42%) and that the apartments have the highest prices by squared meter and the least change in supply\nLikewise the data reveal the variation in quantities, prices and sizes of each type of housing unit\nIn this case we can see, for example, that, for apartments, the rise in size (total squared meters) causes that, even though the total price risses, the price by squared meters falls. The oposite occurs for the PH\u0026rsquo;s\nThe dataset also provides housing units that have activaly reduce it\u0026rsquo;s prices in the last three months\n Bajaron de Precio  Tipo Cantidad Bajó Precio_M2 var Precio m2 Tamaño var Tamaño Precio var Precio    Departamento 207 8.63% 1891 -13.51% 54 -33.65% 100875 -35.72%  PH 10 9.80% 1173 12.05% 81 -50.80% 91780 -27.27%    In the table we can see that the average price of apartments drop 8.63%, meanwhile the prices of the PHs dropped even more, an average of 9.8%.\nThe housing units that lowered the price also present a series of features we can exam.\nThe prices for squared meter that lowered the price are 13.5% cheaper than the overall average, meaning that those which lower the price were already cheaper than the overall sample. The same for sizes\nThe PH\u0026rsquo;s have a peculiar feature. Even though, those which lowered the price are smaller than the general sample, the price by squared meter is higher. This is because the difference between the average size si higher than the difference between the price of the housing unit\nReal State brokers The dataset provides as well, the real state broker since november 2019. This allows us to evaluate better who is offering.\nTo begin with, we can see that the majority of the brokers have a few publications, 77% have less than 22 in this trimester. But there is also brokers with a lot of publications; 1% of the brokers have between 276 and 419 publications\nThis data can also be used to track the market-share of each broker, and analyse the evolution\nZones of Supply Density The data is geolocated in 97%, which permits us to map it and visualize the density of the real state supply. As expected, the major density are near the center of town, near the bus terminal and near the cost; given that Mar del Plata is a major tourist center\n A further anlysis of each zone can be made, so we can describe them\nOverall, how is the supply distributed in each zone?\nEsto nos dice que si bien la zona 6 ocupa el 10% de los m² ofrecidos, constituye el 26% de las publicaciones. Y, conjuntamente con la zona 5, son el 25% de los m² ofrecidos, pero superan el 52% del total de publicaciones. This tell us that, even though zone 6 occupies 10% the m² offered, is 26% of the publications. And, toghether with zone 5, they are more than 52% of the publications\nThe variables than will be described for each zone are:\n Type of housing unit Size in squared meters Price by squared meter Total price  Type Below, we can see that the zone of least density (zone 0), the market is composed mainly with land and houses. And as we advance to the zone of major density (zone 6) the composition of the supply varies. In the first place, the land are no longer offered, the apartments takes over as the main type and in the center of town also appears comercial shops and garages\nThis is exactly what one would expect, if the housing unit is smaller, more units fit in a given space\nSizes The sizes of each unit play a fundamental role in the geographical concentration of the supply\nAs we saw in the \u0026ldquo;Type\u0026rdquo; section, we can quantify how the types which are traditionaly the largest (houses, land) affect the concentration\nPrices The prices in each area are more homogeneous than the sizes or any other variable previously analyse. There is still a tendency that more supply means less prices. For example, the least average price is in zone 6\nThis information, doesn\u0026rsquo;t reflect how the prices varies in each zone. Prices in zone 6 may be the least, in average, but this prices hide a major variabilty. Prices in zone 6 have 50% variation than in zone 0. Meaning that, even though the prices are lower, in average, there are a lot with higher prices, and with lower prices. And in zones where houses are the main unit, the prices are more steady\nPrice by squared meter This a sort of conclution of the two previous parts. We saw that the units gets larger in less concentrated areas, and the total prices doesn\u0026rsquo;t change that much. This means that the prices by squared meter will be higher there where there is a major supply concentration\nAs price zones\u0026hellip; The same methology can be used to extract different zones, Prices, number of rooms, etc which can be made ad hoc\n Some of the alternative maps can be seen in: mapa interactivo\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"89408894346c4876cc960709d6888a89","permalink":"/en/en/post/mardel_realstate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/en/post/mardel_realstate/","section":"en","summary":"ANALYSIS OF SUPPLY AND EVOLUTION OF THE REAL STATE MARKET The real state market is composed of a series of goods and sevices that are heterogeneous in their characteristics and in their localization\nThe features of each of the housing units in supply are, together, the features of the market in general in a given timeframe\nUsually, what is obvserved is the supply stock and it is assumed that the price is in equilibrium, in this sense, the supply is fixing a price which will allow them to sell or rent at a price in the least amount of time. Nevertheless, the flux of actual sales is not observable in this sample, but it might be yet possible to determine by analysing the changes in supply over time\nThe scrapping methods takes the data monthly in order to get the necessary data to perform such analysis.\nIn this presentation, we\u0026rsquo;ll give a view of the supply. Leaving the demand analysis for future oportunity\nRead more \n","tags":null,"title":"","type":"en"},{"authors":null,"categories":null,"content":"ANALYSIS OF SUPPLY AND EVOLUTION OF THE REAL STATE MARKET The real state market is composed of a series of goods and sevices that are heterogeneous in their characteristics and in their localization\nThe features of each of the housing units in supply are, together, the features of the market in general in a given timeframe\nUsually, what is obvserved is the supply stock and it is assumed that the price is in equilibrium, in this sense, the supply is fixing a price which will allow them to sell or rent at a price in the least amount of time. Nevertheless, the flux of actual sales is not observable in this sample, but it might be yet possible to determine by analysing the changes in supply over time\nThe scrapping methods takes the data monthly in order to get the necessary data to perform such analysis.\nIn this presentation, we\u0026rsquo;ll give a view of the supply. Leaving the demand analysis for future oportunity\nRead more \nStudy of supply The total amount of registers we have for the third trimester of 2019 in our scrapping, once deleted repeated adds, is roughly 25675; of which 15562 are apartments and 5489 to houses.\nThe next table is a summary of total and average squared meters, and their values in US Dollars for Mar del Plata\nreferences Departamento = Apartment Casa = House PH = Horizontal Property, a building with housing units much bigger that apartments but has less than 3 floors Terreno = Land Local Comercial = Comerce shop Local Comercial = Comerce office Garage = Garage Fondo de Comercio = On-going business in sale Bodega-Galpon = shed (as big as a block usually) Edificio = Building\n Features of Sales dataset  Tipo Cantidad Precio_M2 Tamaño Precio    Departamento 15562 2066 70 138747  Casa 5489 811 434 245960  PH 1645 1207 101 99352  Terrenos 1451 743 700 316334  Local comercial 659 1751 208 197579  Oficina comercial 287 1569 98 125887  Garage 245 912 1932 803429  Fondo de Comercio 132 1221 968 1340016  Bodega-Galpón 99 611 875 336943  Edificio 51 1737 759 887350    From this table it came out that 82.16% of the supply is composed by Departamento (60.74%) y Casa (21.42%) and that the apartments have the highest prices by squared meter and the least change in supply\nLikewise the data reveal the variation in quantities, prices and sizes of each type of housing unit\nIn this case we can see, for example, that, for apartments, the rise in size (total squared meters) causes that, even though the total price risses, the price by squared meters falls. The oposite occurs for the PH\u0026rsquo;s\nThe dataset also provides housing units that have activaly reduce it\u0026rsquo;s prices in the last three months\n Bajaron de Precio  Tipo Cantidad Bajó Precio_M2 var Precio m2 Tamaño var Tamaño Precio var Precio    Departamento 207 8.63% 1891 -13.51% 54 -33.65% 100875 -35.72%  PH 10 9.80% 1173 12.05% 81 -50.80% 91780 -27.27%    In the table we can see that the average price of apartments drop 8.63%, meanwhile the prices of the PHs dropped even more, an average of 9.8%.\nThe housing units that lowered the price also present a series of features we can exam.\nThe prices for squared meter that lowered the price are 13.5% cheaper than the overall average, meaning that those which lower the price were already cheaper than the overall sample. The same for sizes\nThe PH\u0026rsquo;s have a peculiar feature. Even though, those which lowered the price are smaller than the general sample, the price by squared meter is higher. This is because the difference between the average size si higher than the difference between the price of the housing unit\nReal State brokers The dataset provides as well, the real state broker since november 2019. This allows us to evaluate better who is offering.\nTo begin with, we can see that the majority of the brokers have a few publications, 77% have less than 22 in this trimester. But there is also brokers with a lot of publications; 1% of the brokers have between 276 and 419 publications\nThis data can also be used to track the market-share of each broker, and analyse the evolution\nZones of Supply Density The data is geolocated in 97%, which permits us to map it and visualize the density of the real state supply. As expected, the major density are near the center of town, near the bus terminal and near the cost; given that Mar del Plata is a major tourist center\n A further anlysis of each zone can be made, so we can describe them\nOverall, how is the supply distributed in each zone?\nEsto nos dice que si bien la zona 6 ocupa el 10% de los m² ofrecidos, constituye el 26% de las publicaciones. Y, conjuntamente con la zona 5, son el 25% de los m² ofrecidos, pero superan el 52% del total de publicaciones. This tell us that, even though zone 6 occupies 10% the m² offered, is 26% of the publications. And, toghether with zone 5, they are more than 52% of the publications\nThe variables than will be described for each zone are:\n Type of housing unit Size in squared meters Price by squared meter Total price  Type Below, we can see that the zone of least density (zone 0), the market is composed mainly with land and houses. And as we advance to the zone of major density (zone 6) the composition of the supply varies. In the first place, the land are no longer offered, the apartments takes over as the main type and in the center of town also appears comercial shops and garages\nThis is exactly what one would expect, if the housing unit is smaller, more units fit in a given space\nSizes The sizes of each unit play a fundamental role in the geographical concentration of the supply\nAs we saw in the \u0026ldquo;Type\u0026rdquo; section, we can quantify how the types which are traditionaly the largest (houses, land) affect the concentration\nPrices The prices in each area are more homogeneous than the sizes or any other variable previously analyse. There is still a tendency that more supply means less prices. For example, the least average price is in zone 6\nThis information, doesn\u0026rsquo;t reflect how the prices varies in each zone. Prices in zone 6 may be the least, in average, but this prices hide a major variabilty. Prices in zone 6 have 50% variation than in zone 0. Meaning that, even though the prices are lower, in average, there are a lot with higher prices, and with lower prices. And in zones where houses are the main unit, the prices are more steady\nPrice by squared meter This a sort of conclution of the two previous parts. We saw that the units gets larger in less concentrated areas, and the total prices doesn\u0026rsquo;t change that much. This means that the prices by squared meter will be higher there where there is a major supply concentration\nAs price zones\u0026hellip; The same methology can be used to extract different zones, Prices, number of rooms, etc which can be made ad hoc\n Some of the alternative maps can be seen in: mapa interactivo\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3ca8b3c35f2da3cfb1d34d4a015e35a1","permalink":"/en/post/mardel_realstate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/post/mardel_realstate/","section":"post","summary":"ANALYSIS OF SUPPLY AND EVOLUTION OF THE REAL STATE MARKET The real state market is composed of a series of goods and sevices that are heterogeneous in their characteristics and in their localization\nThe features of each of the housing units in supply are, together, the features of the market in general in a given timeframe\nUsually, what is obvserved is the supply stock and it is assumed that the price is in equilibrium, in this sense, the supply is fixing a price which will allow them to sell or rent at a price in the least amount of time. Nevertheless, the flux of actual sales is not observable in this sample, but it might be yet possible to determine by analysing the changes in supply over time\nThe scrapping methods takes the data monthly in order to get the necessary data to perform such analysis.\nIn this presentation, we\u0026rsquo;ll give a view of the supply. Leaving the demand analysis for future oportunity\nRead more \n","tags":null,"title":"","type":"post"}]