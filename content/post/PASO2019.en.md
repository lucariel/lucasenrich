Twitter and the Argentinean Primaries
==================

This was my first try mining twitter to get acquaintance with API's
and some rudimentary analysis. What was done here was to replicate
other's methodologies.

To begin with, lets cover the basis, how to mine data from Twitter?
Well, for that case, I used Tweepy. (<http://www.tweepy.org/>)



<a href="https://lucasenrich.netlify.com/en/post/paso2019/">Read more </a>

<!--more-->
Let's import the important


    import tweepy
    from tweepy.streaming import StreamListener
    from tweepy import Stream
    import time
    from slistener import SListener
    import os
    import matplotlib.pyplot as plt
    import json
    import requests



slistener is a ad-hoc script from
<https://github.com/alexhanna/hse-twitter/blob/master/bin/slistener.py>
that allows you to create a "listener" object, which collects the data in
real time, and saving it

Once the toolbox is imported, let's take out the tools I'm going to use out
of it.

    auth = tweepy.OAuthHandler('zarazatoken', 'zarazatoken)
    auth.set_access_token('zarazatoken', 'zarazatoken')
    api = tweepy.API(auth)

    datapath = os.path.join(os.getcwd(), 'data')
    datafiles = os.listdir(datapath)

After that, I needed to make a list of words to listen to, the keywords. The listener class
will filter from all the twits in the sample provided by the API and save it 
in a csv file.

    keywords_to_track = ['EleccionesPASO2019', 'FrenteDeTodos','Frente Todos',
                         'Juntos por el Cambio','Juntos Cambio','Elecciones','PASO',
                         'YoTeVotoAlberto','NoVuelvenNuncaMas',
                        
    'ArgentinaVota','Macri','YoLoVoto','Fernandez','Kirchner']
    stream.filter(track = keywords_to_track)

The kewords were selected acording to two criteria:

	+ related trending topics 

	+ selected by me, acording to some I thought would be usefull 



The listener listened from 7am to 5pm. Meaning, an hour before voting starts
and one hour before it closes.

Next part was harder than anticipated, before being able to save the 
scrap in a csv file, it was necessary to convert it to it from  a json file.




    import pandas as pd
    import numpy as np
    tweets = []
    with open(os.path.join(datapath,datafiles[1]), 'r',encoding='utf-8', errors='ignore') as t:
        tw_json = t.read().split('\n')
        for tw in tw_json:
            #print(tw)
            #print('\t\t')
            try:
                tweet_obj = json.loads(tw)
            except:
                pass
            if 'extended_tweet'in tweet_obj:
                tweet_obj['extended_tweet-text'] =  tweet_obj['extended_tweet']['full_text']
                if tw != '':
                    tweets.append(tw)
            
    pd.DataFrame(tweets)

Now we are ready! A beautiful data frame is ready to be analysed.

Well, one of the methologies I took inspiration from did a topic extraction.
I, when I tried to do it, found that it wouldn't really make sense, since
by the very nature of the analysis, all topics would be about the elections

Then I filtered the dataset to keep the keywords which named the two 
main candidates, and those datasets are the "topics"



    {'YoVotoMM','juntosporelcambio', 'votomm','NoVuelvenNuncaMas', 'yolovoto','Macri'} ## 6,320 rows left

    {'FrenteDeTodos','futurocontodos','YoTeVotoAlberto','FernandezFernandez','CFK,'cristina kirchner''Alberto Fernandez'} ## 5,554 rows left



One thing I wondered was, who was more mentionated?

First in each dataframe is aggregated by keyword, and count which is more mentionanted in each hour.




    macri['p'] = 'MM'
    frente_de_todos['p'] = 'FF'

    df1 = pd.concat([macri, frente_de_todos])

    df2 = pd.get_dummies(df1.p)


    mean_mm = df2['MM'].resample('1 h').mean()
    mean_ff = df2['FF'].resample('1 h').mean()

Plotting:

<img src="/img/plotpaso1.jpeg" width="30%" style="float:center; padding:0% 35%" />

Se ve que salvo a la ma√±ana y bien entrada la tarde, se hablo mas de
Macri.

It seems that, except in the morning and very late afternoon, Macri was more talked 
about.

Well, having done the first part in Python, now it's time to continue with
Sentiment Analysis with R.



    library(tidyverse)
    library(tidytext)
    library(stopwords)
    library(syuzhet)
    library(stopwords)

Get the data from the python script:

    paso<-read_csv('paso.csv')
    paso<-paso[colnames(paso)!="X1"]
    paso_unique<-unique(paso$`extended_tweet-full_text`)
    paso_unique2<-as_tibble(paso_unique)

unique() is necessary because retweet and quotes duplicate some of the tw

    length(paso$`extended_tweet-full_text`)
     
    # 44423 rows left


    length(unique(paso$`extended_tweet-full_text`)) 
    # 43996 rows left


Tokenize time:


    tweet_token<-paso_unique2%>%
      unnest_tokens(word, txt)


    tweet_token<-tweet_token%>%
      count(word, sort = T)%>%
      filter(!word%in% stopwords('es'))%>%
      filter(!word%in% stopwords('en'))%>%
      filter(str_detect(word, "^[a-zA-z]|^#|^@"))%>%
      ungroup()%>%
      arrange(desc(n))%>%
      mutate(w = word,
             freq = n)%>%
      select(w, freq)

    ## Result

       w                   freq
       <chr>              <int>
     1 t.co               18839
     2 https              18834
     3 paso               16293
     4 elecciones          8401
     5 macri               7982
     6 si                  5853
     7 eleccionespaso2019  5498
     8 votar               4950
     9 q                   4318
    10 hoy                 3304

This is not so good, there are many that is not relevant,such as "t.co"

    tweet_token_2<-tweet_token%>%filter(w!='t.co')%>%filter(w!='https')%>%
      filter(w!='q')%>%filter(w!='to')%>%filter(w!='si')%>%filter(w!='and')%>%
      filter(w!='rt')

       w                    freq
       <chr>               <int>
     1 paso                16293
     2 elecciones           8401
     3 macri                7982
     4 eleccionespaso2019   5498
     5 votar                4950
     6 hoy                  3304
     7 argentinavota        3043
     8 eleccionesargentina  2878
     9 voto                 2588
    10 trump                2409


The trump count must be about another subject, but this looks so much better




    tweet_token_2 [ 1 : 25 , ] %>%
      mutate ( w = forcats :: fct_inorder ( w ) ) %>%
      ggplot ( aes ( x = w , y = freq ) ) +
      geom_segment ( aes ( x = w , xend = w , y = 0 , yend = freq ) , color= "grey" )+
      geom_point(size = 3, color = "#009A44")+
      coord_flip()

<img src="/img/plotpaso2.jpeg" width="30%" style="float:center; padding:0% 35%" />

Well, "paso" is the most frequent term, which is more than expected.
To get the sentiment of each, the package "syuzhet" is great. Because
it not only get the polarization of the text but asign it to a 
particular set of emotions



    base_emocion<-get_nrc_sentiment(unlist(paso_unique2))
    base_emocion <- data.frame(t(base_emocion))
    base_emocion <- data.frame ( rowMeans ( base_emocion ) )
    names ( base_emocion ) [ 1 ] <- "Proporcion"
    base_emocion <- cbind ( 'Sentimiento' = rownames ( base_emocion ) , base_emocion )

    base_emocion%>%
      ggplot()+geom_bar(aes(x = Sentimiento, y = Proporcion), stat = 'identity', fill = 'green', alpha = 0.8)+
      theme(axis.text.x = element_text(angle = 45, hjust = 1))

This is the most relevant part I took from other's methodology,
he took the sum of each of the cases in each emotion, but
I took the proportion, this, I think, better reflex the 
participations of each ones.


<img src="/img/plotpaso3.jpeg" width="30%" style="float:center; padding:0% 35%" />
In this general case, it seems that between "positive" and "negative" are what 
stands out, but again, it's easier to get polarity than specific emotions.
Then "trust" and anger.


This same method can be used in both datasets, one for each of the two most
competitive parties


<img src="/img/plotpaso4.jpeg" width="30%" style="float:center; padding:0% 35%" />

<img src="/img/plotpaso5.jpeg" width="30%" style="float:center; padding:0% 35%" />

Former president Cristina Fernandez is in everyone's mouth apparently, and she 
is not even a candidate.


How is everyone feeling each time they mention one candidate or the other

<img src="/img/plotpaso6.jpeg" width="30%" style="float:center; padding:0% 35%" />
<img src="/img/plotpaso7.jpeg" width="30%" style="float:center; padding:0% 35%" />


It's kind of interesting, this results not only confirm what would be later
the results, but also the "angry votes", those that vote "against" instead
of "in favor of"


