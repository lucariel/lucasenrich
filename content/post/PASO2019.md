Esta fue mi primera aproximacion a minar datos de redes sociales,
amigarme con las APIs, y algún analysis rudimentario. Siguiendo
metodologias propuestas por otros.

En principio vamos a comenzar con lo primero ¿como hice para minar los
datos de twitter? Bueno para eso use tweepy (<http://www.tweepy.org/>)
Asique la primera parte va a estar en Python.


<a href="https://lucasenrich.netlify.com/post/PASO2019/">Read more </a>


<!--more-->

Importando lo importante



    import tweepy
    from tweepy.streaming import StreamListener
    from tweepy import Stream
    import time
    from slistener import SListener
    import os
    import matplotlib.pyplot as plt
    import json
    import requests

por slistener es un script cortesia de
<https://github.com/alexhanna/hse-twitter/blob/master/bin/slistener.py>
que permite crear un objeto que va a ser el "listener" o "escuchante" de
twitter. Para tomar los datos en tiempo real y poder ir guardandolos.

Instanciando lo instanciable y setiando los paths donde van las cosas

    auth = tweepy.OAuthHandler('zarazatoken', 'zarazatoken)
    auth.set_access_token('zarazatoken', 'zarazatoken')
    api = tweepy.API(auth)

    datapath = os.path.join(os.getcwd(), 'data')
    datafiles = os.listdir(datapath)

    Y vamos a poder el escuchante a escuchar twitter

    keywords_to_track = ['EleccionesPASO2019', 'FrenteDeTodos','Frente Todos',
                         'Juntos por el Cambio','Juntos Cambio','Elecciones','PASO',
                         'YoTeVotoAlberto','NoVuelvenNuncaMas',
                        
    'ArgentinaVota','Macri','YoLoVoto','Fernandez','Kirchner']
    stream.filter(track = keywords_to_track)

Los keywords a trackear se eligieron tomando los trending topic en
argentina referidos a las elecciones y algunos elegidos por mi, a mano

stream.filter() lo que se encarga de hacer es ir tomando la muestra en
tiempo real de datos de twitter que se ajusten al filtro. Mientras corra
(es decir, mientras no se interrumpa) va a ir juntando los datos. Esto
lo empece a correr el domingo de las paso a las 7am y lo frené el mismo
día a las 17hs.

La siguiente parte me fue bastante mas dificil de lo que habia
anticipado, porque estas muestras se guardan en formato ".json" lo cual
tenia que convertir a ".csv" para poder trabajar mejor


    import pandas as pd
    import numpy as np
    tweets = []
    with open(os.path.join(datapath,datafiles[1]), 'r',encoding='utf-8', errors='ignore') as t:
        tw_json = t.read().split('\n')
        for tw in tw_json:
            #print(tw)
            #print('\t\t')
            try:
                tweet_obj = json.loads(tw)
            except:
                pass
            if 'extended_tweet'in tweet_obj:
                tweet_obj['extended_tweet-text'] =  tweet_obj['extended_tweet']['full_text']
                if tw != '':
                    tweets.append(tw)
            
    pd.DataFrame(tweets)

¡Ahora si! Ya tenemos bonito el data\_frame en pandas para guardarlo y
seguir desde allí

La siguiente tarea seria el topic extraction, pero la realidad es que
cuando lo hice no llegue a ningun lado, porque, obviamente y como es de
esperar, estaba todo referido a as elecciones. Lo que si termine
haciendo fue filtrar el dataset que me quedo por las keywords que
nombrar a los dos principales candidatos



    {'YoVotoMM','juntosporelcambio', 'votomm','NoVuelvenNuncaMas', 'yolovoto','Macri'} ## Quedarón 6,320 registros

    {'FrenteDeTodos','futurocontodos','YoTeVotoAlberto','FernandezFernandez','CFK,'cristina kirchner''Alberto Fernandez'} ## Quedarón 5,554 registros

Ahora bien, ¿de quien se hablaba mas en twitter?

Primero al dataframe de cada topic se agrega la variable
correspondiente, se generan las dummies y luego se saca el promedio por
hora, lo que resulta en la proporcion de tuits de cada uno


    macri['p'] = 'MM'
    frente_de_todos['p'] = 'FF'

    df1 = pd.concat([macri, frente_de_todos])

    df2 = pd.get_dummies(df1.p)


    mean_mm = df2['MM'].resample('1 h').mean()
    mean_ff = df2['FF'].resample('1 h').mean()

Sacando el plot:

<img src="/img/plotpaso1.jpeg" width="30%" style="float:center; padding:0% 35%" />

Se ve que salvo a la mañana y bien entrada la tarde, se hablo mas de
Macri.

Bueno, habiendo hecho la primera parte en Python, es hora de continuar
con la parte de sentiment analysis de los tuits de las PASO. Esta vez,
en R. Vamos a empezar por las bibliotecas que necesitamos:


    library(tidyverse)
    library(tidytext)
    library(stopwords)
    library(syuzhet)
    library(stopwords)

Luego traemos los datos:


    paso<-read_csv('paso.csv')
    paso<-paso[colnames(paso)!="X1"]
    paso_unique<-unique(paso$`extended_tweet-full_text`)
    paso_unique2<-as_tibble(paso_unique)

El unique() nos sirve para filtrar tuits duplicados. Que pueden ocurrir
por que un usuario citó a otro.


    length(paso$`extended_tweet-full_text`)
     
    #Quedan 44423 registros


    length(unique(paso$`extended_tweet-full_text`)) 
    #Quedan 43996 registros

Vamos a tokenizar las palabras:


    tweet_token<-paso_unique2%>%
      unnest_tokens(word, txt)


    tweet_token<-tweet_token%>%
      count(word, sort = T)%>%
      filter(!word%in% stopwords('es'))%>%
      filter(!word%in% stopwords('en'))%>%
      filter(str_detect(word, "^[a-zA-z]|^#|^@"))%>%
      ungroup()%>%
      arrange(desc(n))%>%
      mutate(w = word,
             freq = n)%>%
      select(w, freq)

    ## Resultado

       w                   freq
       <chr>              <int>
     1 t.co               18839
     2 https              18834
     3 paso               16293
     4 elecciones          8401
     5 macri               7982
     6 si                  5853
     7 eleccionespaso2019  5498
     8 votar               4950
     9 q                   4318
    10 hoy                 3304

Esto no es muuy bueno, hay tokens que hay que sacar.


    tweet_token_2<-tweet_token%>%filter(w!='t.co')%>%filter(w!='https')%>%
      filter(w!='q')%>%filter(w!='to')%>%filter(w!='si')%>%filter(w!='and')%>%
      filter(w!='rt')

       w                    freq
       <chr>               <int>
     1 paso                16293
     2 elecciones           8401
     3 macri                7982
     4 eleccionespaso2019   5498
     5 votar                4950
     6 hoy                  3304
     7 argentinavota        3043
     8 eleccionesargentina  2878
     9 voto                 2588
    10 trump                2409

Ahora si, mira que loco lo de Trump. Igual esto se ve mucho mejor con un
gráfico, además, no filtre todavia los stopwords y no filtre por tuits
en español, asique probablemente sean tuits colados de otro tema


    tweet_token_2 [ 1 : 25 , ] %>%
      mutate ( w = forcats :: fct_inorder ( w ) ) %>%
      ggplot ( aes ( x = w , y = freq ) ) +
      geom_segment ( aes ( x = w , xend = w , y = 0 , yend = freq ) , color= "grey" )+
      geom_point(size = 3, color = "#009A44")+
      coord_flip()

<img src="/img/plotpaso2.jpeg" width="30%" style="float:center; padding:0% 35%" />

Bueno, el termino "paso" es evidentemente el mas frecuente, lo cual es
mas que esperable. Luego, nos quedaria ver como se sentia la gente
respecto a esto. Para esto se uso la libreria syuzhet


    base_emocion<-get_nrc_sentiment(unlist(paso_unique2))
    base_emocion <- data.frame(t(base_emocion))
    base_emocion <- data.frame ( rowMeans ( base_emocion ) )
    names ( base_emocion ) [ 1 ] <- "Proporcion"
    base_emocion <- cbind ( 'Sentimiento' = rownames ( base_emocion ) , base_emocion )

    base_emocion%>%
      ggplot()+geom_bar(aes(x = Sentimiento, y = Proporcion), stat = 'identity', fill = 'green', alpha = 0.8)+
      theme(axis.text.x = element_text(angle = 45, hjust = 1))

Esta es la parte mas relevante que tome de Hernan, la diferencia que
tome, fue que él tomo la suma de cada una de los casos de cada
sentimiento, y yo la proporcion. Creo que eso puede reflejar de otra
forma cual es la emocion predominante en cada caso:

<img src="/img/plotpaso3.jpeg" width="30%" style="float:center; padding:0% 35%" />
En este caso general, se ve que entre "positivo" y "negativo" son los
predominantes, seguidos por "confianza" y "enojo".

Esta misma metodologia se puede usar para los dos datasets separados
para cada topic pre-seleccionado, los referidos al frente de todos y a
juntos por el cambio

<img src="/img/plotpaso4.jpeg" width="30%" style="float:center; padding:0% 35%" />

<img src="/img/plotpaso5.jpeg" width="30%" style="float:center; padding:0% 35%" />

Todos se quejan, pero de Cristina hablan todos. Igual hay que considerar
que de este conteo, se filtraron los nombres y apellidos de los
candidatos a la presidencia ya que es lo que se uso de filtro.

¿Como se sienten?
<img src="/img/plotpaso6.jpeg" width="30%" style="float:center; padding:0% 35%" />
<img src="/img/plotpaso7.jpeg" width="30%" style="float:center; padding:0% 35%" />

Un poco de esto confirma no solo los resultado de la eleccion sino
tambien la lectura del voto "enojo". Porque los sentimientos asociados a
cambiemos tienen mayor participacion de enojo y sentimientos negativos.
Mientras que los asociados al frente de todos tiene mucha mayor
participacion los tuits positivos.

Un bonus track de python nada mas (esbozo de network analysis) ¿A quien
se le contestaba mas para cada grupo?

Las libreras de python son las mismas que el post anterior solo con la
adicional de Networkx que permite hacer el analisis de redes.

    import networkx as nx

    frente_de_todos = pd.read_csv('frente_de_todos.csv')
    cambiemos = pd.read_csv('cambiemos.csv')

Una vez leidos, filtramos los tuits que "son respuesta a"

    cambiemos_nx = cambiemos_nx[-cambiemos_nx['in_reply_to_screen_name'].isnull()]
    cambiemos_nx['in_reply_to_screen_name']
    frente_de_todos_nx = frente_de_todos_nx[-frente_de_todos_nx['in_reply_to_screen_name'].isnull()]
    frente_de_todos_nx['in_reply_to_screen_name']

Se generan las redes de c/u; esto genera que cada usuario sea un nodo y
que la relacion entre los usuarios se dá, en este caso particular, si
responden a un tuit es decir: si yo te respondo un tuit, nosotros dos
generamos una red que tiene mi nombre (mi usuario) como nodo de inicio y
tu nodo (tu usuario) como nodo destino. Cada objeto, entonces, va a
tener tantas salidas como respuestas haya hecho y tantas entradas como
respuestas haya recibido:


    G_reply_c = nx.from_pandas_edgelist(
        cambiemos_nx,
        source = 'user-screen_name',
        target = 'in_reply_to_screen_name',
        create_using = nx.DiGraph())

    G_reply_f = nx.from_pandas_edgelist(
        frente_de_todos_nx,
        source = 'user-screen_name',
        target = 'in_reply_to_screen_name',
        create_using = nx.DiGraph())

Y vemos la centralidad de cada tuitero ("in-degree-centrality"), que en
realidad seria la respuesta a la pregunta "¿A quien se le esta
contestando más?"

    #Para Frente de Todos - ¿a quien se le contesta cuando se habla de estse #tema?

    G_reply_f = nx.from_pandas_edgelist(
        frente_de_todos_nx,
        source = 'user-screen_name',
        target = 'in_reply_to_screen_name',
        create_using = nx.DiGraph())


    bc = nx.in_degree_centrality(G_reply_f)
    indg = pd.DataFrame(list(bc.items()), columns =["Name",'Cent'])
    indg.sort_values('Cent', ascending=False)
    Name    Cent
    153 alferdez        0.018328
    18  ierrejon        0.017182
    137 LotusHerbals    0.017182
    115 todonoticias    0.016037
    113 fllorenteantoni 0.013746
    746 AlbertoRavell   0.010309
    159 FernandezAnibal 0.010309
    147 LeonelFernandez 0.006873
    236 mirthalegrand   0.006873

Y en el caso del Juntos Por el Cambio

    G_reply_c = nx.from_pandas_edgelist(
        cambiemos_nx,
        source = 'user-screen_name',
        target = 'in_reply_to_screen_name',
        create_using = nx.DiGraph())

    bc = nx.in_degree_centrality(G_reply_c)
    indg = pd.DataFrame(list(bc.items()), columns =["Name",'Cent'])
    indg.sort_values('Cent', ascending=False)

    Name    Cent
    36  mauriciomacri   0.021858
    148 fllorenteantoni 0.012610
    131 gabicerru       0.011349
    84  juansolervalls  0.008827
    119 EsmeraldaMitre  0.007566
    3   todonoticias    0.005885
    99  CamiSolovitas   0.004624
    17  Alfredo5019     0.004624
    23  SantoroLeandro  0.004203 

FIN! Gracias por leer hasta acá! Si tienen alguna recomendacion para
tener en cuenta futuros analisis se los agradece!
